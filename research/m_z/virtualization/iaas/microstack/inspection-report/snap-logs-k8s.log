2025-05-16T19:32:48-04:00 systemd[1]: Started snap.k8s.k8sd.service - Service for snap application k8s.k8sd.
2025-05-16T19:32:48-04:00 k8s.k8sd[35160]: + ulimit -c unlimited
2025-05-16T19:32:48-04:00 k8s.k8sd[35160]: + export GOTRACEBACK=crash
2025-05-16T19:32:48-04:00 k8s.k8sd[35160]: + GOTRACEBACK=crash
2025-05-16T19:32:48-04:00 k8s.k8sd[35160]: + [[ -f /var/snap/k8s/common/args/k8sd-env ]]
2025-05-16T19:32:48-04:00 k8s.k8sd[35160]: + exec /snap/k8s/2500/bin/k8sd --state-dir=/var/snap/k8s/common/var/lib/k8sd/state
2025-05-16T19:32:48-04:00 k8s.k8sd[35160]: time="2025-05-16T19:32:48-04:00" level=warning msg="Microcluster database is uninitialized"
2025-05-16T19:32:48-04:00 k8s.k8sd[35160]: time="2025-05-16T19:32:48-04:00" level=info msg=" - binding control socket" socket=/var/snap/k8s/common/var/lib/k8sd/state/control.socket
2025-05-16T19:32:48-04:00 k8s.k8sd[35160]: I0516 19:32:48.696327   35160 controllers/node_configuration.go:49] "Waiting for node to be ready" logger="k8sd" controller="node-configuration"
2025-05-16T19:32:56-04:00 k8s.k8sd[35160]: time="2025-05-16T19:32:56-04:00" level=info msg=" - binding https socket" network="172.24.188.57:6400"
2025-05-16T19:32:56-04:00 k8s.k8sd[35160]: time="2025-05-16T19:32:56-04:00" level=info msg="Preparing statements for Go project \"k8s-binaries\""
2025-05-16T19:32:56-04:00 k8s.k8sd[35160]: I0516 19:32:56.914884   35160 setup/directories.go:57] "Creating cni bin directory" logger="k8sd" cniBinDir="/opt/cni/bin"
2025-05-16T19:33:03-04:00 k8s.k8sd[35695]: Kubernetes v1.32.2
2025-05-16T19:33:03-04:00 k8s.k8sd[35703]: Kubernetes v1.32.2
2025-05-16T19:33:03-04:00 k8s.k8sd[35710]: Kubernetes v1.32.2
2025-05-16T19:33:04-04:00 k8s.k8sd[35721]: Kubernetes v1.32.2
2025-05-16T19:33:04-04:00 k8s.k8sd[35738]: I0516 19:33:04.250472   35738 resolv_conf.go:34] Using /run/systemd/resolve/resolv.conf for the DNS resolver config
2025-05-16T19:33:04-04:00 k8s.k8sd[35738]: Kubernetes v1.32.2
2025-05-16T19:33:04-04:00 k8s.k8sd[35160]: I0516 19:33:04.941831   35160 app/hooks_bootstrap.go:496] "Starting control-plane services" logger="k8sd" hook="bootstrap"
2025-05-16T19:33:06-04:00 systemd[1]: Started snap.k8s.k8s-dqlite.service - Service for snap application k8s.k8s-dqlite.
2025-05-16T19:33:06-04:00 k8s.k8s-dqlite[35913]: + ulimit -c unlimited
2025-05-16T19:33:06-04:00 k8s.k8s-dqlite[35913]: + export GOTRACEBACK=crash
2025-05-16T19:33:06-04:00 k8s.k8s-dqlite[35913]: + GOTRACEBACK=crash
2025-05-16T19:33:06-04:00 k8s.k8s-dqlite[35913]: + [[ -f /var/snap/k8s/common/args/k8s-dqlite-env ]]
2025-05-16T19:33:06-04:00 k8s.k8s-dqlite[35913]: + exec /snap/k8s/2500/bin/k8s-dqlite --listen=unix:///var/snap/k8s/common/var/lib/k8s-dqlite/k8s-dqlite.sock --storage-dir=/var/snap/k8s/common/var/lib/k8s-dqlite
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="Will initialize dqlite node" address="172.24.188.57:9000" cluster="[]"
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="Configure dqlite failure domain" failure-domain=0
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="Disable TLS ClientSessionCache"
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="Enable TLS" min_tls_version=tls12
2025-05-16T19:33:07-04:00 systemd[1]: Starting snap.k8s.containerd.service - Service for snap application k8s.containerd...
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="Started dqlite" address="172.24.188.57:9000" id=3297041220608546238
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="New kine for dqlite"
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="DriverName is dqlite-1."
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="Configuring database connection pooling: maxIdleConns=5, maxOpenConns=5, connMaxLifetime=1m0s, connMaxIdleTime=0s "
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="Kine listening on unix:///var/snap/k8s/common/var/lib/k8s-dqlite/k8s-dqlite.sock"
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="Started kine" address="unix:///var/snap/k8s/common/var/lib/k8s-dqlite/k8s-dqlite.sock" database="dqlite://k8s?driver-name=dqlite-1&watch-query-timeout=20s"
2025-05-16T19:33:07-04:00 k8s.k8s-dqlite[35913]: time="2025-05-16T19:33:07-04:00" level=info msg="Enable periodic check for available disk size" dir=/var/snap/k8s/common/var/lib/k8s-dqlite interval=5s
2025-05-16T19:33:07-04:00 k8s.containerd[36097]: + ulimit -c unlimited
2025-05-16T19:33:07-04:00 k8s.containerd[36097]: + export GOTRACEBACK=crash
2025-05-16T19:33:07-04:00 k8s.containerd[36097]: + GOTRACEBACK=crash
2025-05-16T19:33:07-04:00 k8s.containerd[36097]: + [[ -f /var/snap/k8s/common/args/containerd-env ]]
2025-05-16T19:33:07-04:00 k8s.containerd[36097]: + exec /snap/k8s/2500/bin/containerd --address=/run/containerd/containerd.sock --config=/etc/containerd/config.toml --root=/var/lib/containerd --state=/run/containerd
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.109329636-04:00" level=info msg="starting containerd" revision=75931e35c0721f957144c09fe18773018cdd3740 version=v1.6.36
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.155975964-04:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.aufs\"..." type=io.containerd.snapshotter.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.297341667-04:00" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.aufs\"..." error="aufs is not supported (modprobe aufs failed: exit status 1 \"modprobe: FATAL: Module aufs not found in directory /lib/modules/6.11.0-25-generic\\n\"): skip plugin" type=io.containerd.snapshotter.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.297428158-04:00" level=info msg="loading plugin \"io.containerd.content.v1.content\"..." type=io.containerd.content.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.297823999-04:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." type=io.containerd.snapshotter.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.298478885-04:00" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.btrfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.btrfs (ext4) must be a btrfs filesystem to be used with the btrfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.298530187-04:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.devmapper\"..." type=io.containerd.snapshotter.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.298574102-04:00" level=warning msg="failed to load plugin io.containerd.snapshotter.v1.devmapper" error="devmapper not configured"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.298605875-04:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.native\"..." type=io.containerd.snapshotter.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.298822881-04:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.overlayfs\"..." type=io.containerd.snapshotter.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.299513365-04:00" level=info msg="loading plugin \"io.containerd.snapshotter.v1.zfs\"..." type=io.containerd.snapshotter.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.299947125-04:00" level=info msg="skip loading plugin \"io.containerd.snapshotter.v1.zfs\"..." error="path /var/lib/containerd/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin" type=io.containerd.snapshotter.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.299997923-04:00" level=info msg="loading plugin \"io.containerd.metadata.v1.bolt\"..." type=io.containerd.metadata.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.300155468-04:00" level=warning msg="could not use snapshotter devmapper in metadata plugin" error="devmapper not configured"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.300205757-04:00" level=info msg="metadata content store policy set" policy=shared
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.308651610-04:00" level=info msg="loading plugin \"io.containerd.differ.v1.walking\"..." type=io.containerd.differ.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.308730473-04:00" level=info msg="loading plugin \"io.containerd.event.v1.exchange\"..." type=io.containerd.event.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.308775832-04:00" level=info msg="loading plugin \"io.containerd.gc.v1.scheduler\"..." type=io.containerd.gc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.308874939-04:00" level=info msg="loading plugin \"io.containerd.runtime.v1.linux\"..." type=io.containerd.runtime.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.309179299-04:00" level=info msg="loading plugin \"io.containerd.runtime.v2.task\"..." type=io.containerd.runtime.v2
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.309449440-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.opt\"..." type=io.containerd.internal.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.309894150-04:00" level=info msg="loading plugin \"io.containerd.warning.v1.deprecations\"..." type=io.containerd.warning.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.309948560-04:00" level=info msg="loading plugin \"io.containerd.monitor.v1.cgroups\"..." type=io.containerd.monitor.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310527940-04:00" level=info msg="loading plugin \"io.containerd.service.v1.containers-service\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310587552-04:00" level=info msg="loading plugin \"io.containerd.service.v1.content-service\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310628106-04:00" level=info msg="loading plugin \"io.containerd.service.v1.diff-service\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310670324-04:00" level=info msg="loading plugin \"io.containerd.service.v1.images-service\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310725904-04:00" level=info msg="loading plugin \"io.containerd.service.v1.introspection-service\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310767858-04:00" level=info msg="loading plugin \"io.containerd.service.v1.leases-service\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310808653-04:00" level=info msg="loading plugin \"io.containerd.service.v1.namespaces-service\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310851489-04:00" level=info msg="loading plugin \"io.containerd.service.v1.snapshots-service\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310890519-04:00" level=info msg="loading plugin \"io.containerd.service.v1.tasks-service\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.310946760-04:00" level=info msg="loading plugin \"io.containerd.service.v1.sideload-images\"..." type=io.containerd.service.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311011857-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.containers\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311057389-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.content\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311097201-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.diff\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311134821-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.events\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311175736-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.images\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311216554-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.introspection\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311253663-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.leases\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311294090-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.namespaces\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311336371-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.snapshots\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311388774-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.tasks\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311482468-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.version\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311573804-04:00" level=info msg="loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." type=io.containerd.tracing.processor.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311624549-04:00" level=info msg="skip loading plugin \"io.containerd.tracing.processor.v1.otlp\"..." error="skip plugin: tracing endpoint not configured" type=io.containerd.tracing.processor.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311661454-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.tracing\"..." type=io.containerd.internal.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311700261-04:00" level=info msg="skip loading plugin \"io.containerd.internal.v1.tracing\"..." error="skip plugin: tracing endpoint not configured" type=io.containerd.internal.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311732037-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.restart\"..." type=io.containerd.internal.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311824438-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.healthcheck\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.311872885-04:00" level=info msg="loading plugin \"io.containerd.grpc.v1.cri\"..." type=io.containerd.grpc.v1
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.312692233-04:00" level=info msg="Start cri plugin with config {PluginConfig:{ContainerdConfig:{Snapshotter:overlayfs DefaultRuntimeName:runc DefaultRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} UntrustedWorkloadRuntime:{Type: Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0} Runtimes:map[runc:{Type:io.containerd.runc.v2 Path: Engine: PodAnnotations:[] ContainerAnnotations:[] Root: Options:map[SystemdCgroup:true] PrivilegedWithoutHostDevices:false BaseRuntimeSpec: NetworkPluginConfDir: NetworkPluginMaxConfNum:0}] NoPivot:false DisableSnapshotAnnotations:true DiscardUnpackedLayers:false IgnoreRdtNotEnabledErrors:false} CniConfig:{NetworkPluginBinDir:/opt/cni/bin NetworkPluginConfDir:/etc/cni/net.d NetworkPluginMaxConfNum:1 NetworkPluginConfTemplate: IPPreference:} Registry:{ConfigPath:/etc/containerd/hosts.d Mirrors:map[] Configs:map[] Auths:map[] Headers:map[]} ImageDecryption:{KeyModel:node} DisableTCPService:true StreamServerAddress:127.0.0.1 StreamServerPort:0 StreamIdleTimeout:4h0m0s EnableSelinux:false SelinuxCategoryRange:1024 SandboxImage:ghcr.io/canonical/k8s-snap/pause:3.10 StatsCollectPeriod:10 SystemdCgroup:false EnableTLSStreaming:false X509KeyPairStreaming:{TLSCertFile: TLSKeyFile:} MaxContainerLogLineSize:16384 DisableCgroup:false DisableApparmor:false RestrictOOMScoreAdj:false MaxConcurrentDownloads:3 DisableProcMount:false UnsetSeccompProfile: TolerateMissingHugetlbController:true DisableHugetlbController:true DeviceOwnershipFromSecurityContext:false IgnoreImageDefinedVolumes:false NetNSMountsUnderStateDir:false EnableUnprivilegedPorts:false EnableUnprivilegedICMP:false IgnoreDeprecationWarnings:[] DrainExecSyncIOTimeout:0s} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/run/containerd/io.containerd.grpc.v1.cri}"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.312836689-04:00" level=info msg="Connect containerd service"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.313093328-04:00" level=info msg="Get image filesystem path \"/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs\""
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.334872622-04:00" level=error msg="failed to load cni during init, please check CRI plugin status before setting up network for pods" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.335166999-04:00" level=info msg="Start subscribing containerd event"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.335260055-04:00" level=info msg="Start recovering state"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.335397427-04:00" level=info msg="Start event monitor"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.335440100-04:00" level=info msg="Start snapshots syncer"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.335485948-04:00" level=info msg="Start cni network conf syncer for default"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.335508092-04:00" level=info msg="Start streaming server"
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.335651804-04:00" level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.335788987-04:00" level=info msg=serving... address=/run/containerd/containerd.sock
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.335945794-04:00" level=info msg="containerd successfully booted in 0.228196s"
2025-05-16T19:33:09-04:00 systemd[1]: Started snap.k8s.containerd.service - Service for snap application k8s.containerd.
2025-05-16T19:33:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:09.346724427-04:00" level=info msg="Plugin terminating" plugin=sideload-images
2025-05-16T19:33:10-04:00 systemd[1]: Started snap.k8s.kube-controller-manager.service - Service for snap application k8s.kube-controller-manager.
2025-05-16T19:33:10-04:00 k8s.kube-controller-manager[36373]: The connection to the server 127.0.0.1:6443 was refused - did you specify the right host or port?
2025-05-16T19:33:10-04:00 k8s.kube-controller-manager[36331]: Waiting for kube-apiserver to start
2025-05-16T19:33:11-04:00 systemd[1]: Started snap.k8s.kube-proxy.service - Service for snap application k8s.kube-proxy.
2025-05-16T19:33:12-04:00 k8s.kube-proxy[36655]: The connection to the server 127.0.0.1:6443 was refused - did you specify the right host or port?
2025-05-16T19:33:12-04:00 k8s.kube-proxy[36626]: Waiting for kube-apiserver to start
2025-05-16T19:33:13-04:00 systemd[1]: Started snap.k8s.kube-scheduler.service - Service for snap application k8s.kube-scheduler.
2025-05-16T19:33:13-04:00 k8s.kube-scheduler[36840]: The connection to the server 127.0.0.1:6443 was refused - did you specify the right host or port?
2025-05-16T19:33:13-04:00 k8s.kube-scheduler[36811]: Waiting for kube-apiserver to start
2025-05-16T19:33:14-04:00 k8s.kube-controller-manager[36994]: The connection to the server 127.0.0.1:6443 was refused - did you specify the right host or port?
2025-05-16T19:33:14-04:00 k8s.kube-controller-manager[36331]: Waiting for kube-apiserver to start
2025-05-16T19:33:14-04:00 systemd[1]: Started snap.k8s.kubelet.service - Service for snap application k8s.kubelet.
2025-05-16T19:33:15-04:00 k8s.kube-proxy[37219]: The connection to the server 127.0.0.1:6443 was refused - did you specify the right host or port?
2025-05-16T19:33:15-04:00 k8s.kube-proxy[36626]: Waiting for kube-apiserver to start
2025-05-16T19:33:15-04:00 systemd[1]: Started snap.k8s.kube-apiserver.service - Service for snap application k8s.kube-apiserver.
2025-05-16T19:33:15-04:00 k8s.kubelet[37226]: The connection to the server 127.0.0.1:6443 was refused - did you specify the right host or port?
2025-05-16T19:33:15-04:00 k8s.k8sd[35160]: I0516 19:33:15.423228   35160 app/hooks_bootstrap.go:507] "Waiting for kube-apiserver to become ready" logger="k8sd" hook="bootstrap"
2025-05-16T19:33:15-04:00 k8s.kubelet[37018]: Waiting for kube-apiserver to start
2025-05-16T19:33:15-04:00 k8s.kube-apiserver[37236]: + ulimit -c unlimited
2025-05-16T19:33:15-04:00 k8s.kube-apiserver[37236]: + export GOTRACEBACK=crash
2025-05-16T19:33:15-04:00 k8s.kube-apiserver[37236]: + GOTRACEBACK=crash
2025-05-16T19:33:15-04:00 k8s.kube-apiserver[37236]: + [[ -f /var/snap/k8s/common/args/kube-apiserver-env ]]
2025-05-16T19:33:15-04:00 k8s.kube-apiserver[37236]: + exec /snap/k8s/2500/bin/kube-apiserver --advertise-address=172.24.188.57 --allow-privileged=true --anonymous-auth=false --authentication-token-webhook-config-file=/var/snap/k8s/common/args/conf.d/auth-token-webhook.conf --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/client-ca.crt --enable-admission-plugins=NodeRestriction --etcd-servers=unix:///var/snap/k8s/common/var/lib/k8s-dqlite/k8s-dqlite.sock --feature-gates=WatchList=false --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP --profiling=false --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --request-timeout=300s --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc --service-account-key-file=/etc/kubernetes/pki/serviceaccount.key --service-account-signing-key-file=/etc/kubernetes/pki/serviceaccount.key --service-cluster-ip-range=10.152.183.0/24 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-cipher-suites=TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
2025-05-16T19:33:15-04:00 k8s.kube-apiserver[37236]: W0516 19:33:15.579288   37236 registry.go:256] calling componentGlobalsRegistry.AddFlags more than once, the registry will be set by the latest flags
2025-05-16T19:33:15-04:00 k8s.kube-apiserver[37236]: I0516 19:33:15.580000   37236 options.go:238] external host was not specified, using 172.24.188.57
2025-05-16T19:33:15-04:00 k8s.kube-apiserver[37236]: I0516 19:33:15.583437   37236 server.go:143] Version: v1.32.2
2025-05-16T19:33:15-04:00 k8s.kube-apiserver[37236]: I0516 19:33:15.583491   37236 server.go:145] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK="crash"
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.035321   37236 shared_informer.go:313] Waiting for caches to sync for node_authorizer
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.055993   37236 shared_informer.go:313] Waiting for caches to sync for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.068518   37236 plugins.go:157] Loaded 13 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionPolicy,MutatingAdmissionWebhook.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.068563   37236 plugins.go:160] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.069108   37236 instance.go:233] Using reconciler: lease
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: E0516 19:33:17.090343   37236 feature_support_checker.go:165] "Failed to parse etcd version" err="could not parse \"\" as version" version=""
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.090385   37236 feature_support_checker.go:133] RequestWatchProgress feature is not supported by "unix:///var/snap/k8s/common/var/lib/k8s-dqlite/k8s-dqlite.sock" endpoint
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.091910   37236 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.091943   37236 genericapiserver.go:767] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.224392   37236 handler.go:286] Adding GroupVersion  v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.224831   37236 apis.go:106] API group "internal.apiserver.k8s.io" is not enabled, skipping.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.270473   37236 apis.go:106] API group "storagemigration.k8s.io" is not enabled, skipping.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.295366   37236 apis.go:106] API group "resource.k8s.io" is not enabled, skipping.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.326193   37236 handler.go:286] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.326240   37236 genericapiserver.go:767] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.326257   37236 genericapiserver.go:767] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.327082   37236 handler.go:286] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.327104   37236 genericapiserver.go:767] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.330351   37236 handler.go:286] Adding GroupVersion autoscaling v2 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.331597   37236 handler.go:286] Adding GroupVersion autoscaling v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.331620   37236 genericapiserver.go:767] Skipping API autoscaling/v2beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.331633   37236 genericapiserver.go:767] Skipping API autoscaling/v2beta2 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.334002   37236 handler.go:286] Adding GroupVersion batch v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.334025   37236 genericapiserver.go:767] Skipping API batch/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.335470   37236 handler.go:286] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.335493   37236 genericapiserver.go:767] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.335506   37236 genericapiserver.go:767] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.336666   37236 handler.go:286] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.336688   37236 genericapiserver.go:767] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.336701   37236 genericapiserver.go:767] Skipping API coordination.k8s.io/v1alpha2 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.337751   37236 handler.go:286] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.337776   37236 genericapiserver.go:767] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.340535   37236 handler.go:286] Adding GroupVersion networking.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.340557   37236 genericapiserver.go:767] Skipping API networking.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.340570   37236 genericapiserver.go:767] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.341373   37236 handler.go:286] Adding GroupVersion node.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.341395   37236 genericapiserver.go:767] Skipping API node.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.341409   37236 genericapiserver.go:767] Skipping API node.k8s.io/v1alpha1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.342740   37236 handler.go:286] Adding GroupVersion policy v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.342762   37236 genericapiserver.go:767] Skipping API policy/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.345689   37236 handler.go:286] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.345711   37236 genericapiserver.go:767] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.345726   37236 genericapiserver.go:767] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.346517   37236 handler.go:286] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.346542   37236 genericapiserver.go:767] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.346555   37236 genericapiserver.go:767] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.350194   37236 handler.go:286] Adding GroupVersion storage.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.350219   37236 genericapiserver.go:767] Skipping API storage.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.350231   37236 genericapiserver.go:767] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.352294   37236 handler.go:286] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.352317   37236 genericapiserver.go:767] Skipping API flowcontrol.apiserver.k8s.io/v1beta3 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.352331   37236 genericapiserver.go:767] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.352341   37236 genericapiserver.go:767] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.359301   37236 handler.go:286] Adding GroupVersion apps v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.359325   37236 genericapiserver.go:767] Skipping API apps/v1beta2 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.359338   37236 genericapiserver.go:767] Skipping API apps/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.363199   37236 handler.go:286] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.363221   37236 genericapiserver.go:767] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.363235   37236 genericapiserver.go:767] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.364517   37236 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.364539   37236 genericapiserver.go:767] Skipping API events.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: I0516 19:33:17.370346   37236 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
2025-05-16T19:33:17-04:00 k8s.kube-apiserver[37236]: W0516 19:33:17.370376   37236 genericapiserver.go:767] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: W0516 19:33:18.140326   37236 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA' detected.
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: W0516 19:33:18.140370   37236 secure_serving.go:69] Use of insecure cipher 'TLS_RSA_WITH_3DES_EDE_CBC_SHA' detected.
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: W0516 19:33:18.140380   37236 secure_serving.go:69] Use of insecure cipher 'TLS_RSA_WITH_AES_128_CBC_SHA' detected.
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: W0516 19:33:18.140391   37236 secure_serving.go:69] Use of insecure cipher 'TLS_RSA_WITH_AES_128_GCM_SHA256' detected.
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: W0516 19:33:18.140398   37236 secure_serving.go:69] Use of insecure cipher 'TLS_RSA_WITH_AES_256_CBC_SHA' detected.
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: W0516 19:33:18.140406   37236 secure_serving.go:69] Use of insecure cipher 'TLS_RSA_WITH_AES_256_GCM_SHA384' detected.
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.141502   37236 secure_serving.go:213] Serving securely on [::]:6443
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.142622   37236 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.142690   37236 tlsconfig.go:243] "Starting DynamicServingCertificateController"
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.142623   37236 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/client-ca.crt"
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.142841   37236 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.142973   37236 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.142997   37236 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143023   37236 aggregator.go:169] waiting for initial CRD sync...
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143031   37236 remote_available_controller.go:411] Starting RemoteAvailability controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143046   37236 apiservice_controller.go:100] Starting APIServiceRegistrationController
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143062   37236 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143171   37236 controller.go:119] Starting legacy_token_tracking_controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143191   37236 shared_informer.go:313] Waiting for caches to sync for configmaps
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143229   37236 controller.go:80] Starting OpenAPI V3 AggregationController
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143568   37236 crdregistration_controller.go:114] Starting crd-autoregister controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143586   37236 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.144214   37236 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.148410   37236 customresource_discovery_controller.go:292] Starting DiscoveryController
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.148487   37236 local_available_controller.go:156] Starting LocalAvailability controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.148503   37236 cache.go:32] Waiting for caches to sync for LocalAvailability controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.153003   37236 gc_controller.go:78] Starting apiserver lease garbage collector
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.153082   37236 system_namespaces_controller.go:66] Starting system namespaces controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.153130   37236 controller.go:78] Starting OpenAPI AggregationController
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.153208   37236 apf_controller.go:377] Starting API Priority and Fairness config controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.153291   37236 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/client-ca.crt"
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.153401   37236 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.143051   37236 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.161065   37236 controller.go:142] Starting OpenAPI controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.161127   37236 controller.go:90] Starting OpenAPI V3 controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.161162   37236 naming_controller.go:294] Starting NamingConditionController
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.161190   37236 establishing_controller.go:81] Starting EstablishingController
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.161215   37236 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.161238   37236 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.161260   37236 crd_finalizer.go:269] Starting CRDFinalizer
2025-05-16T19:33:18-04:00 k8s.kube-controller-manager[37279]: Error from server (Forbidden): forbidden: User "system:node:research21" cannot get path "/readyz"
2025-05-16T19:33:18-04:00 k8s.kube-controller-manager[36331]: Waiting for kube-apiserver to start
2025-05-16T19:33:18-04:00 k8s.kube-scheduler[37272]: Error from server (Forbidden): forbidden: User "system:node:research21" cannot get path "/readyz"
2025-05-16T19:33:18-04:00 k8s.kube-scheduler[36811]: Waiting for kube-apiserver to start
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.248541   37236 cache.go:39] Caches are synced for LocalAvailability controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.252391   37236 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.252442   37236 cache.go:39] Caches are synced for APIServiceRegistrationController controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.253335   37236 apf_controller.go:382] Running API Priority and Fairness config worker
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.253362   37236 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.253863   37236 cache.go:39] Caches are synced for RemoteAvailability controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.253914   37236 handler_discovery.go:451] Starting ResourceDiscoveryManager
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.255861   37236 shared_informer.go:320] Caches are synced for crd-autoregister
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.255904   37236 aggregator.go:171] initial CRD sync complete...
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.255918   37236 autoregister_controller.go:144] Starting autoregister controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.255928   37236 cache.go:32] Waiting for caches to sync for autoregister controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.255940   37236 cache.go:39] Caches are synced for autoregister controller
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.257330   37236 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.257361   37236 policy_source.go:240] refreshing policies
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.271315   37236 controller.go:615] quota admission added evaluator for: namespaces
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.327550   37236 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.335407   37236 shared_informer.go:320] Caches are synced for node_authorizer
2025-05-16T19:33:18-04:00 k8s.kube-apiserver[37236]: I0516 19:33:18.343279   37236 shared_informer.go:320] Caches are synced for configmaps
2025-05-16T19:33:18-04:00 k8s.kube-proxy[37289]: Error from server (Forbidden): forbidden: User "system:node:research21" cannot get path "/readyz"
2025-05-16T19:33:18-04:00 k8s.kube-proxy[36626]: Waiting for kube-apiserver to start
2025-05-16T19:33:18-04:00 k8s.kubelet[37295]: Error from server (Forbidden): forbidden: User "system:node:research21" cannot get path "/readyz"
2025-05-16T19:33:18-04:00 k8s.kubelet[37018]: Waiting for kube-apiserver to start
2025-05-16T19:33:19-04:00 k8s.kube-apiserver[37236]: I0516 19:33:19.184634   37236 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
2025-05-16T19:33:19-04:00 k8s.kube-apiserver[37236]: I0516 19:33:19.194433   37236 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
2025-05-16T19:33:19-04:00 k8s.kube-apiserver[37236]: I0516 19:33:19.194481   37236 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
2025-05-16T19:33:20-04:00 k8s.kube-apiserver[37236]: I0516 19:33:20.147600   37236 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
2025-05-16T19:33:20-04:00 k8s.kube-apiserver[37236]: I0516 19:33:20.220323   37236 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
2025-05-16T19:33:20-04:00 k8s.kube-apiserver[37236]: I0516 19:33:20.378933   37236 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.152.183.1"}
2025-05-16T19:33:20-04:00 k8s.kube-apiserver[37236]: W0516 19:33:20.392911   37236 lease.go:265] Resetting endpoints for master service "kubernetes" to [172.24.188.57]
2025-05-16T19:33:20-04:00 k8s.kube-apiserver[37236]: I0516 19:33:20.395029   37236 controller.go:615] quota admission added evaluator for: endpoints
2025-05-16T19:33:20-04:00 k8s.kube-apiserver[37236]: I0516 19:33:20.407446   37236 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
2025-05-16T19:33:21-04:00 k8s.k8sd[35160]: I0516 19:33:21.236071   35160 app/hooks_bootstrap.go:511] "API server is ready - notify controllers" logger="k8sd" hook="bootstrap"
2025-05-16T19:33:21-04:00 k8s.k8sd[35160]: I0516 19:33:21.240661   35160 controllers/node_configuration.go:53] "Starting node configuration controller" logger="k8sd" controller="node-configuration"
2025-05-16T19:33:21-04:00 k8s.k8sd[35160]: I0516 19:33:21.250914   35160 leaderelection/leaderelection.go:254] attempting to acquire leader lease kube-system/a27980c4.k8sd-csrsigning-controller...
2025-05-16T19:33:21-04:00 k8s.k8sd[35160]: I0516 19:33:21.290886   35160 leaderelection/leaderelection.go:268] successfully acquired lease kube-system/a27980c4.k8sd-csrsigning-controller
2025-05-16T19:33:21-04:00 k8s.k8sd[35160]: I0516 19:33:21.295298   35160 controller/controller.go:175] "Starting EventSource" logger="k8sd.csrsigning" controller="certificatesigningrequest" controllerGroup="certificates.k8s.io" controllerKind="CertificateSigningRequest" source="kind source: *v1.CertificateSigningRequest"
2025-05-16T19:33:21-04:00 k8s.k8sd[35160]: I0516 19:33:21.295983   35160 controller/controller.go:183] "Starting Controller" logger="k8sd.csrsigning" controller="certificatesigningrequest" controllerGroup="certificates.k8s.io" controllerKind="CertificateSigningRequest"
2025-05-16T19:33:21-04:00 k8s.k8sd[35160]: I0516 19:33:21.419050   35160 controller/controller.go:217] "Starting workers" logger="k8sd.csrsigning" controller="certificatesigningrequest" controllerGroup="certificates.k8s.io" controllerKind="CertificateSigningRequest" worker count=1
2025-05-16T19:33:21-04:00 k8s.kube-scheduler[36811]: + ulimit -c unlimited
2025-05-16T19:33:21-04:00 k8s.kube-scheduler[36811]: + export GOTRACEBACK=crash
2025-05-16T19:33:21-04:00 k8s.kube-scheduler[36811]: + GOTRACEBACK=crash
2025-05-16T19:33:21-04:00 k8s.kube-scheduler[36811]: + [[ -f /var/snap/k8s/common/args/kube-scheduler-env ]]
2025-05-16T19:33:21-04:00 k8s.kube-scheduler[36811]: + exec /snap/k8s/2500/bin/kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect-lease-duration=30s --leader-elect-renew-deadline=15s --profiling=false --tls-min-version=VersionTLS12
2025-05-16T19:33:21-04:00 k8s.kube-controller-manager[36331]: + ulimit -c unlimited
2025-05-16T19:33:21-04:00 k8s.kube-controller-manager[36331]: + export GOTRACEBACK=crash
2025-05-16T19:33:21-04:00 k8s.kube-controller-manager[36331]: + GOTRACEBACK=crash
2025-05-16T19:33:21-04:00 k8s.kube-controller-manager[36331]: + [[ -f /var/snap/k8s/common/args/kube-controller-manager-env ]]
2025-05-16T19:33:21-04:00 k8s.kube-controller-manager[36331]: + exec /snap/k8s/2500/bin/kube-controller-manager --authentication-kubeconfig=/etc/kubernetes/controller.conf --authorization-kubeconfig=/etc/kubernetes/controller.conf --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --kubeconfig=/etc/kubernetes/controller.conf --leader-elect-lease-duration=30s --leader-elect-renew-deadline=15s --profiling=false --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/serviceaccount.key --terminated-pod-gc-threshold=12500 --tls-min-version=VersionTLS12 --use-service-account-credentials=true
2025-05-16T19:33:21-04:00 k8s.kube-proxy[36626]: Kernel module nf_conntrack was already loaded before kube-proxy startup.
2025-05-16T19:33:21-04:00 k8s.kube-proxy[36626]: + ulimit -c unlimited
2025-05-16T19:33:21-04:00 k8s.kube-proxy[36626]: + export GOTRACEBACK=crash
2025-05-16T19:33:21-04:00 k8s.kube-proxy[36626]: + GOTRACEBACK=crash
2025-05-16T19:33:21-04:00 k8s.kube-proxy[36626]: + [[ -f /var/snap/k8s/common/args/kube-proxy-env ]]
2025-05-16T19:33:21-04:00 k8s.kube-proxy[36626]: + exec /snap/k8s/2500/bin/kube-proxy --cluster-cidr=10.1.0.0/16 --healthz-bind-address=127.0.0.1:10256 --kubeconfig=/etc/kubernetes/proxy.conf --profiling=false
2025-05-16T19:33:21-04:00 k8s.kubelet[37018]: + ulimit -c unlimited
2025-05-16T19:33:21-04:00 k8s.kubelet[37018]: + export GOTRACEBACK=crash
2025-05-16T19:33:21-04:00 k8s.kubelet[37018]: + GOTRACEBACK=crash
2025-05-16T19:33:21-04:00 k8s.kubelet[37018]: + [[ -f /var/snap/k8s/common/args/kubelet-env ]]
2025-05-16T19:33:21-04:00 k8s.kubelet[37018]: + exec /snap/k8s/2500/bin/kubelet --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/etc/kubernetes/pki/client-ca.crt --cluster-domain=cluster.local --container-runtime-endpoint=/run/containerd/containerd.sock --containerd=/run/containerd/containerd.sock '--eviction-hard=memory.available<100Mi,nodefs.available<1Gi,imagefs.available<1Gi' --fail-swap-on=false --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=172.24.188.57 --node-labels=node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/worker=,k8sd.io/role=control-plane --read-only-port=0 --register-with-taints= --root-dir=/var/lib/kubelet --serialize-image-pulls=false --tls-cert-file=/etc/kubernetes/pki/kubelet.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-private-key-file=/etc/kubernetes/pki/kubelet.key
2025-05-16T19:33:21-04:00 k8s.kube-proxy[36626]: I0516 19:33:21.989269   36626 server_linux.go:66] "Using iptables proxy"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.050070   37018 resolv_conf.go:34] Using /run/systemd/resolve/resolv.conf for the DNS resolver config
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --eviction-hard has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --read-only-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --register-with-taints has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --serialize-image-pulls has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.052057   37018 server.go:1170] "Use of insecure cipher detected." cipher="TLS_RSA_WITH_AES_128_GCM_SHA256"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.052082   37018 server.go:1170] "Use of insecure cipher detected." cipher="TLS_RSA_WITH_AES_256_GCM_SHA384"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.086769   37018 server.go:520] "Kubelet version" kubeletVersion="v1.32.2"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.086836   37018 server.go:522] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK="crash"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.096938   37018 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/client-ca.crt"
2025-05-16T19:33:22-04:00 k8s.kube-proxy[36626]: E0516 19:33:22.127142   36626 server.go:687] "Failed to retrieve node info" err="nodes \"research21\" not found"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: E0516 19:33:22.141874   37018 log.go:32] "RuntimeConfig from runtime service failed" err="rpc error: code = Unimplemented desc = unknown method RuntimeConfig for service runtime.v1.RuntimeService"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.141988   37018 server.go:1421] "CRI implementation should be updated to support RuntimeConfig when KubeletCgroupDriverFromCRI feature gate has been enabled. Falling back to using cgroupDriver from kubelet config."
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.164961   37018 server.go:772] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.165034   37018 server.go:841] "NoSwap is set due to memorySwapBehavior not specified" memorySwapBehavior="" FailSwapOn=false
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.165219   37018 swap_util.go:115] "Swap is on" /proc/swaps contents=<
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: 	Filename				Type		Size		Used		Priority
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: 	/swap.img                               file		8388604		0		-2
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]:  >
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.165559   37018 container_manager_linux.go:268] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.165596   37018 container_manager_linux.go:273] "Creating Container Manager object based on Node Config" nodeConfig={"NodeName":"research21","RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"memory.available","Operator":"LessThan","Value":{"Quantity":"100Mi","Percentage":0},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":"1Gi","Percentage":0},"GracePeriod":0,"MinReclaim":null},{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":"1Gi","Percentage":0},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null,"CgroupVersion":2}
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.165979   37018 topology_manager.go:138] "Creating topology manager with none policy"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.166000   37018 container_manager_linux.go:304] "Creating device plugin manager"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.166176   37018 state_mem.go:36] "Initialized new in-memory state store"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.173552   37018 kubelet.go:446] "Attempting to sync node with API server"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.174987   37018 kubelet.go:352] "Adding apiserver pod source"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.175049   37018 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.177500   37018 kuberuntime_manager.go:269] "Container runtime initialized" containerRuntime="containerd" version="v1.6.36" apiVersion="v1"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.178232   37018 kubelet.go:890] "Not starting ClusterTrustBundle informer because we are in static kubelet mode"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: W0516 19:33:22.178341   37018 probe.go:272] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.193132   37018 watchdog_linux.go:99] "Systemd watchdog is not enabled"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.193236   37018 server.go:1287] "Started kubelet"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.198550   37018 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.199025   37018 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100 burstTokens=10
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.199434   37018 server.go:243] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.199533   37018 server.go:169] "Starting to listen" address="0.0.0.0" port=10250
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.224354   37018 dynamic_serving_content.go:135] "Starting controller" name="kubelet-server-cert-files::/etc/kubernetes/pki/kubelet.crt::/etc/kubernetes/pki/kubelet.key"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.227144   37018 volume_manager.go:297] "Starting Kubelet Volume Manager"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.227184   37018 server.go:479] "Adding debug handlers to kubelet server"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.227642   37018 desired_state_of_world_populator.go:149] "Desired state populator starts to run"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: E0516 19:33:22.228492   37018 kubelet_node_status.go:467] "Error getting the current node from lister" err="node \"research21\" not found"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.265991   37018 reconciler.go:26] "Reconciler: start to sync state"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.266635   37018 factory.go:221] Registration of the containerd container factory successfully
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.266659   37018 factory.go:221] Registration of the systemd container factory successfully
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.266821   37018 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: E0516 19:33:22.309637   37018 kubelet.go:1561] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: E0516 19:33:22.333141   37018 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"research21\" not found" node="research21"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: E0516 19:33:22.367401   37018 kubelet_node_status.go:467] "Error getting the current node from lister" err="node \"research21\" not found"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.428741   37018 cpu_manager.go:221] "Starting CPU manager" policy="none"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.428772   37018 cpu_manager.go:222] "Reconciling" reconcilePeriod="10s"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.428891   37018 state_mem.go:36] "Initialized new in-memory state store"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.432223   37018 policy_none.go:49] "None policy: Start"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.432256   37018 memory_manager.go:186] "Starting memorymanager" policy="None"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.432280   37018 state_mem.go:35] "Initializing new in-memory state store"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: E0516 19:33:22.467875   37018 kubelet_node_status.go:467] "Error getting the current node from lister" err="node \"research21\" not found"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.491357   37018 manager.go:519] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.491628   37018 eviction_manager.go:189] "Eviction manager: starting control loop"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.491661   37018 container_log_manager.go:189] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.494283   37018 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: E0516 19:33:22.496857   37018 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: E0516 19:33:22.496948   37018 eviction_manager.go:292] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"research21\" not found"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.590264   37018 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.600704   37018 kubelet_node_status.go:76] "Attempting to register node" node="research21"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.636784   37018 kubelet_node_status.go:79] "Successfully registered node" node="research21"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.665435   37018 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.665694   37018 status_manager.go:227] "Starting to sync pod status with apiserver"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.667477   37018 watchdog_linux.go:127] "Systemd watchdog is not enabled or the interval is invalid, so health checking will not be started."
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: I0516 19:33:22.667504   37018 kubelet.go:2388] "Starting kubelet main sync loop"
2025-05-16T19:33:22-04:00 k8s.kubelet[37018]: E0516 19:33:22.674027   37018 kubelet.go:2412] "Skipping pod synchronization" err="PLEG is not healthy: pleg has yet to be successful"
2025-05-16T19:33:22-04:00 k8s.k8sd[35160]: E0516 19:33:22.951595   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to delete Gateway API cilium configuration: cannot upgrade ck-network as it is not installed" logger="k8sd" controller="feature" feature="gateway"
2025-05-16T19:33:23-04:00 k8s.k8sd[35160]: E0516 19:33:23.036283   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: cannot upgrade ck-network as it is not installed" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.137711   36626 server.go:698] "Successfully retrieved node IP(s)" IPs=["172.24.188.57"]
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.151532   36626 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_max" value=131072
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.151650   36626 conntrack.go:60] "Setting nf_conntrack_max" nfConntrackMax=131072
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.151822   36626 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_established" value=86400
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.151964   36626 conntrack.go:121] "Set sysctl" entry="net/netfilter/nf_conntrack_tcp_timeout_close_wait" value=3600
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: E0516 19:33:23.152050   36626 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.169582   36626 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.169674   36626 server_linux.go:170] "Using iptables Proxier"
2025-05-16T19:33:23-04:00 k8s.kubelet[37018]: I0516 19:33:23.175931   37018 apiserver.go:52] "Watching apiserver"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.179343   36626 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.191611   36626 server.go:497] "Version info" version="v1.32.2"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.191653   36626 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK="crash"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.201348   36626 config.go:199] "Starting service config controller"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.206519   36626 shared_informer.go:313] Waiting for caches to sync for service config
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.202391   36626 config.go:105] "Starting endpoint slice config controller"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.206931   36626 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.202950   36626 config.go:329] "Starting node config controller"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.206978   36626 shared_informer.go:313] Waiting for caches to sync for node config
2025-05-16T19:33:23-04:00 k8s.kubelet[37018]: I0516 19:33:23.266441   37018 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.307375   36626 shared_informer.go:320] Caches are synced for node config
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.307407   36626 shared_informer.go:320] Caches are synced for service config
2025-05-16T19:33:23-04:00 k8s.kube-proxy[36626]: I0516 19:33:23.307432   36626 shared_informer.go:320] Caches are synced for endpoint slice config
2025-05-16T19:33:23-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:23.577344   36331 serving.go:386] Generated self-signed cert in-memory
2025-05-16T19:33:24-04:00 k8s.kube-scheduler[36811]: I0516 19:33:24.693055   36811 serving.go:386] Generated self-signed cert in-memory
2025-05-16T19:33:24-04:00 k8s.k8sd[35160]: I0516 19:33:24.766674   35160 helm/client.go:40] "creating 1 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:24-04:00 k8s.k8sd[35160]: I0516 19:33:24.920250   35160 helm/client.go:40] "creating 5 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]: I0516 19:33:25.055046   37236 alloc.go:330] "allocated clusterIPs" service="kube-system/coredns" clusterIPs={"IPv4":"10.152.183.178"}
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]: I0516 19:33:25.107191   37236 controller.go:615] quota admission added evaluator for: deployments.apps
2025-05-16T19:33:25-04:00 k8s.k8sd[35160]: I0516 19:33:25.427488   35160 helm/client.go:40] "creating 1 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:25-04:00 k8s.k8sd[35160]: I0516 19:33:25.541128   35160 helm/client.go:40] "creating 9 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]: I0516 19:33:25.543498   37236 controller.go:615] quota admission added evaluator for: serviceaccounts
2025-05-16T19:33:25-04:00 k8s.k8sd[35160]: I0516 19:33:25.577113   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:25-04:00 k8s.k8sd[35160]: I0516 19:33:25.615310   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]: I0516 19:33:25.674469   37236 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.152.183.156"}
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]: I0516 19:33:25.768301   37236 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]: W0516 19:33:25.802726   37236 handler_proxy.go:99] no RequestInfo found in the context
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]: E0516 19:33:25.805224   37236 controller.go:146] "Unhandled Error" err=<
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]: 	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]: 	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
2025-05-16T19:33:25-04:00 k8s.kube-apiserver[37236]:  > logger="UnhandledError"
2025-05-16T19:33:26-04:00 k8s.k8sd[35160]: I0516 19:33:26.055198   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:26-04:00 systemd[1]: Stopping snap.k8s.kubelet.service - Service for snap application k8s.kubelet...
2025-05-16T19:33:26-04:00 k8s.k8sd[35160]: I0516 19:33:26.112537   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:26-04:00 systemd[1]: snap.k8s.kubelet.service: Deactivated successfully.
2025-05-16T19:33:26-04:00 systemd[1]: Stopped snap.k8s.kubelet.service - Service for snap application k8s.kubelet.
2025-05-16T19:33:26-04:00 systemd[1]: snap.k8s.kubelet.service: Consumed 2.352s CPU time, 49.7M memory peak, 0B memory swap peak.
2025-05-16T19:33:26-04:00 systemd[1]: Started snap.k8s.kubelet.service - Service for snap application k8s.kubelet.
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.376363   36811 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.2"
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.376411   36811 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK="crash"
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.384721   36811 secure_serving.go:213] Serving securely on [::]:10259
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.384874   36811 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.384912   36811 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.384954   36811 tlsconfig.go:243] "Starting DynamicServingCertificateController"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.390540   36331 controllermanager.go:185] "Starting" version="v1.32.2"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.390581   36331 controllermanager.go:187] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK="crash"
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.397662   36811 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.397708   36811 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.397756   36811 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.397772   36811 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.420689   36331 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.420758   36331 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.420824   36331 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.420847   36331 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.420878   36331 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.420895   36331 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.422358   36331 secure_serving.go:213] Serving securely on [::]:10257
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.423039   36331 leaderelection.go:257] attempting to acquire leader lease kube-system/kube-controller-manager...
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.423302   36331 tlsconfig.go:243] "Starting DynamicServingCertificateController"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.428784   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.430071   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.434363   36331 type.go:183] The watchlist request for configmaps ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.435789   36331 type.go:183] The watchlist request for configmaps ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.436035   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.442186   36331 type.go:183] The watchlist request for configmaps ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.464106   36331 leaderelection.go:271] successfully acquired lease kube-system/kube-controller-manager
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.466033   36331 event.go:389] "Event occurred" object="kube-system/kube-controller-manager" fieldPath="" kind="Lease" apiVersion="coordination.k8s.io/v1" type="Normal" reason="LeaderElection" message="research21_c2ea56ed-4929-4bc9-b09d-1f8c48f2681b became leader"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.481530   36331 shared_informer.go:313] Waiting for caches to sync for tokens
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.484959   36811 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.484976   36811 leaderelection.go:257] attempting to acquire leader lease kube-system/kube-scheduler...
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.482438   36331 controllermanager.go:765] "Started controller" controller="serviceaccount-token-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.496763   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.498301   36811 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.498598   36811 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.504091   36331 type.go:183] The watchlist request for secrets ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.504400   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:26.508234   36331 type.go:183] The watchlist request for serviceaccounts ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:26-04:00 k8s.kube-scheduler[36811]: I0516 19:33:26.517821   36811 leaderelection.go:271] successfully acquired lease kube-system/kube-scheduler
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.521802   36331 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.521880   36331 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.522013   36331 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.555310   36331 controllermanager.go:765] "Started controller" controller="serviceaccount-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.555628   36331 serviceaccounts_controller.go:114] "Starting service account controller" logger="serviceaccount-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.555651   36331 shared_informer.go:313] Waiting for caches to sync for service account
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.582266   36331 shared_informer.go:320] Caches are synced for tokens
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.597221   36331 controllermanager.go:765] "Started controller" controller="garbage-collector-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.597679   36331 controllermanager.go:728] "Warning: controller is disabled" controller="bootstrap-signer-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.597494   36331 garbagecollector.go:144] "Starting controller" logger="garbage-collector-controller" controller="garbagecollector"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.598143   36331 shared_informer.go:313] Waiting for caches to sync for garbage collector
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.598185   36331 graph_builder.go:351] "Running" logger="garbage-collector-controller" component="GraphBuilder"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.641289   36331 node_lifecycle_controller.go:432] "Controller will reconcile labels" logger="node-lifecycle-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.641357   36331 controllermanager.go:765] "Started controller" controller="node-lifecycle-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.641682   36331 node_lifecycle_controller.go:466] "Sending events to api server" logger="node-lifecycle-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.641733   36331 node_lifecycle_controller.go:477] "Starting node controller" logger="node-lifecycle-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.641751   36331 shared_informer.go:313] Waiting for caches to sync for taint
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.675372   36331 controllermanager.go:765] "Started controller" controller="persistentvolume-expander-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.675641   36331 expand_controller.go:329] "Starting expand controller" logger="persistentvolume-expander-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.675663   36331 shared_informer.go:313] Waiting for caches to sync for expand
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.679355   36331 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.697379   36331 controllermanager.go:765] "Started controller" controller="endpointslice-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.697770   36331 endpointslice_controller.go:281] "Starting endpoint slice controller" logger="endpointslice-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.697795   36331 shared_informer.go:313] Waiting for caches to sync for endpoint_slice
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.716821   36331 controllermanager.go:765] "Started controller" controller="replicationcontroller-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.717211   36331 replica_set.go:217] "Starting controller" logger="replicationcontroller-controller" name="replicationcontroller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.717239   36331 shared_informer.go:313] Waiting for caches to sync for ReplicationController
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.747668   36331 controllermanager.go:765] "Started controller" controller="persistentvolume-attach-detach-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.747779   36331 attach_detach_controller.go:338] "Starting attach detach controller" logger="persistentvolume-attach-detach-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.747906   36331 shared_informer.go:313] Waiting for caches to sync for attach detach
2025-05-16T19:33:26-04:00 k8s.kube-apiserver[37236]: W0516 19:33:26.770507   37236 handler_proxy.go:99] no RequestInfo found in the context
2025-05-16T19:33:26-04:00 k8s.kube-apiserver[37236]: E0516 19:33:26.770575   37236 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
2025-05-16T19:33:26-04:00 k8s.kube-apiserver[37236]: W0516 19:33:26.770637   37236 handler_proxy.go:99] no RequestInfo found in the context
2025-05-16T19:33:26-04:00 k8s.kube-apiserver[37236]: E0516 19:33:26.770717   37236 controller.go:102] "Unhandled Error" err=<
2025-05-16T19:33:26-04:00 k8s.kube-apiserver[37236]: 	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
2025-05-16T19:33:26-04:00 k8s.kube-apiserver[37236]: 	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
2025-05-16T19:33:26-04:00 k8s.kube-apiserver[37236]:  > logger="UnhandledError"
2025-05-16T19:33:26-04:00 k8s.kube-apiserver[37236]: I0516 19:33:26.771714   37236 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
2025-05-16T19:33:26-04:00 k8s.kube-apiserver[37236]: I0516 19:33:26.771752   37236 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.789095   36331 controllermanager.go:765] "Started controller" controller="legacy-serviceaccount-token-cleaner-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.789283   36331 legacy_serviceaccount_token_cleaner.go:103] "Starting legacy service account token cleaner controller" logger="legacy-serviceaccount-token-cleaner-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.789307   36331 shared_informer.go:313] Waiting for caches to sync for legacy-service-account-token-cleaner
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.825148   36331 controllermanager.go:765] "Started controller" controller="clusterrole-aggregation-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.825650   36331 clusterroleaggregation_controller.go:194] "Starting ClusterRoleAggregator controller" logger="clusterrole-aggregation-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.826019   36331 shared_informer.go:313] Waiting for caches to sync for ClusterRoleAggregator
2025-05-16T19:33:26-04:00 k8s.kubelet[38157]: + ulimit -c unlimited
2025-05-16T19:33:26-04:00 k8s.kubelet[38157]: + export GOTRACEBACK=crash
2025-05-16T19:33:26-04:00 k8s.kubelet[38157]: + GOTRACEBACK=crash
2025-05-16T19:33:26-04:00 k8s.kubelet[38157]: + [[ -f /var/snap/k8s/common/args/kubelet-env ]]
2025-05-16T19:33:26-04:00 k8s.kubelet[38157]: + exec /snap/k8s/2500/bin/kubelet --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=systemd --client-ca-file=/etc/kubernetes/pki/client-ca.crt --cluster-dns=10.152.183.178 --cluster-domain=cluster.local --container-runtime-endpoint=/run/containerd/containerd.sock --containerd=/run/containerd/containerd.sock '--eviction-hard=memory.available<100Mi,nodefs.available<1Gi,imagefs.available<1Gi' --fail-swap-on=false --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=172.24.188.57 --node-labels=node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/worker=,k8sd.io/role=control-plane --read-only-port=0 --register-with-taints= --root-dir=/var/lib/kubelet --serialize-image-pulls=false --tls-cert-file=/etc/kubernetes/pki/kubelet.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384 --tls-private-key-file=/etc/kubernetes/pki/kubelet.key
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.857024   36331 controllermanager.go:765] "Started controller" controller="persistentvolume-protection-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.857070   36331 controllermanager.go:717] "Controller is disabled by a feature gate" controller="kube-apiserver-serving-clustertrustbundle-publisher-controller" requiredFeatureGates=["ClusterTrustBundle"]
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.857349   36331 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.857372   36331 shared_informer.go:313] Waiting for caches to sync for PV protection
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.864925   36331 controllermanager.go:765] "Started controller" controller="taint-eviction-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.864965   36331 controllermanager.go:717] "Controller is disabled by a feature gate" controller="selinux-warning-controller" requiredFeatureGates=["SELinuxChangePolicy"]
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.864992   36331 controllermanager.go:723] "Skipping a cloud provider controller" controller="cloud-node-lifecycle-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.865228   36331 taint_eviction.go:281] "Starting" logger="taint-eviction-controller" controller="taint-eviction-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.865273   36331 taint_eviction.go:287] "Sending events to api server" logger="taint-eviction-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.865332   36331 shared_informer.go:313] Waiting for caches to sync for taint-eviction-controller
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.941758   36331 controllermanager.go:765] "Started controller" controller="ttl-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.942210   36331 ttl_controller.go:127] "Starting TTL controller" logger="ttl-controller"
2025-05-16T19:33:26-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:26.942737   36331 shared_informer.go:313] Waiting for caches to sync for TTL
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.079817   36331 controllermanager.go:765] "Started controller" controller="ttl-after-finished-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.079949   36331 ttlafterfinished_controller.go:112] "Starting TTL after finished controller" logger="ttl-after-finished-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.079974   36331 shared_informer.go:313] Waiting for caches to sync for TTL after finished
2025-05-16T19:33:27-04:00 k8s.k8sd[35160]: I0516 19:33:27.085720   35160 helm/client.go:40] "creating 1 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.089088   38157 resolv_conf.go:34] Using /run/systemd/resolve/resolv.conf for the DNS resolver config
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --cluster-dns has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --containerd has been deprecated, This is a cadvisor flag that was mistakenly registered with the Kubelet. Due to legacy concerns, it will follow the standard CLI deprecation timeline before being removed.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --eviction-hard has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --read-only-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --register-with-taints has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --serialize-image-pulls has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --tls-cipher-suites has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.097581   38157 server.go:1170] "Use of insecure cipher detected." cipher="TLS_RSA_WITH_AES_128_GCM_SHA256"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.097621   38157 server.go:1170] "Use of insecure cipher detected." cipher="TLS_RSA_WITH_AES_256_GCM_SHA384"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.119878   38157 server.go:520] "Kubelet version" kubeletVersion="v1.32.2"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.120142   38157 server.go:522] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK="crash"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.135076   38157 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/client-ca.crt"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: E0516 19:33:27.147496   38157 log.go:32] "RuntimeConfig from runtime service failed" err="rpc error: code = Unimplemented desc = unknown method RuntimeConfig for service runtime.v1.RuntimeService"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.147847   38157 server.go:1421] "CRI implementation should be updated to support RuntimeConfig when KubeletCgroupDriverFromCRI feature gate has been enabled. Falling back to using cgroupDriver from kubelet config."
2025-05-16T19:33:27-04:00 k8s.k8sd[35160]: I0516 19:33:27.160024   35160 helm/client.go:40] "creating 15 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.167819   38157 server.go:772] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.167938   38157 server.go:841] "NoSwap is set due to memorySwapBehavior not specified" memorySwapBehavior="" FailSwapOn=false
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.168168   38157 swap_util.go:115] "Swap is on" /proc/swaps contents=<
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: 	Filename				Type		Size		Used		Priority
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: 	/swap.img                               file		8388604		0		-2
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]:  >
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.168815   38157 container_manager_linux.go:268] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.168864   38157 container_manager_linux.go:273] "Creating Container Manager object based on Node Config" nodeConfig={"NodeName":"research21","RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"memory.available","Operator":"LessThan","Value":{"Quantity":"100Mi","Percentage":0},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":"1Gi","Percentage":0},"GracePeriod":0,"MinReclaim":null},{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":"1Gi","Percentage":0},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null,"CgroupVersion":2}
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.169228   38157 topology_manager.go:138] "Creating topology manager with none policy"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.169250   38157 container_manager_linux.go:304] "Creating device plugin manager"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.169345   38157 state_mem.go:36] "Initialized new in-memory state store"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.171278   38157 kubelet.go:446] "Attempting to sync node with API server"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.171501   38157 kubelet.go:352] "Adding apiserver pod source"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.171642   38157 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.174958   38157 kuberuntime_manager.go:269] "Container runtime initialized" containerRuntime="containerd" version="v1.6.36" apiVersion="v1"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.175715   38157 kubelet.go:890] "Not starting ClusterTrustBundle informer because we are in static kubelet mode"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.180111   38157 watchdog_linux.go:99] "Systemd watchdog is not enabled"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.180174   38157 server.go:1287] "Started kubelet"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.183032   38157 server.go:169] "Starting to listen" address="0.0.0.0" port=10250
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.185757   38157 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100 burstTokens=10
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.192543   38157 server.go:243] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.202222   38157 server.go:479] "Adding debug handlers to kubelet server"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.215682   38157 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.234178   36331 controllermanager.go:765] "Started controller" controller="certificatesigningrequest-approving-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.234275   36331 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-approving-controller" name="csrapproving"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.234312   36331 shared_informer.go:313] Waiting for caches to sync for certificate-csrapproving
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: E0516 19:33:27.237571   38157 kubelet_node_status.go:467] "Error getting the current node from lister" err="node \"research21\" not found"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.240649   38157 dynamic_serving_content.go:135] "Starting controller" name="kubelet-server-cert-files::/etc/kubernetes/pki/kubelet.crt::/etc/kubernetes/pki/kubelet.key"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: E0516 19:33:27.245769   38157 kubelet.go:1561] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.255769   38157 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.259045   38157 volume_manager.go:297] "Starting Kubelet Volume Manager"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.259211   38157 desired_state_of_world_populator.go:149] "Desired state populator starts to run"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.259309   38157 reconciler.go:26] "Reconciler: start to sync state"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.266506   38157 factory.go:221] Registration of the containerd container factory successfully
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.266541   38157 factory.go:221] Registration of the systemd container factory successfully
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.338492   38157 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.347150   38157 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.347222   38157 status_manager.go:227] "Starting to sync pod status with apiserver"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.347291   38157 watchdog_linux.go:127] "Systemd watchdog is not enabled or the interval is invalid, so health checking will not be started."
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.347306   38157 kubelet.go:2388] "Starting kubelet main sync loop"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: E0516 19:33:27.347388   38157 kubelet.go:2412] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.384406   36331 controllermanager.go:765] "Started controller" controller="deployment-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.384612   36331 deployment_controller.go:173] "Starting controller" logger="deployment-controller" controller="deployment"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.384636   36331 shared_informer.go:313] Waiting for caches to sync for deployment
2025-05-16T19:33:27-04:00 k8s.kube-apiserver[37236]: I0516 19:33:27.402583   37236 alloc.go:330] "allocated clusterIPs" service="kube-system/ck-storage-rawfile-csi-node" clusterIPs={"IPv4":"10.152.183.57"}
2025-05-16T19:33:27-04:00 k8s.kube-apiserver[37236]: I0516 19:33:27.415087   37236 controller.go:615] quota admission added evaluator for: daemonsets.apps
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.433528   36331 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kubelet-serving"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.433586   36331 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-kubelet-serving
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.433624   36331 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.434209   36331 controllermanager.go:765] "Started controller" controller="certificatesigningrequest-signing-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.434243   36331 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kubelet-client"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.434269   36331 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-kubelet-client
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.434316   36331 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.434391   36331 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-legacy-unknown"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.434416   36331 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-legacy-unknown
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.434817   36331 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kube-apiserver-client"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.434841   36331 shared_informer.go:313] Waiting for caches to sync for certificate-csrsigning-kube-apiserver-client
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.434869   36331 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.435373   36331 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/etc/kubernetes/pki/ca.crt::/etc/kubernetes/pki/ca.key"
2025-05-16T19:33:27-04:00 k8s.kube-apiserver[37236]: I0516 19:33:27.444269   37236 controller.go:615] quota admission added evaluator for: statefulsets.apps
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.444939   38157 cpu_manager.go:221] "Starting CPU manager" policy="none"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.444962   38157 cpu_manager.go:222] "Reconciling" reconcilePeriod="10s"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.444996   38157 state_mem.go:36] "Initialized new in-memory state store"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.447032   38157 state_mem.go:88] "Updated default CPUSet" cpuSet=""
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.447064   38157 state_mem.go:96] "Updated CPUSet assignments" assignments={}
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.447100   38157 policy_none.go:49] "None policy: Start"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.447126   38157 memory_manager.go:186] "Starting memorymanager" policy="None"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.447151   38157 state_mem.go:35] "Initializing new in-memory state store"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: E0516 19:33:27.447872   38157 kubelet.go:2412] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.449378   38157 state_mem.go:75] "Updated machine memory state"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.463865   38157 manager.go:519] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.467256   38157 eviction_manager.go:189] "Eviction manager: starting control loop"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.467287   38157 container_log_manager.go:189] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.467880   38157 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: E0516 19:33:27.473352   38157 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.577031   36331 controllermanager.go:765] "Started controller" controller="persistentvolume-binder-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.577067   36331 controllermanager.go:717] "Controller is disabled by a feature gate" controller="resourceclaim-controller" requiredFeatureGates=["DynamicResourceAllocation"]
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.577087   36331 controllermanager.go:717] "Controller is disabled by a feature gate" controller="service-cidr-controller" requiredFeatureGates=["MultiCIDRServiceAllocator"]
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.577217   36331 pv_controller_base.go:308] "Starting persistent volume controller" logger="persistentvolume-binder-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.577238   36331 shared_informer.go:313] Waiting for caches to sync for persistent volume
2025-05-16T19:33:27-04:00 k8s.k8sd[35160]: I0516 19:33:27.579483   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.585784   38157 kubelet_node_status.go:76] "Attempting to register node" node="research21"
2025-05-16T19:33:27-04:00 k8s.k8sd[35160]: I0516 19:33:27.601580   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.612087   38157 kubelet_node_status.go:125] "Node was previously registered" node="research21"
2025-05-16T19:33:27-04:00 k8s.kubelet[38157]: I0516 19:33:27.612233   38157 kubelet_node_status.go:79] "Successfully registered node" node="research21"
2025-05-16T19:33:27-04:00 k8s.k8sd[35160]: I0516 19:33:27.662845   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.733845   36331 controllermanager.go:765] "Started controller" controller="job-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.734891   36331 job_controller.go:243] "Starting job controller" logger="job-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.739036   36331 shared_informer.go:313] Waiting for caches to sync for job
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.877169   36331 controllermanager.go:765] "Started controller" controller="daemonset-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.877320   36331 daemon_controller.go:294] "Starting daemon sets controller" logger="daemonset-controller"
2025-05-16T19:33:27-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:27.877343   36331 shared_informer.go:313] Waiting for caches to sync for daemon sets
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: E0516 19:33:28.027903   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to delete Gateway API cilium configuration: cannot upgrade ck-network as it is not installed" logger="k8sd" controller="feature" feature="gateway"
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: I0516 19:33:28.129390   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: E0516 19:33:28.143731   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: cannot upgrade ck-network as it is not installed" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.172573   36331 controllermanager.go:765] "Started controller" controller="horizontal-pod-autoscaler-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.172658   36331 horizontal.go:201] "Starting HPA controller" logger="horizontal-pod-autoscaler-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.172679   36331 shared_informer.go:313] Waiting for caches to sync for HPA
2025-05-16T19:33:28-04:00 k8s.kubelet[38157]: I0516 19:33:28.173034   38157 apiserver.go:52] "Watching apiserver"
2025-05-16T19:33:28-04:00 k8s.kubelet[38157]: I0516 19:33:28.259697   38157 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: I0516 19:33:28.313910   35160 helm/client.go:40] "creating 1 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.378898   36331 controllermanager.go:765] "Started controller" controller="disruption-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.379029   36331 disruption.go:452] "Sending events to api server." logger="disruption-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.379143   36331 disruption.go:463] "Starting disruption controller" logger="disruption-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.379165   36331 shared_informer.go:313] Waiting for caches to sync for disruption
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: I0516 19:33:28.380976   35160 helm/client.go:40] "creating 23 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.552634   36331 controllermanager.go:765] "Started controller" controller="statefulset-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.552839   36331 stateful_set.go:166] "Starting stateful set controller" logger="statefulset-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.552863   36331 shared_informer.go:313] Waiting for caches to sync for stateful set
2025-05-16T19:33:28-04:00 k8s.kube-apiserver[37236]: I0516 19:33:28.579098   37236 handler.go:286] Adding GroupVersion metallb.io v1beta1 to ResourceManager
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.581983   36331 controllermanager.go:765] "Started controller" controller="certificatesigningrequest-cleaner-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.582023   36331 controllermanager.go:728] "Warning: controller is disabled" controller="token-cleaner-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.582041   36331 controllermanager.go:717] "Controller is disabled by a feature gate" controller="storageversion-garbage-collector-controller" requiredFeatureGates=["APIServerIdentity","StorageVersionAPI"]
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.582116   36331 cleaner.go:83] "Starting CSR cleaner controller" logger="certificatesigningrequest-cleaner-controller"
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: I0516 19:33:28.587311   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:28-04:00 k8s.kube-apiserver[37236]: I0516 19:33:28.631740   37236 handler.go:286] Adding GroupVersion metallb.io v1beta1 to ResourceManager
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: I0516 19:33:28.657424   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:28-04:00 k8s.kube-apiserver[37236]: I0516 19:33:28.708772   37236 handler.go:286] Adding GroupVersion metallb.io v1beta1 to ResourceManager
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: I0516 19:33:28.720825   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:28-04:00 k8s.kube-apiserver[37236]: I0516 19:33:28.724896   37236 alloc.go:330] "allocated clusterIPs" service="metallb-system/metallb-webhook-service" clusterIPs={"IPv4":"10.152.183.205"}
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: I0516 19:33:28.769551   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:28-04:00 k8s.kube-apiserver[37236]: I0516 19:33:28.788592   37236 handler.go:286] Adding GroupVersion metallb.io v1beta1 to ResourceManager
2025-05-16T19:33:28-04:00 k8s.kube-apiserver[37236]: I0516 19:33:28.832871   37236 handler.go:286] Adding GroupVersion metallb.io v1beta1 to ResourceManager
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: I0516 19:33:28.856983   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:28-04:00 k8s.kube-apiserver[37236]: I0516 19:33:28.874105   37236 handler.go:286] Adding GroupVersion metallb.io v1beta1 to ResourceManager
2025-05-16T19:33:28-04:00 k8s.k8sd[35160]: I0516 19:33:28.904989   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:28-04:00 k8s.kube-apiserver[37236]: I0516 19:33:28.918223   37236 handler.go:286] Adding GroupVersion metallb.io v1beta1 to ResourceManager
2025-05-16T19:33:28-04:00 k8s.kube-apiserver[37236]: I0516 19:33:28.918337   37236 handler.go:286] Adding GroupVersion metallb.io v1beta2 to ResourceManager
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:28.920984   36331 resource_quota_controller.go:175] "Unhandled Error" err="initial discovery check failure, continuing and counting on future sync update: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.922237   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="rolebindings.rbac.authorization.k8s.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.922492   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="csistoragecapacities.storage.k8s.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.922724   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="l2advertisements.metallb.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.922928   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="horizontalpodautoscalers.autoscaling"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.923545   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="leases.coordination.k8s.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.923789   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="endpointslices.discovery.k8s.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.923979   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="bgpadvertisements.metallb.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924154   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="bfdprofiles.metallb.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924335   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="controllerrevisions.apps"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924531   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="ingresses.networking.k8s.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924585   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="roles.rbac.authorization.k8s.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924618   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="jobs.batch"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924719   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="servicel2statuses.metallb.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924764   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="ipaddresspools.metallb.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924801   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="podtemplates"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924894   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="cronjobs.batch"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924937   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="networkpolicies.networking.k8s.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924970   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="deployments.apps"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.924999   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="daemonsets.apps"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.925049   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="communities.metallb.io"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.925122   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="replicasets.apps"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:28.925160   36331 shared_informer.go:597] resyncPeriod 13h14m51.293463902s is smaller than resyncCheckPeriod 20h42m25.792578649s and the informer has already started. Changing it to 20h42m25.792578649s
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.925257   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="limitranges"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:28.925287   36331 shared_informer.go:597] resyncPeriod 16h14m54.793053304s is smaller than resyncCheckPeriod 20h42m25.792578649s and the informer has already started. Changing it to 20h42m25.792578649s
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.925346   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="serviceaccounts"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.925389   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="poddisruptionbudgets.policy"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.925440   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="endpoints"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.929625   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="statefulsets.apps"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.929684   36331 controllermanager.go:765] "Started controller" controller="resourcequota-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.929710   36331 controllermanager.go:743] "Warning: skipping controller" controller="storage-version-migrator-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.930063   36331 resource_quota_controller.go:300] "Starting resource quota controller" logger="resourcequota-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.930115   36331 shared_informer.go:313] Waiting for caches to sync for resource quota
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:28.930146   36331 resource_quota_monitor.go:308] "QuotaMonitor running" logger="resourcequota-controller"
2025-05-16T19:33:28-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:28.963130   36331 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.078410   36331 controllermanager.go:765] "Started controller" controller="validatingadmissionpolicy-status-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.078533   36331 shared_informer.go:313] Waiting for caches to sync for validatingadmissionpolicy-status
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.082991   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.156103   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.304443   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:29.336316   36331 namespaced_resources_deleter.go:162] "Unhandled Error" err="unable to get all supported resources from server: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.337557   36331 controllermanager.go:765] "Started controller" controller="namespace-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.337639   36331 namespace_controller.go:202] "Starting namespace controller" logger="namespace-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.337659   36331 shared_informer.go:313] Waiting for caches to sync for namespace
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.350880   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.360832   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.374435   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.387305   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.399905   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.407088   35160 helm/client.go:40] "creating 1 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.413903   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.434204   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.446487   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.460865   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.475764   36331 controllermanager.go:765] "Started controller" controller="replicaset-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.475906   36331 replica_set.go:217] "Starting controller" logger="replicaset-controller" name="replicaset"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.475929   36331 shared_informer.go:313] Waiting for caches to sync for ReplicaSet
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.502966   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.541902   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.542394   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.630146   36331 controllermanager.go:765] "Started controller" controller="cronjob-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.630188   36331 controllermanager.go:723] "Skipping a cloud provider controller" controller="service-lb-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.630332   36331 cronjob_controllerv2.go:145] "Starting cronjob controller v2" logger="cronjob-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.630354   36331 shared_informer.go:313] Waiting for caches to sync for cronjob
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.777971   36331 controllermanager.go:765] "Started controller" controller="persistentvolumeclaim-protection-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.778072   36331 pvc_protection_controller.go:168] "Starting PVC protection controller" logger="persistentvolumeclaim-protection-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.778093   36331 shared_informer.go:313] Waiting for caches to sync for PVC protection
2025-05-16T19:33:29-04:00 k8s.k8sd[35160]: I0516 19:33:29.793478   35160 helm/client.go:40] "creating 14 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:29-04:00 k8s.kube-apiserver[37236]: I0516 19:33:29.906405   37236 alloc.go:330] "allocated clusterIPs" service="kube-system/hubble-peer" clusterIPs={"IPv4":"10.152.183.85"}
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.930325   36331 controllermanager.go:765] "Started controller" controller="pod-garbage-collector-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.930434   36331 gc_controller.go:99] "Starting GC controller" logger="pod-garbage-collector-controller"
2025-05-16T19:33:29-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:29.930468   36331 shared_informer.go:313] Waiting for caches to sync for GC
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.084084   36331 controllermanager.go:765] "Started controller" controller="endpointslice-mirroring-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.084133   36331 controllermanager.go:743] "Warning: skipping controller" controller="node-ipam-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.084147   36331 controllermanager.go:723] "Skipping a cloud provider controller" controller="node-route-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.084160   36331 controllermanager.go:717] "Controller is disabled by a feature gate" controller="volumeattributesclass-protection-controller" requiredFeatureGates=["VolumeAttributesClass"]
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.084405   36331 endpointslicemirroring_controller.go:227] "Starting EndpointSliceMirroring controller" logger="endpointslice-mirroring-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.084429   36331 shared_informer.go:313] Waiting for caches to sync for endpoint_slice_mirroring
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.285177   36331 controllermanager.go:765] "Started controller" controller="root-ca-certificate-publisher-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.285507   36331 publisher.go:107] "Starting root CA cert publisher controller" logger="root-ca-certificate-publisher-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.285531   36331 shared_informer.go:313] Waiting for caches to sync for crt configmap
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.383720   36331 controllermanager.go:765] "Started controller" controller="ephemeral-volume-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.383822   36331 controller.go:173] "Starting ephemeral volume controller" logger="ephemeral-volume-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.383843   36331 shared_informer.go:313] Waiting for caches to sync for ephemeral
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.525941   36331 controllermanager.go:765] "Started controller" controller="endpoints-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.531366   36331 endpoints_controller.go:182] "Starting endpoint controller" logger="endpoints-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.531408   36331 shared_informer.go:313] Waiting for caches to sync for endpoint
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.540884   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.550147   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.550745   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.554505   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.554995   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.555233   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.555438   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.560002   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="bgppeers.metallb.io"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556230   36331 type.go:183] The watchlist request for controllerrevisions ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556252   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556342   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556358   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556440   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556529   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556566   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556859   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556877   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556929   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556942   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.556954   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.557354   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.557388   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.557434   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.560074   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.560096   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.560180   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.560771   36331 shared_informer.go:313] Waiting for caches to sync for resource quota
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.561799   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.582400   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.582595   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.582767   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.582850   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.583508   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.593551   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.597570   36331 type.go:183] The watchlist request for persistentvolumeclaims ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.597846   36331 type.go:183] The watchlist request for csistoragecapacities ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.598048   36331 type.go:183] The watchlist request for poddisruptionbudgets ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.598084   36331 type.go:183] The watchlist request for resourcequotas ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.598346   36331 type.go:183] The watchlist request for configmaps ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.598371   36331 type.go:183] The watchlist request for rolebindings ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.596911   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.597851   36331 type.go:183] The watchlist request for ingresses ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.596989   36331 type.go:183] The watchlist request for limitranges ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.599783   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.598355   36331 type.go:183] The watchlist request for cronjobs ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.600921   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.601730   36331 type.go:183] The watchlist request for horizontalpodautoscalers ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.601900   36331 type.go:183] The watchlist request for csidrivers ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.602057   36331 type.go:183] The watchlist request for networkpolicies ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.602242   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.602447   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.602504   36331 type.go:183] The watchlist request for persistentvolumes ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.602659   36331 type.go:183] The watchlist request for storageclasses ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.602757   36331 type.go:183] The watchlist request for leases ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.602828   36331 type.go:183] The watchlist request for replicationcontrollers ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.602916   36331 type.go:183] The watchlist request for nodes ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603018   36331 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603080   36331 type.go:183] The watchlist request for roles ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603176   36331 type.go:183] The watchlist request for namespaces ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603259   36331 type.go:183] The watchlist request for deployments ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603331   36331 type.go:183] The watchlist request for podtemplates ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603438   36331 type.go:183] The watchlist request for services ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603561   36331 type.go:183] The watchlist request for clusterroles ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603672   36331 type.go:183] The watchlist request for endpoints ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603772   36331 type.go:183] The watchlist request for jobs ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603860   36331 type.go:183] The watchlist request for daemonsets ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.603972   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.604189   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.604387   36331 type.go:183] The watchlist request for validatingadmissionpolicies ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.604522   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.607583   36331 type.go:183] The watchlist request for certificatesigningrequests ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.627095   36331 shared_informer.go:313] Waiting for caches to sync for garbage collector
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.629905   36331 type.go:183] The watchlist request for replicasets ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.632283   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.634739   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.642823   36331 type.go:183] The watchlist request for volumeattachments ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:30.648176   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta1, Resource=bfdprofiles"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.648592   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:30.649197   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta1, Resource=bgpadvertisements"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:30.649508   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta1, Resource=l2advertisements"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:30.651584   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta1, Resource=ipaddresspools"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.648633   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.648817   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.648865   36331 type.go:183] The watchlist request for csinodes ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.649145   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.662493   36331 type.go:183] The watchlist request for statefulsets ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.670046   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.670547   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.670750   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.671318   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.675773   36331 shared_informer.go:320] Caches are synced for expand
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.678620   36331 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.681389   36331 shared_informer.go:320] Caches are synced for TTL after finished
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.685786   36331 shared_informer.go:320] Caches are synced for crt configmap
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.711845   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:30.712315   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="apiregistration.k8s.io/v1, Resource=apiservices"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.713664   36331 type.go:183] The watchlist request for flowschemas ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.713805   36331 type.go:183] The watchlist request for validatingadmissionpolicybindings ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.713907   36331 type.go:183] The watchlist request for mutatingwebhookconfigurations ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.713996   36331 type.go:183] The watchlist request for priorityclasses ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.714496   36331 type.go:183] The watchlist request for ingressclasses ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.714700   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.715216   36331 type.go:183] The watchlist request for runtimeclasses ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.715316   36331 type.go:183] The watchlist request for prioritylevelconfigurations ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.716415   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.716680   36331 type.go:183] The watchlist request for endpointslices ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.717147   36331 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"research21\" does not exist"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.717313   36331 type.go:183] The watchlist request for validatingwebhookconfigurations ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.717393   36331 shared_informer.go:320] Caches are synced for ReplicationController
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.717993   36331 type.go:183] The watchlist request for clusterrolebindings ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: W0516 19:33:30.727339   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.727528   36331 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:30.728947   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta1, Resource=communities"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:30.729140   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta1, Resource=servicel2statuses"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.730811   36331 shared_informer.go:320] Caches are synced for GC
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.731729   36331 shared_informer.go:320] Caches are synced for endpoint
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.731594   36331 shared_informer.go:320] Caches are synced for cronjob
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:30.732941   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta2, Resource=bgppeers"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.734365   36331 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.735562   36331 shared_informer.go:320] Caches are synced for certificate-csrapproving
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.735611   36331 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.735649   36331 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.736450   36331 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: E0516 19:33:30.736985   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="apiextensions.k8s.io/v1, Resource=customresourcedefinitions"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.737835   36331 shared_informer.go:320] Caches are synced for namespace
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.742450   36331 shared_informer.go:320] Caches are synced for job
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.743354   36331 shared_informer.go:320] Caches are synced for TTL
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.746563   36331 shared_informer.go:320] Caches are synced for taint
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.746661   36331 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.746771   36331 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="research21"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.746825   36331 node_lifecycle_controller.go:1038] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.748032   36331 shared_informer.go:320] Caches are synced for attach detach
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.756748   36331 shared_informer.go:320] Caches are synced for service account
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.757526   36331 shared_informer.go:320] Caches are synced for PV protection
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.765754   36331 shared_informer.go:320] Caches are synced for taint-eviction-controller
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.772738   36331 shared_informer.go:320] Caches are synced for HPA
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.776467   36331 shared_informer.go:320] Caches are synced for ReplicaSet
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.777673   36331 shared_informer.go:320] Caches are synced for persistent volume
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.778250   36331 shared_informer.go:320] Caches are synced for PVC protection
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.778523   36331 shared_informer.go:320] Caches are synced for daemon sets
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.780704   36331 shared_informer.go:320] Caches are synced for disruption
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.785080   36331 shared_informer.go:320] Caches are synced for deployment
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: I0516 19:33:30.789276   35160 helm/client.go:40] "creating 1 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.795050   36331 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.796420   36331 shared_informer.go:320] Caches are synced for ephemeral
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: I0516 19:33:30.847921   35160 helm/client.go:40] "creating 2 resource(s)" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:30-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:30.853241   36331 shared_informer.go:320] Caches are synced for stateful set
2025-05-16T19:33:30-04:00 k8s.kube-apiserver[37236]: W0516 19:33:30.854286   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: I0516 19:33:30.872862   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: I0516 19:33:30.879753   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:30-04:00 k8s.kube-apiserver[37236]: W0516 19:33:30.885694   37236 dispatcher.go:217] Failed calling webhook, failing closed l2advertisementvalidationwebhook.metallb.io: failed calling webhook "l2advertisementvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-l2advertisement?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: I0516 19:33:30.910107   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: I0516 19:33:30.929007   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:30-04:00 k8s.kube-apiserver[37236]: W0516 19:33:30.971334   37236 handler_proxy.go:99] no RequestInfo found in the context
2025-05-16T19:33:30-04:00 k8s.kube-apiserver[37236]: E0516 19:33:30.971430   37236 controller.go:146] "Unhandled Error" err=<
2025-05-16T19:33:30-04:00 k8s.kube-apiserver[37236]: 	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
2025-05-16T19:33:30-04:00 k8s.kube-apiserver[37236]: 	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
2025-05-16T19:33:30-04:00 k8s.kube-apiserver[37236]:  > logger="UnhandledError"
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: I0516 19:33:30.979022   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: E0516 19:33:30.979733   35160 controllers/feature.go:168] "Failed to apply feature configuration" err=<
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: 	failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to install metallb-loadbalancer: 2 errors occurred:
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: 		* Internal error occurred: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: 		* Internal error occurred: failed calling webhook "l2advertisementvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-l2advertisement?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: 	
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]:  > logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:33:30-04:00 k8s.k8sd[35160]: I0516 19:33:30.997966   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.019225   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.034355   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.052099   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.066266   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.070371   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.103142   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.110316   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.135082   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.158866   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:31.170691   36331 shared_informer.go:320] Caches are synced for resource quota
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.177497   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:31.184632   36331 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.196425   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:31.198241   36331 shared_informer.go:320] Caches are synced for endpoint_slice
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.212169   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:31.230269   36331 shared_informer.go:320] Caches are synced for resource quota
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.253692   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:31.298899   36331 shared_informer.go:320] Caches are synced for garbage collector
2025-05-16T19:33:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:31.298944   36331 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
2025-05-16T19:33:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:31.298963   36331 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.298938   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:31.329732   36331 shared_informer.go:320] Caches are synced for garbage collector
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.332047   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.358870   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: I0516 19:33:31.409136   37236 controller.go:615] quota admission added evaluator for: replicasets.apps
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: I0516 19:33:31.477846   37236 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
2025-05-16T19:33:31-04:00 k8s.k8sd[35160]: I0516 19:33:31.914917   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:31-04:00 k8s.kubelet[38157]: E0516 19:33:31.954650   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: W0516 19:33:31.976921   37236 handler_proxy.go:99] no RequestInfo found in the context
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: E0516 19:33:31.977006   37236 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: I0516 19:33:31.979794   37236 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: W0516 19:33:31.984196   37236 handler_proxy.go:99] no RequestInfo found in the context
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: E0516 19:33:31.984314   37236 controller.go:102] "Unhandled Error" err=<
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: 	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: 	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]:  > logger="UnhandledError"
2025-05-16T19:33:31-04:00 k8s.kube-apiserver[37236]: I0516 19:33:31.986186   37236 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092392   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"registration-dir\" (UniqueName: \"kubernetes.io/host-path/478e2f54-5815-4945-accc-2453884e1a31-registration-dir\") pod \"ck-storage-rawfile-csi-node-z9lvt\" (UID: \"478e2f54-5815-4945-accc-2453884e1a31\") " pod="kube-system/ck-storage-rawfile-csi-node-z9lvt"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092477   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"hostproc\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-hostproc\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092524   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-xtables-lock\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092567   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"host-proc-sys-net\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-host-proc-sys-net\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092605   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"host-proc-sys-kernel\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-host-proc-sys-kernel\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092640   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"hubble-tls\" (UniqueName: \"kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-hubble-tls\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092674   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4kdjq\" (UniqueName: \"kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-kube-api-access-4kdjq\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092710   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/4c36c8ca-e2a9-403c-9602-bd18429db717-tmp\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092757   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-lib-modules\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092794   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6xcpl\" (UniqueName: \"kubernetes.io/projected/478e2f54-5815-4945-accc-2453884e1a31-kube-api-access-6xcpl\") pod \"ck-storage-rawfile-csi-node-z9lvt\" (UID: \"478e2f54-5815-4945-accc-2453884e1a31\") " pod="kube-system/ck-storage-rawfile-csi-node-z9lvt"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092829   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cilium-run\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-run\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092864   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cilium-netns\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-netns\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092903   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"bpf-maps\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-bpf-maps\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092941   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cilium-cgroup\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-cgroup\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.092975   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-cni-netd\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-etc-cni-netd\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.093009   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"mountpoint-dir\" (UniqueName: \"kubernetes.io/host-path/478e2f54-5815-4945-accc-2453884e1a31-mountpoint-dir\") pod \"ck-storage-rawfile-csi-node-z9lvt\" (UID: \"478e2f54-5815-4945-accc-2453884e1a31\") " pod="kube-system/ck-storage-rawfile-csi-node-z9lvt"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.093046   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-path\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cni-path\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.093083   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"clustermesh-secrets\" (UniqueName: \"kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-clustermesh-secrets\") pod \"cilium-lhphd\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") " pod="kube-system/cilium-lhphd"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.093119   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"socket-dir\" (UniqueName: \"kubernetes.io/host-path/478e2f54-5815-4945-accc-2453884e1a31-socket-dir\") pod \"ck-storage-rawfile-csi-node-z9lvt\" (UID: \"478e2f54-5815-4945-accc-2453884e1a31\") " pod="kube-system/ck-storage-rawfile-csi-node-z9lvt"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.093162   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"data-dir\" (UniqueName: \"kubernetes.io/host-path/478e2f54-5815-4945-accc-2453884e1a31-data-dir\") pod \"ck-storage-rawfile-csi-node-z9lvt\" (UID: \"478e2f54-5815-4945-accc-2453884e1a31\") " pod="kube-system/ck-storage-rawfile-csi-node-z9lvt"
2025-05-16T19:33:32-04:00 k8s.k8sd[35160]: I0516 19:33:32.149294   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.159554   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="727.548023ms"
2025-05-16T19:33:32-04:00 k8s.k8sd[35160]: I0516 19:33:32.160287   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.184694   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="752.333339ms"
2025-05-16T19:33:32-04:00 k8s.k8sd[35160]: I0516 19:33:32.258258   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.259382   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="811.130059ms"
2025-05-16T19:33:32-04:00 k8s.k8sd[35160]: I0516 19:33:32.273890   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.293450   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="839.52518ms"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.342320   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="48.793246ms"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.366299   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="104.687µs"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.382063   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="76.368µs"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.397742   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hffcj\" (UniqueName: \"kubernetes.io/projected/e0d83fa0-5e3b-4c70-b91d-e6491c41322c-kube-api-access-hffcj\") pod \"cilium-operator-6978488575-fzsj2\" (UID: \"e0d83fa0-5e3b-4c70-b91d-e6491c41322c\") " pod="kube-system/cilium-operator-6978488575-fzsj2"
2025-05-16T19:33:32-04:00 k8s.kubelet[38157]: I0516 19:33:32.398246   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cilium-config-path\" (UniqueName: \"kubernetes.io/configmap/e0d83fa0-5e3b-4c70-b91d-e6491c41322c-cilium-config-path\") pod \"cilium-operator-6978488575-fzsj2\" (UID: \"e0d83fa0-5e3b-4c70-b91d-e6491c41322c\") " pod="kube-system/cilium-operator-6978488575-fzsj2"
2025-05-16T19:33:32-04:00 k8s.k8sd[35160]: I0516 19:33:32.433517   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.452895   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="193.44643ms"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.453127   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="153.035µs"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.506371   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="346.720274ms"
2025-05-16T19:33:32-04:00 k8s.k8sd[35160]: I0516 19:33:32.507497   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.510175   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="121.933µs"
2025-05-16T19:33:32-04:00 k8s.k8sd[35160]: I0516 19:33:32.512578   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.521134   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="336.353266ms"
2025-05-16T19:33:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:32.551258173-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cilium-lhphd,Uid:4c36c8ca-e2a9-403c-9602-bd18429db717,Namespace:kube-system,Attempt:0,}"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.690720   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="165.199257ms"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.692010   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="119.653µs"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.721485   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="99.741µs"
2025-05-16T19:33:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:32.738299   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="81.5µs"
2025-05-16T19:33:32-04:00 k8s.k8sd[35160]: I0516 19:33:32.747841   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:32.758373816-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cilium-operator-6978488575-fzsj2,Uid:e0d83fa0-5e3b-4c70-b91d-e6491c41322c,Namespace:kube-system,Attempt:0,}"
2025-05-16T19:33:32-04:00 k8s.k8sd[35160]: I0516 19:33:32.920821   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:33-04:00 k8s.kubelet[38157]: E0516 19:33:33.348784   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:33.729117857-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/pause:3.10,Labels:map[string]string{io.cri-containerd.image: managed,io.cri-containerd.pinned: pinned,},XXX_unrecognized:[],}"
2025-05-16T19:33:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:33.733968972-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136,Labels:map[string]string{io.cri-containerd.image: managed,io.cri-containerd.pinned: pinned,},XXX_unrecognized:[],}"
2025-05-16T19:33:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:33.740508154-04:00" level=info msg="ImageUpdate event &ImageUpdate{Name:ghcr.io/canonical/k8s-snap/pause:3.10,Labels:map[string]string{io.cri-containerd.image: managed,io.cri-containerd.pinned: pinned,},XXX_unrecognized:[],}"
2025-05-16T19:33:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:33.747797273-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/pause@sha256:ee6521f290b2168b6e0935a181d4cff9be1ac3f505666ef0e3c98fae8199917a,Labels:map[string]string{io.cri-containerd.image: managed,io.cri-containerd.pinned: pinned,},XXX_unrecognized:[],}"
2025-05-16T19:33:34-04:00 k8s.k8sd[35160]: I0516 19:33:34.155798   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:34-04:00 k8s.k8sd[35160]: I0516 19:33:34.312107   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:34.354932946-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:33:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:34.355178145-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:33:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:34.355271727-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:33:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:34.355647432-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b pid=39009 runtime=io.containerd.runc.v2
2025-05-16T19:33:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:34.373438083-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:33:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:34.376145179-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:33:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:34.380877587-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:33:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:34.383058617-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3 pid=39021 runtime=io.containerd.runc.v2
2025-05-16T19:33:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:35.224163578-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cilium-lhphd,Uid:4c36c8ca-e2a9-403c-9602-bd18429db717,Namespace:kube-system,Attempt:0,} returns sandbox id \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\""
2025-05-16T19:33:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:35.233165201-04:00" level=info msg="PullImage \"ghcr.io/canonical/cilium:1.16.3-ck0\""
2025-05-16T19:33:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:35.270581917-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cilium-operator-6978488575-fzsj2,Uid:e0d83fa0-5e3b-4c70-b91d-e6491c41322c,Namespace:kube-system,Attempt:0,} returns sandbox id \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\""
2025-05-16T19:33:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:35.276954216-04:00" level=info msg="PullImage \"ghcr.io/canonical/cilium-operator-generic:1.16.3-ck0\""
2025-05-16T19:33:35-04:00 k8s.kubelet[38157]: E0516 19:33:35.347941   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:35-04:00 k8s.k8sd[35160]: I0516 19:33:35.833747   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:35-04:00 k8s.k8sd[35160]: I0516 19:33:35.847040   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:35-04:00 k8s.k8sd[35160]: I0516 19:33:35.876004   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:35-04:00 k8s.k8sd[35160]: I0516 19:33:35.892378   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:35-04:00 k8s.k8sd[35160]: I0516 19:33:35.919099   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:35-04:00 k8s.k8sd[35160]: I0516 19:33:35.940475   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:35-04:00 k8s.k8sd[35160]: I0516 19:33:35.958649   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.003124   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.040963   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.098015   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.122598   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.136137   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.193667   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.200294   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.240958   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.295151   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.327334   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.371048   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.396038   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.420911   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.461132   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.528412   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:36-04:00 k8s.k8sd[35160]: I0516 19:33:36.593389   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:37-04:00 k8s.kubelet[38157]: E0516 19:33:37.349857   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:37-04:00 k8s.k8sd[35160]: I0516 19:33:37.567797   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:38-04:00 k8s.k8sd[35160]: I0516 19:33:38.202760   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:38-04:00 k8s.k8sd[35160]: I0516 19:33:38.590230   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:38-04:00 k8s.k8sd[35160]: I0516 19:33:38.624242   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:38-04:00 k8s.k8sd[35160]: I0516 19:33:38.686039   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:38-04:00 k8s.k8sd[35160]: I0516 19:33:38.688041   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:39-04:00 k8s.kubelet[38157]: E0516 19:33:39.354848   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:40-04:00 k8s.k8sd[35160]: I0516 19:33:40.697849   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:40-04:00 k8s.k8sd[35160]: I0516 19:33:40.945036   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:40-04:00 k8s.k8sd[35160]: I0516 19:33:40.965423   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:40-04:00 k8s.k8sd[35160]: I0516 19:33:40.979758   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:40-04:00 k8s.k8sd[35160]: I0516 19:33:40.997305   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.020755   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.043945   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.069264   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.105382   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.148082   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.167248   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.205524   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.243062   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.276261   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.306181   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.344129   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.kubelet[38157]: E0516 19:33:41.356197   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.407048   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.445021   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.470649   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.582438   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.650169   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.663338   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:41-04:00 k8s.k8sd[35160]: I0516 19:33:41.705128   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.092990   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.208863   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.241088   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.295476   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.347092   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.390038   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.399784   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.434999   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.482520   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.520473   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.540305   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.577695   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.582983   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.641148   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.656908   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.689036   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.708715   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.731999   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.760309   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.770348   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.800149   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.831410   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.847474   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.871603   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.901639   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.907842   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.929922   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.937612   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.956556   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.968074   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:42-04:00 k8s.k8sd[35160]: I0516 19:33:42.992763   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.017906   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.036691   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.042948   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.077805   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.114737   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.119383   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.134666   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.161982   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.171586   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.207486   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.224991   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.269298   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.314610   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.328701   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.335307   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.kubelet[38157]: E0516 19:33:43.351943   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.447117   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.450705   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.579777   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:43-04:00 k8s.k8sd[35160]: I0516 19:33:43.663194   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.043470   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.071046   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: E0516 19:33:44.161399   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.260949   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.358918   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.410963   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.480684   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.542722   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.603425   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.645407   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.694546   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: E0516 19:33:44.704467   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to delete Gateway API cilium configuration: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="gateway"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.817696   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.933836   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.954099   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:44-04:00 k8s.k8sd[35160]: I0516 19:33:44.973434   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:44.997370   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:45.113797   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:45.286737   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:45.319001   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.kubelet[38157]: E0516 19:33:45.351439   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:45.565222   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:45.760734   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:45.847228   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:45.864377   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.kube-apiserver[37236]: W0516 19:33:45.885645   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:45.983194   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:45-04:00 k8s.k8sd[35160]: I0516 19:33:45.987207   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:46-04:00 k8s.k8sd[35160]: E0516 19:33:46.025675   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:33:46-04:00 k8s.k8sd[35160]: I0516 19:33:46.086645   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:46-04:00 k8s.k8sd[35160]: I0516 19:33:46.212744   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:46-04:00 k8s.k8sd[35160]: I0516 19:33:46.326603   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:46-04:00 k8s.k8sd[35160]: I0516 19:33:46.474588   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:46-04:00 k8s.k8sd[35160]: I0516 19:33:46.799195   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:47-04:00 k8s.kubelet[38157]: E0516 19:33:47.348245   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.059595   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.090924   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.212198   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.341078   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.616895   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.657734   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.669047   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.695428   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.737418   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.768136   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.801114   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.842827   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.864350   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.886870   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.918478   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.968492   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:48-04:00 k8s.k8sd[35160]: I0516 19:33:48.990303   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:49-04:00 k8s.k8sd[35160]: I0516 19:33:49.017604   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:49-04:00 k8s.k8sd[35160]: I0516 19:33:49.051621   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:49-04:00 k8s.k8sd[35160]: I0516 19:33:49.087629   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:49-04:00 k8s.k8sd[35160]: I0516 19:33:49.107967   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:49-04:00 k8s.k8sd[35160]: I0516 19:33:49.138636   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:49-04:00 k8s.k8sd[35160]: I0516 19:33:49.283985   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:49-04:00 k8s.kubelet[38157]: E0516 19:33:49.348072   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:49-04:00 k8s.k8sd[35160]: I0516 19:33:49.362605   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:50-04:00 k8s.k8sd[35160]: I0516 19:33:50.191831   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:50-04:00 k8s.k8sd[35160]: I0516 19:33:50.833897   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:51-04:00 k8s.k8sd[35160]: I0516 19:33:51.343689   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:51-04:00 k8s.kubelet[38157]: E0516 19:33:51.358252   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:51.419601866-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/cilium-operator-generic:1.16.3-ck0,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:33:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:51.427796127-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:3d39e3fc8825f269605d11fdce937f02ef16da15a0e4eac7516ab31872bc2e99,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:33:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:51.445150462-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/cilium-operator-generic@sha256:57d62b57f5bb67376e36865d0390acbdc635ee7b5b133bd23d18ba157e1497c3,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:33:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:51.447633089-04:00" level=info msg="PullImage \"ghcr.io/canonical/cilium-operator-generic:1.16.3-ck0\" returns image reference \"sha256:3d39e3fc8825f269605d11fdce937f02ef16da15a0e4eac7516ab31872bc2e99\""
2025-05-16T19:33:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:51.463219015-04:00" level=info msg="CreateContainer within sandbox \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\" for container &ContainerMetadata{Name:cilium-operator,Attempt:0,}"
2025-05-16T19:33:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:51.545181759-04:00" level=info msg="CreateContainer within sandbox \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\" for &ContainerMetadata{Name:cilium-operator,Attempt:0,} returns container id \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\""
2025-05-16T19:33:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:51.547643593-04:00" level=info msg="StartContainer for \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\""
2025-05-16T19:33:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:33:51.727946311-04:00" level=info msg="StartContainer for \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\" returns successfully"
2025-05-16T19:33:51-04:00 k8s.k8sd[35160]: I0516 19:33:51.998425   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:52-04:00 k8s.k8sd[35160]: I0516 19:33:52.280484   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:52-04:00 k8s.k8sd[35160]: I0516 19:33:52.384746   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:52-04:00 k8s.k8sd[35160]: I0516 19:33:52.404249   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:52-04:00 k8s.k8sd[35160]: I0516 19:33:52.434303   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:52-04:00 k8s.k8sd[35160]: I0516 19:33:52.483001   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:52-04:00 k8s.k8sd[35160]: I0516 19:33:52.528909   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:52-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:52.559854   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="85.629µs"
2025-05-16T19:33:52-04:00 k8s.k8sd[35160]: I0516 19:33:52.581512   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:52-04:00 k8s.k8sd[35160]: I0516 19:33:52.819198   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:52-04:00 k8s.kube-apiserver[37236]: I0516 19:33:52.920149   37236 handler.go:286] Adding GroupVersion cilium.io v2alpha1 to ResourceManager
2025-05-16T19:33:52-04:00 k8s.kube-apiserver[37236]: I0516 19:33:52.988733   37236 handler.go:286] Adding GroupVersion cilium.io v2alpha1 to ResourceManager
2025-05-16T19:33:53-04:00 k8s.kube-apiserver[37236]: I0516 19:33:53.087322   37236 handler.go:286] Adding GroupVersion cilium.io v2alpha1 to ResourceManager
2025-05-16T19:33:53-04:00 k8s.kube-apiserver[37236]: I0516 19:33:53.097771   37236 handler.go:286] Adding GroupVersion cilium.io v2 to ResourceManager
2025-05-16T19:33:53-04:00 k8s.k8sd[35160]: I0516 19:33:53.127651   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:53-04:00 k8s.kube-apiserver[37236]: I0516 19:33:53.131859   37236 handler.go:286] Adding GroupVersion cilium.io v2 to ResourceManager
2025-05-16T19:33:53-04:00 k8s.kube-apiserver[37236]: I0516 19:33:53.282706   37236 handler.go:286] Adding GroupVersion cilium.io v2 to ResourceManager
2025-05-16T19:33:53-04:00 k8s.kube-apiserver[37236]: I0516 19:33:53.314202   37236 handler.go:286] Adding GroupVersion cilium.io v2 to ResourceManager
2025-05-16T19:33:53-04:00 k8s.kube-apiserver[37236]: I0516 19:33:53.334707   37236 handler.go:286] Adding GroupVersion cilium.io v2 to ResourceManager
2025-05-16T19:33:53-04:00 k8s.kube-apiserver[37236]: I0516 19:33:53.334792   37236 handler.go:286] Adding GroupVersion cilium.io v2alpha1 to ResourceManager
2025-05-16T19:33:53-04:00 k8s.kubelet[38157]: E0516 19:33:53.353806   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:53-04:00 k8s.kube-apiserver[37236]: I0516 19:33:53.388771   37236 handler.go:286] Adding GroupVersion cilium.io v2alpha1 to ResourceManager
2025-05-16T19:33:53-04:00 k8s.k8sd[35160]: I0516 19:33:53.430375   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:53-04:00 k8s.k8sd[35160]: I0516 19:33:53.531002   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:53-04:00 k8s.k8sd[35160]: I0516 19:33:53.595906   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:53-04:00 k8s.kubelet[38157]: I0516 19:33:53.599442   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/cilium-operator-6978488575-fzsj2" podStartSLOduration=5.417097091 podStartE2EDuration="21.599416158s" podCreationTimestamp="2025-05-16 19:33:32 -0400 EDT" firstStartedPulling="2025-05-16 19:33:35.276105126 -0400 EDT m=+8.413701811" lastFinishedPulling="2025-05-16 19:33:51.458424193 -0400 EDT m=+24.596020878" observedRunningTime="2025-05-16 19:33:52.560786127 -0400 EDT m=+25.698382821" watchObservedRunningTime="2025-05-16 19:33:53.599416158 -0400 EDT m=+26.737012852"
2025-05-16T19:33:53-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:53.631769   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="34.096428ms"
2025-05-16T19:33:53-04:00 k8s.kube-controller-manager[36331]: I0516 19:33:53.632022   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="170.123µs"
2025-05-16T19:33:53-04:00 k8s.k8sd[35160]: I0516 19:33:53.712982   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:53-04:00 k8s.k8sd[35160]: I0516 19:33:53.775474   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:53-04:00 k8s.k8sd[35160]: I0516 19:33:53.820592   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:53-04:00 k8s.k8sd[35160]: I0516 19:33:53.909388   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:53-04:00 k8s.k8sd[35160]: I0516 19:33:53.988873   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:54-04:00 k8s.k8sd[35160]: I0516 19:33:54.067276   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:54-04:00 k8s.k8sd[35160]: I0516 19:33:54.158221   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:54-04:00 k8s.k8sd[35160]: I0516 19:33:54.221158   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:54-04:00 k8s.k8sd[35160]: I0516 19:33:54.259431   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:54-04:00 k8s.k8sd[35160]: I0516 19:33:54.323661   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:33:54-04:00 k8s.k8sd[35160]: I0516 19:33:54.427390   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:54-04:00 k8s.kube-apiserver[37236]: I0516 19:33:54.785491   37236 handler.go:286] Adding GroupVersion cilium.io v2 to ResourceManager
2025-05-16T19:33:55-04:00 k8s.k8sd[35160]: I0516 19:33:55.172291   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:55-04:00 k8s.k8sd[35160]: I0516 19:33:55.213239   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:55-04:00 k8s.kubelet[38157]: E0516 19:33:55.360966   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:55-04:00 k8s.kube-apiserver[37236]: I0516 19:33:55.383813   37236 handler.go:286] Adding GroupVersion cilium.io v2 to ResourceManager
2025-05-16T19:33:55-04:00 k8s.k8sd[35160]: I0516 19:33:55.595908   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:55-04:00 k8s.k8sd[35160]: I0516 19:33:55.606070   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:56-04:00 k8s.k8sd[35160]: I0516 19:33:56.971054   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.066223   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.217185   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.268963   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.kubelet[38157]: E0516 19:33:57.350045   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.369751   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.406410   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.437796   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.458344   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.476794   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.501748   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.519502   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.585944   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.622258   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.629352   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.635671   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.652645   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.658239   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.662830   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.678153   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.704279   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.735912   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.744183   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.750843   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.810927   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.818332   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.842511   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.848472   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.861841   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.878703   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.884993   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.899521   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.914127   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.918732   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.925528   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.942203   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.943486   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.967636   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.989251   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.993988   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:57-04:00 k8s.k8sd[35160]: I0516 19:33:57.996351   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.049693   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.051799   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.067574   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.105432   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.121745   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.134194   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.143381   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.146866   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.169869   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.212978   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.234319   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.246631   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.250055   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: E0516 19:33:58.252186   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to delete Gateway API cilium configuration: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="gateway"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.291500   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.298580   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.314744   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.387150   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.405364   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.407190   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.465310   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.533008   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.570417   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.576821   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.660120   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.673771   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.679709   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.764300   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.775697   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.814304   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.868502   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.915712   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.946035   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:58-04:00 k8s.k8sd[35160]: I0516 19:33:58.962355   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.001215   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.088855   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.181530   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: E0516 19:33:59.254244   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.311394   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.kubelet[38157]: E0516 19:33:59.349825   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.365912   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.387946   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.408289   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.476671   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.477093   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.573397   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.594216   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.684159   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.730358   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:33:59-04:00 k8s.k8sd[35160]: I0516 19:33:59.787431   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:00-04:00 k8s.k8sd[35160]: I0516 19:34:00.021987   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:00-04:00 k8s.k8sd[35160]: I0516 19:34:00.189888   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:00-04:00 k8s.k8sd[35160]: I0516 19:34:00.289998   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:00-04:00 k8s.k8sd[35160]: I0516 19:34:00.671360   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:00-04:00 k8s.k8sd[35160]: I0516 19:34:00.713565   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:00-04:00 k8s.k8sd[35160]: E0516 19:34:00.865080   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to delete Gateway API cilium configuration: failed to upgrade ck-network: another operation (install/upgrade/rollback) is in progress" logger="k8sd" controller="feature" feature="gateway"
2025-05-16T19:34:01-04:00 k8s.k8sd[35160]: I0516 19:34:01.022444   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:01-04:00 k8s.k8sd[35160]: I0516 19:34:01.086166   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:01-04:00 k8s.kube-apiserver[37236]: W0516 19:34:01.107821   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:34:01-04:00 k8s.k8sd[35160]: I0516 19:34:01.139520   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.180301   36331 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:01.181623   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="ciliumnetworkpolicies.cilium.io"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:01.181689   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="ciliumendpoints.cilium.io"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:01.181766   36331 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="ciliumnodeconfigs.cilium.io"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:01.181839   36331 shared_informer.go:313] Waiting for caches to sync for resource quota
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.185305   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.193766   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.194172   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2, Resource=ciliumnetworkpolicies"
2025-05-16T19:34:01-04:00 k8s.k8sd[35160]: E0516 19:34:01.195478   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.200417   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2, Resource=ciliumendpoints"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.230432   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.232302   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2, Resource=ciliumnodeconfigs"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:01.282009   36331 shared_informer.go:320] Caches are synced for resource quota
2025-05-16T19:34:01-04:00 k8s.k8sd[35160]: I0516 19:34:01.339798   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:01.345377   36331 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
2025-05-16T19:34:01-04:00 k8s.kubelet[38157]: E0516 19:34:01.352820   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:01.361614   36331 shared_informer.go:313] Waiting for caches to sync for garbage collector
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.364137   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.368573   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2, Resource=ciliumclusterwidenetworkpolicies"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.379505   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.379987   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.380113   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.386204   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2, Resource=ciliumnodes"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.387816   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2, Resource=ciliumidentities"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.392731   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2alpha1, Resource=ciliumloadbalancerippools"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.396597   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.403657   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2alpha1, Resource=ciliumcidrgroups"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.410152   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.412192   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2, Resource=ciliumexternalworkloads"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.424659   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.428577   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2alpha1, Resource=ciliumpodippools"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: W0516 19:34:01.435878   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:01.439244   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="cilium.io/v2alpha1, Resource=ciliuml2announcementpolicies"
2025-05-16T19:34:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:01.462576   36331 shared_informer.go:320] Caches are synced for garbage collector
2025-05-16T19:34:01-04:00 k8s.k8sd[35160]: I0516 19:34:01.494840   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:01-04:00 k8s.k8sd[35160]: I0516 19:34:01.873066   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:01-04:00 k8s.k8sd[35160]: I0516 19:34:01.890394   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:01-04:00 k8s.k8sd[35160]: I0516 19:34:01.958597   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:02-04:00 k8s.k8sd[35160]: I0516 19:34:02.393680   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:02-04:00 k8s.k8sd[35160]: I0516 19:34:02.422772   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:02-04:00 k8s.k8sd[35160]: I0516 19:34:02.689912   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:02-04:00 k8s.k8sd[35160]: I0516 19:34:02.729825   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:03-04:00 k8s.k8sd[35160]: I0516 19:34:03.250484   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:03-04:00 k8s.kubelet[38157]: E0516 19:34:03.355524   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:03-04:00 k8s.k8sd[35160]: I0516 19:34:03.450739   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:04-04:00 k8s.k8sd[35160]: I0516 19:34:04.515033   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:04-04:00 k8s.k8sd[35160]: I0516 19:34:04.945010   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:05-04:00 k8s.k8sd[35160]: I0516 19:34:05.252320   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:05-04:00 k8s.kubelet[38157]: E0516 19:34:05.349600   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.324076   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.401092   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.417914   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.436414   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.459178   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.478024   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.498707   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.499540   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.585005   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.587609   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.625106   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.681394   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.705001   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.729806   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.752902   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.772474   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.792648   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.793939   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.825278   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.847933   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.871474   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.891700   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.929388   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:06-04:00 k8s.k8sd[35160]: I0516 19:34:06.990184   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.032569   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.069609   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.137292   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.147062   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.178850   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.191186   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.228778   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.251344   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.304862   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.kubelet[38157]: E0516 19:34:07.353638   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.394771   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.490000   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.532642   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.544555   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.738919   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.810142   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.832593   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:07-04:00 k8s.k8sd[35160]: I0516 19:34:07.950939   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:08-04:00 k8s.k8sd[35160]: I0516 19:34:08.552992   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:08-04:00 k8s.k8sd[35160]: I0516 19:34:08.876330   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:08-04:00 k8s.k8sd[35160]: I0516 19:34:08.881387   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:09-04:00 k8s.k8sd[35160]: I0516 19:34:09.008837   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:09-04:00 k8s.k8sd[35160]: I0516 19:34:09.228576   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:09-04:00 k8s.kubelet[38157]: E0516 19:34:09.352532   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:09-04:00 k8s.k8sd[35160]: I0516 19:34:09.432912   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:10-04:00 k8s.k8sd[35160]: I0516 19:34:10.590671   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:11-04:00 k8s.k8sd[35160]: I0516 19:34:11.180311   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:11-04:00 k8s.k8sd[35160]: I0516 19:34:11.321687   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:11-04:00 k8s.kubelet[38157]: E0516 19:34:11.351871   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:11-04:00 k8s.k8sd[35160]: I0516 19:34:11.886658   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.670321   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.742908   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.763391   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.789140   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.808528   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.828558   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.852447   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.901050   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.944944   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:12-04:00 k8s.k8sd[35160]: I0516 19:34:12.982419   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.015110   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.086028   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.119657   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.143893   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.169427   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.191850   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.207529   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.231093   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.251878   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.284004   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.318138   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.353719   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.kubelet[38157]: E0516 19:34:13.368296   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.378724   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.396509   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.434602   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.474001   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.525014   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.593021   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.622855   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.670206   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.684791   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.721646   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.794537   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.824345   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.888074   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.904234   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.941927   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.975060   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:13-04:00 k8s.k8sd[35160]: I0516 19:34:13.986286   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.048348   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.101890   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.175658   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.198633   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: E0516 19:34:14.220651   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable network: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="network"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.224083   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.225147   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.250807   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.258636   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.274350   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.285946   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.305647   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.314182   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.361968   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.416599   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.424173   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.443735   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.454494   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.460590   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.475437   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.492114   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.516271   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.539157   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.565386   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.582134   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.599408   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.611550   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.617806   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.622809   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.633177   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.639000   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.671670   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.683928   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.748729   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.821663   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.848144   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.851492   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.964223   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.974880   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:14-04:00 k8s.k8sd[35160]: I0516 19:34:14.998923   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.130998   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.203739   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.233216   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: E0516 19:34:15.267512   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to delete Gateway API cilium configuration: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="gateway"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.306837   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.307438   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.kubelet[38157]: E0516 19:34:15.350907   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.368366   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.515359   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.570007   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.596862   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.688055   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.827988   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.903314   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.k8sd[35160]: I0516 19:34:15.928176   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:15-04:00 k8s.kube-apiserver[37236]: W0516 19:34:15.981588   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:34:16-04:00 k8s.k8sd[35160]: I0516 19:34:16.048144   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:16-04:00 k8s.k8sd[35160]: I0516 19:34:16.102942   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:16-04:00 k8s.k8sd[35160]: I0516 19:34:16.108280   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:16-04:00 k8s.k8sd[35160]: E0516 19:34:16.224879   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:34:16-04:00 k8s.k8sd[35160]: I0516 19:34:16.563370   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:16-04:00 k8s.k8sd[35160]: I0516 19:34:16.767245   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:16-04:00 k8s.k8sd[35160]: I0516 19:34:16.862006   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:17-04:00 k8s.k8sd[35160]: I0516 19:34:17.123394   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:17-04:00 k8s.kubelet[38157]: E0516 19:34:17.352967   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:17-04:00 k8s.k8sd[35160]: I0516 19:34:17.485644   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:17-04:00 k8s.k8sd[35160]: I0516 19:34:17.541693   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:17-04:00 k8s.k8sd[35160]: I0516 19:34:17.722909   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:17-04:00 k8s.k8sd[35160]: E0516 19:34:17.766697   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable network: failed to upgrade ck-network: another operation (install/upgrade/rollback) is in progress" logger="k8sd" controller="feature" feature="network"
2025-05-16T19:34:18-04:00 k8s.k8sd[35160]: I0516 19:34:18.489249   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:18-04:00 k8s.k8sd[35160]: I0516 19:34:18.826031   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:19-04:00 k8s.kubelet[38157]: E0516 19:34:19.348130   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:19-04:00 k8s.k8sd[35160]: I0516 19:34:19.579219   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:19-04:00 k8s.k8sd[35160]: I0516 19:34:19.698841   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:19-04:00 k8s.k8sd[35160]: I0516 19:34:19.744139   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:19-04:00 k8s.k8sd[35160]: I0516 19:34:19.810562   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:19-04:00 k8s.k8sd[35160]: I0516 19:34:19.871868   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:19-04:00 k8s.k8sd[35160]: I0516 19:34:19.954827   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.087300   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.195969   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.199077   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.202387   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.231228   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.309959   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.413538   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.507359   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.579063   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.681046   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.723467   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.742587   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.813443   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.939190   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:20-04:00 k8s.k8sd[35160]: I0516 19:34:20.987713   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:21-04:00 k8s.k8sd[35160]: I0516 19:34:21.048512   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:21-04:00 k8s.k8sd[35160]: I0516 19:34:21.108812   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:21-04:00 k8s.k8sd[35160]: I0516 19:34:21.145292   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:21-04:00 k8s.k8sd[35160]: I0516 19:34:21.291503   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:21-04:00 k8s.kubelet[38157]: E0516 19:34:21.350793   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:22-04:00 k8s.k8sd[35160]: I0516 19:34:22.928225   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:22-04:00 k8s.k8sd[35160]: I0516 19:34:22.946706   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:22-04:00 k8s.k8sd[35160]: I0516 19:34:22.957503   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:22-04:00 k8s.k8sd[35160]: I0516 19:34:22.975039   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:22-04:00 k8s.k8sd[35160]: I0516 19:34:22.990014   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.005954   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.020947   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.038241   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.058015   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.073964   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.094936   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.107306   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.121326   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.150531   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.201481   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.235716   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.259853   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.294070   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.kubelet[38157]: E0516 19:34:23.348365   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.715022   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.763638   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.785594   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.810228   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.822024   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.836808   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.869179   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.915717   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:23-04:00 k8s.k8sd[35160]: I0516 19:34:23.999179   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.033089   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.070265   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.100834   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.125938   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.140094   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.153083   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.170537   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.179145   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.187053   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.208856   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.244551   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.271720   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.293323   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.309868   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.330021   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.359111   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.361281   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.420782   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.445322   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:24-04:00 k8s.k8sd[35160]: I0516 19:34:24.482572   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:25-04:00 k8s.kubelet[38157]: E0516 19:34:25.348809   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:25-04:00 k8s.k8sd[35160]: I0516 19:34:25.614982   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:25-04:00 k8s.k8sd[35160]: I0516 19:34:25.662157   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:25-04:00 k8s.k8sd[35160]: I0516 19:34:25.780325   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:25-04:00 k8s.k8sd[35160]: I0516 19:34:25.920248   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:25-04:00 k8s.k8sd[35160]: I0516 19:34:25.947203   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:25-04:00 k8s.k8sd[35160]: I0516 19:34:25.953381   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:25-04:00 k8s.kube-apiserver[37236]: W0516 19:34:25.961820   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:34:25-04:00 k8s.k8sd[35160]: I0516 19:34:25.974195   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: E0516 19:34:26.020967   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.171169   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.171934   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.211645   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.288561   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.404189   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.421052   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.438599   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.461476   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.494641   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.541094   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.583193   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: E0516 19:34:26.616812   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable network: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="network"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.617930   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: E0516 19:34:26.629924   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.638568   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.660409   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.695380   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.716014   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.737161   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.841079   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:26-04:00 k8s.k8sd[35160]: I0516 19:34:26.906176   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:27-04:00 k8s.kubelet[38157]: E0516 19:34:27.353123   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:27-04:00 k8s.k8sd[35160]: I0516 19:34:27.411938   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:27-04:00 k8s.k8sd[35160]: I0516 19:34:27.756004   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:28-04:00 k8s.k8sd[35160]: I0516 19:34:28.114197   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:28.417704   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="135.949772ms"
2025-05-16T19:34:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:28.541837007-04:00" level=info msg="StopContainer for \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\" with timeout 30 (s)"
2025-05-16T19:34:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:28.546094900-04:00" level=info msg="Stop container \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\" with signal terminated"
2025-05-16T19:34:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:28.665927   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="261.794694ms"
2025-05-16T19:34:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:28.671702   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="253.926769ms"
2025-05-16T19:34:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:28.674388   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="97.275µs"
2025-05-16T19:34:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:28.745448   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="79.44331ms"
2025-05-16T19:34:28-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:28.746740   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="86.488µs"
2025-05-16T19:34:29-04:00 k8s.kubelet[38157]: E0516 19:34:29.347901   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:30-04:00 k8s.k8sd[35160]: I0516 19:34:30.049735   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:30-04:00 k8s.k8sd[35160]: I0516 19:34:30.445779   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:30.814695225-04:00" level=info msg="shim disconnected" id=44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda
2025-05-16T19:34:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:30.814793062-04:00" level=warning msg="cleaning up after shim disconnected" id=44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda namespace=k8s.io
2025-05-16T19:34:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:30.814814710-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:30.887809320-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:30-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=42938 runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:30.893297250-04:00" level=info msg="StopContainer for \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\" returns successfully"
2025-05-16T19:34:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:30.894882594-04:00" level=info msg="StopPodSandbox for \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\""
2025-05-16T19:34:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:30.895077989-04:00" level=info msg="Container to stop \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.099780891-04:00" level=info msg="shim disconnected" id=b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.099899248-04:00" level=warning msg="cleaning up after shim disconnected" id=b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3 namespace=k8s.io
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.099940291-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.142581818-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:31-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=42973 runtime=io.containerd.runc.v2\ntime=\"2025-05-16T19:34:31-04:00\" level=warning msg=\"failed to remove runc container\" error=\"runc did not terminate successfully: exit status 255: \" runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.143349075-04:00" level=info msg="TearDown network for sandbox \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\" successfully"
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.143413454-04:00" level=info msg="StopPodSandbox for \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\" returns successfully"
2025-05-16T19:34:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:31.200717   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="81.079µs"
2025-05-16T19:34:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:31.216590   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="76.929µs"
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.221307   38157 memory_manager.go:355] "RemoveStaleState removing state" podUID="e0d83fa0-5e3b-4c70-b91d-e6491c41322c" containerName="cilium-operator"
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.273271   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-hffcj\" (UniqueName: \"kubernetes.io/projected/e0d83fa0-5e3b-4c70-b91d-e6491c41322c-kube-api-access-hffcj\") pod \"e0d83fa0-5e3b-4c70-b91d-e6491c41322c\" (UID: \"e0d83fa0-5e3b-4c70-b91d-e6491c41322c\") "
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.273352   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"cilium-config-path\" (UniqueName: \"kubernetes.io/configmap/e0d83fa0-5e3b-4c70-b91d-e6491c41322c-cilium-config-path\") pod \"e0d83fa0-5e3b-4c70-b91d-e6491c41322c\" (UID: \"e0d83fa0-5e3b-4c70-b91d-e6491c41322c\") "
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.273475   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-s54kc\" (UniqueName: \"kubernetes.io/projected/cb44cab7-bf54-41e1-b09e-943f2fa4a677-kube-api-access-s54kc\") pod \"cilium-operator-6f799b7b4-b4v8x\" (UID: \"cb44cab7-bf54-41e1-b09e-943f2fa4a677\") " pod="kube-system/cilium-operator-6f799b7b4-b4v8x"
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.273534   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cilium-config-path\" (UniqueName: \"kubernetes.io/configmap/cb44cab7-bf54-41e1-b09e-943f2fa4a677-cilium-config-path\") pod \"cilium-operator-6f799b7b4-b4v8x\" (UID: \"cb44cab7-bf54-41e1-b09e-943f2fa4a677\") " pod="kube-system/cilium-operator-6f799b7b4-b4v8x"
2025-05-16T19:34:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:31.287186   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="85.082µs"
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.288416   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/e0d83fa0-5e3b-4c70-b91d-e6491c41322c-kube-api-access-hffcj" (OuterVolumeSpecName: "kube-api-access-hffcj") pod "e0d83fa0-5e3b-4c70-b91d-e6491c41322c" (UID: "e0d83fa0-5e3b-4c70-b91d-e6491c41322c"). InnerVolumeSpecName "kube-api-access-hffcj". PluginName "kubernetes.io/projected", VolumeGIDValue ""
2025-05-16T19:34:31-04:00 k8s.kube-controller-manager[36331]: E0516 19:34:31.309787   36331 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.319128   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/e0d83fa0-5e3b-4c70-b91d-e6491c41322c-cilium-config-path" (OuterVolumeSpecName: "cilium-config-path") pod "e0d83fa0-5e3b-4c70-b91d-e6491c41322c" (UID: "e0d83fa0-5e3b-4c70-b91d-e6491c41322c"). InnerVolumeSpecName "cilium-config-path". PluginName "kubernetes.io/configmap", VolumeGIDValue ""
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: E0516 19:34:31.353139   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.376674   38157 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-hffcj\" (UniqueName: \"kubernetes.io/projected/e0d83fa0-5e3b-4c70-b91d-e6491c41322c-kube-api-access-hffcj\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.376723   38157 reconciler_common.go:299] "Volume detached for volume \"cilium-config-path\" (UniqueName: \"kubernetes.io/configmap/e0d83fa0-5e3b-4c70-b91d-e6491c41322c-cilium-config-path\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:31.503653   36331 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.555782272-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cilium-operator-6f799b7b4-b4v8x,Uid:cb44cab7-bf54-41e1-b09e-943f2fa4a677,Namespace:kube-system,Attempt:0,}"
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.659858494-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.660066862-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.660104657-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.662638645-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/73340c8bc3e50231b4f9d68a3c434708c6b972da61ffbfa29b85fcf99670d8a0 pid=43017 runtime=io.containerd.runc.v2
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.714261   38157 scope.go:117] "RemoveContainer" containerID="44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda"
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.756720236-04:00" level=info msg="RemoveContainer for \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\""
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.772971932-04:00" level=info msg="RemoveContainer for \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\" returns successfully"
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.774726   38157 scope.go:117] "RemoveContainer" containerID="44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda"
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.775878801-04:00" level=error msg="ContainerStatus for \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\" failed" error="rpc error: code = NotFound desc = an error occurred when try to find container \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\": not found"
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: E0516 19:34:31.776855   38157 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\": not found" containerID="44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda"
2025-05-16T19:34:31-04:00 k8s.kubelet[38157]: I0516 19:34:31.776912   38157 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda"} err="failed to get container status \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\": rpc error: code = NotFound desc = an error occurred when try to find container \"44c63b0f7e84806afdda83f59689acf7464df303655026d3de3060a165b7feda\": not found"
2025-05-16T19:34:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:31.907617   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="105.794µs"
2025-05-16T19:34:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:31.954339   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="80.177µs"
2025-05-16T19:34:31-04:00 k8s.kube-apiserver[37236]: W0516 19:34:31.983625   37236 handler_proxy.go:99] no RequestInfo found in the context
2025-05-16T19:34:31-04:00 k8s.kube-apiserver[37236]: E0516 19:34:31.983701   37236 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
2025-05-16T19:34:31-04:00 k8s.kube-apiserver[37236]: I0516 19:34:31.984836   37236 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
2025-05-16T19:34:31-04:00 k8s.kube-apiserver[37236]: W0516 19:34:31.987080   37236 handler_proxy.go:99] no RequestInfo found in the context
2025-05-16T19:34:31-04:00 k8s.kube-apiserver[37236]: E0516 19:34:31.987200   37236 controller.go:102] "Unhandled Error" err=<
2025-05-16T19:34:31-04:00 k8s.kube-apiserver[37236]: 	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
2025-05-16T19:34:31-04:00 k8s.kube-apiserver[37236]: 	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
2025-05-16T19:34:31-04:00 k8s.kube-apiserver[37236]:  > logger="UnhandledError"
2025-05-16T19:34:31-04:00 k8s.kube-apiserver[37236]: I0516 19:34:31.989176   37236 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
2025-05-16T19:34:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:31.998499372-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cilium-operator-6f799b7b4-b4v8x,Uid:cb44cab7-bf54-41e1-b09e-943f2fa4a677,Namespace:kube-system,Attempt:0,} returns sandbox id \"73340c8bc3e50231b4f9d68a3c434708c6b972da61ffbfa29b85fcf99670d8a0\""
2025-05-16T19:34:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:32.024105931-04:00" level=info msg="CreateContainer within sandbox \"73340c8bc3e50231b4f9d68a3c434708c6b972da61ffbfa29b85fcf99670d8a0\" for container &ContainerMetadata{Name:cilium-operator,Attempt:0,}"
2025-05-16T19:34:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:32.065004218-04:00" level=info msg="CreateContainer within sandbox \"73340c8bc3e50231b4f9d68a3c434708c6b972da61ffbfa29b85fcf99670d8a0\" for &ContainerMetadata{Name:cilium-operator,Attempt:0,} returns container id \"d396d08f9ff00d8e73735dcf7e58d95209b74b22e70b3e10b11440347c299735\""
2025-05-16T19:34:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:32.088022280-04:00" level=info msg="StartContainer for \"d396d08f9ff00d8e73735dcf7e58d95209b74b22e70b3e10b11440347c299735\""
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.227753   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.287692   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.344133   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:32.372743012-04:00" level=info msg="StartContainer for \"d396d08f9ff00d8e73735dcf7e58d95209b74b22e70b3e10b11440347c299735\" returns successfully"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.383843   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.449074   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.547850   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.594780   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.659320   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.697345   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:32.818003   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="83.698µs"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.828029   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.873396   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.942313   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:32-04:00 k8s.k8sd[35160]: I0516 19:34:32.993432   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.046446   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.072294   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.104397   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.141497   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.207326   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.270894   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.kubelet[38157]: E0516 19:34:33.351328   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:33-04:00 k8s.kubelet[38157]: I0516 19:34:33.389135   38157 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="e0d83fa0-5e3b-4c70-b91d-e6491c41322c" path="/var/lib/kubelet/pods/e0d83fa0-5e3b-4c70-b91d-e6491c41322c/volumes"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.452556   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.537271   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.560210   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.596905   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.615810   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.632342   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.654256   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.k8sd[35160]: I0516 19:34:33.726760   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:33-04:00 k8s.kubelet[38157]: I0516 19:34:33.780242   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/cilium-operator-6f799b7b4-b4v8x" podStartSLOduration=5.780217394 podStartE2EDuration="5.780217394s" podCreationTimestamp="2025-05-16 19:34:28 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:34:32.82368744 -0400 EDT m=+65.961284125" watchObservedRunningTime="2025-05-16 19:34:33.780217394 -0400 EDT m=+66.917814086"
2025-05-16T19:34:33-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:33.795391   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="15.441555ms"
2025-05-16T19:34:33-04:00 k8s.kube-controller-manager[36331]: I0516 19:34:33.799207   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="63.339µs"
2025-05-16T19:34:34-04:00 k8s.k8sd[35160]: I0516 19:34:34.263944   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:34-04:00 k8s.k8sd[35160]: I0516 19:34:34.945038   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:34-04:00 k8s.k8sd[35160]: I0516 19:34:34.996642   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:35-04:00 k8s.k8sd[35160]: I0516 19:34:35.230353   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:35-04:00 k8s.k8sd[35160]: I0516 19:34:35.235297   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:35-04:00 k8s.kubelet[38157]: E0516 19:34:35.355317   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:35-04:00 k8s.k8sd[35160]: I0516 19:34:35.458427   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:35-04:00 k8s.k8sd[35160]: I0516 19:34:35.474419   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:35-04:00 k8s.kube-apiserver[37236]: W0516 19:34:35.485150   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:34:35-04:00 k8s.k8sd[35160]: I0516 19:34:35.501173   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:35-04:00 k8s.k8sd[35160]: E0516 19:34:35.533068   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:34:35-04:00 k8s.k8sd[35160]: I0516 19:34:35.708682   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:35-04:00 k8s.k8sd[35160]: I0516 19:34:35.875303   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:35-04:00 k8s.k8sd[35160]: I0516 19:34:35.895023   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:37-04:00 k8s.kubelet[38157]: E0516 19:34:37.349492   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.145787   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.213160   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.243968   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.352476   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.765483   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.782763   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.801777   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.825880   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.849708   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.875410   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.898259   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.914878   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.930645   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.948219   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:38-04:00 k8s.k8sd[35160]: I0516 19:34:38.981898   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:39-04:00 k8s.k8sd[35160]: I0516 19:34:39.019154   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:39-04:00 k8s.k8sd[35160]: I0516 19:34:39.046157   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:39-04:00 k8s.k8sd[35160]: I0516 19:34:39.099641   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:39-04:00 k8s.k8sd[35160]: I0516 19:34:39.175291   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:39-04:00 k8s.kubelet[38157]: E0516 19:34:39.352812   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:39-04:00 k8s.k8sd[35160]: I0516 19:34:39.625775   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: I0516 19:34:40.530099   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: I0516 19:34:40.574876   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: I0516 19:34:40.688758   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:40.707482450-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/cilium:1.16.3-ck0,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:34:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:40.716666976-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:a1b2ae6fd34a8d56ff9559c3cc659ee02e5aa256d9b5f1327fa2646de058d0dd,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:34:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:40.724064901-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/cilium@sha256:68a45ebd077aa222eb2d9811e3d3fafb0dbd4a27f71ebc135afc16948a30889d,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:34:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:40.724898968-04:00" level=info msg="PullImage \"ghcr.io/canonical/cilium:1.16.3-ck0\" returns image reference \"sha256:a1b2ae6fd34a8d56ff9559c3cc659ee02e5aa256d9b5f1327fa2646de058d0dd\""
2025-05-16T19:34:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:40.736329048-04:00" level=info msg="CreateContainer within sandbox \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\" for container &ContainerMetadata{Name:config,Attempt:0,}"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: I0516 19:34:40.742712   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:40.767675841-04:00" level=info msg="CreateContainer within sandbox \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\" for &ContainerMetadata{Name:config,Attempt:0,} returns container id \"ce218062c0efd9c7fbd5b314c281b8d5fdbfce063e647e70043f81c730cfc30e\""
2025-05-16T19:34:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:40.774162710-04:00" level=info msg="StartContainer for \"ce218062c0efd9c7fbd5b314c281b8d5fdbfce063e647e70043f81c730cfc30e\""
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: I0516 19:34:40.791903   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: E0516 19:34:40.857195   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable network: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="network"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: I0516 19:34:40.860315   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: I0516 19:34:40.889135   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: I0516 19:34:40.919886   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:40.929242741-04:00" level=info msg="StartContainer for \"ce218062c0efd9c7fbd5b314c281b8d5fdbfce063e647e70043f81c730cfc30e\" returns successfully"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: E0516 19:34:40.933096   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:34:40-04:00 k8s.k8sd[35160]: I0516 19:34:40.977239   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.026616   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.063754   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.096510   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.150724   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.185563   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.215965   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.235135   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.248735   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.259493   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.270853   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.281769   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.293026   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.303895   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.313755   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.329942   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.345711   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.kubelet[38157]: E0516 19:34:41.348318   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.366157   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.390654   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.484145   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.579503675-04:00" level=info msg="shim disconnected" id=ce218062c0efd9c7fbd5b314c281b8d5fdbfce063e647e70043f81c730cfc30e
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.579597090-04:00" level=warning msg="cleaning up after shim disconnected" id=ce218062c0efd9c7fbd5b314c281b8d5fdbfce063e647e70043f81c730cfc30e namespace=k8s.io
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.579620435-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.597040930-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:41-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=43257 runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:41-04:00 k8s.k8sd[35160]: I0516 19:34:41.744830   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.810978053-04:00" level=info msg="StopPodSandbox for \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\""
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.811037556-04:00" level=info msg="Container to stop \"ce218062c0efd9c7fbd5b314c281b8d5fdbfce063e647e70043f81c730cfc30e\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.931976404-04:00" level=info msg="shim disconnected" id=de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.932069135-04:00" level=warning msg="cleaning up after shim disconnected" id=de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b namespace=k8s.io
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.932089651-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.963916817-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:41-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=43289 runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.964549721-04:00" level=info msg="TearDown network for sandbox \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\" successfully"
2025-05-16T19:34:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:41.964590965-04:00" level=info msg="StopPodSandbox for \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\" returns successfully"
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083132   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"cni-path\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cni-path\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083204   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"hostproc\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-hostproc\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083241   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-xtables-lock\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083282   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/4c36c8ca-e2a9-403c-9602-bd18429db717-tmp\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083285   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cni-path" (OuterVolumeSpecName: "cni-path") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "cni-path". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083319   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"cilium-cgroup\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-cgroup\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083352   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"bpf-maps\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-bpf-maps\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083372   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-xtables-lock" (OuterVolumeSpecName: "xtables-lock") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "xtables-lock". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083383   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"etc-cni-netd\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-etc-cni-netd\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083418   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-etc-cni-netd" (OuterVolumeSpecName: "etc-cni-netd") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "etc-cni-netd". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083450   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"cilium-run\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-run\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083506   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"cilium-netns\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-netns\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083547   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"host-proc-sys-net\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-host-proc-sys-net\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083545   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-hostproc" (OuterVolumeSpecName: "hostproc") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "hostproc". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083590   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"hubble-tls\" (UniqueName: \"kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-hubble-tls\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083605   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-netns" (OuterVolumeSpecName: "cilium-netns") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "cilium-netns". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083627   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-4kdjq\" (UniqueName: \"kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-kube-api-access-4kdjq\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083658   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-lib-modules\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083694   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"clustermesh-secrets\" (UniqueName: \"kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-clustermesh-secrets\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083728   38157 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"host-proc-sys-kernel\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-host-proc-sys-kernel\") pod \"4c36c8ca-e2a9-403c-9602-bd18429db717\" (UID: \"4c36c8ca-e2a9-403c-9602-bd18429db717\") "
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083780   38157 reconciler_common.go:299] "Volume detached for volume \"etc-cni-netd\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-etc-cni-netd\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083804   38157 reconciler_common.go:299] "Volume detached for volume \"cilium-netns\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-netns\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083828   38157 reconciler_common.go:299] "Volume detached for volume \"cni-path\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cni-path\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083847   38157 reconciler_common.go:299] "Volume detached for volume \"hostproc\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-hostproc\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083866   38157 reconciler_common.go:299] "Volume detached for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-xtables-lock\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083644   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-run" (OuterVolumeSpecName: "cilium-run") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "cilium-run". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083685   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-cgroup" (OuterVolumeSpecName: "cilium-cgroup") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "cilium-cgroup". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083903   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-host-proc-sys-kernel" (OuterVolumeSpecName: "host-proc-sys-kernel") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "host-proc-sys-kernel". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.083930   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-lib-modules" (OuterVolumeSpecName: "lib-modules") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "lib-modules". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.084426   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-bpf-maps" (OuterVolumeSpecName: "bpf-maps") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "bpf-maps". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.084508   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-host-proc-sys-net" (OuterVolumeSpecName: "host-proc-sys-net") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "host-proc-sys-net". PluginName "kubernetes.io/host-path", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.101535   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/4c36c8ca-e2a9-403c-9602-bd18429db717-tmp" (OuterVolumeSpecName: "tmp") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "tmp". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.107101   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-clustermesh-secrets" (OuterVolumeSpecName: "clustermesh-secrets") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "clustermesh-secrets". PluginName "kubernetes.io/projected", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.110953   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-kube-api-access-4kdjq" (OuterVolumeSpecName: "kube-api-access-4kdjq") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "kube-api-access-4kdjq". PluginName "kubernetes.io/projected", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.114398   38157 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-hubble-tls" (OuterVolumeSpecName: "hubble-tls") pod "4c36c8ca-e2a9-403c-9602-bd18429db717" (UID: "4c36c8ca-e2a9-403c-9602-bd18429db717"). InnerVolumeSpecName "hubble-tls". PluginName "kubernetes.io/projected", VolumeGIDValue ""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184393   38157 reconciler_common.go:299] "Volume detached for volume \"bpf-maps\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-bpf-maps\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184442   38157 reconciler_common.go:299] "Volume detached for volume \"cilium-run\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-run\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184481   38157 reconciler_common.go:299] "Volume detached for volume \"host-proc-sys-net\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-host-proc-sys-net\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184502   38157 reconciler_common.go:299] "Volume detached for volume \"hubble-tls\" (UniqueName: \"kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-hubble-tls\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184521   38157 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-4kdjq\" (UniqueName: \"kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-kube-api-access-4kdjq\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184539   38157 reconciler_common.go:299] "Volume detached for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-lib-modules\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184559   38157 reconciler_common.go:299] "Volume detached for volume \"clustermesh-secrets\" (UniqueName: \"kubernetes.io/projected/4c36c8ca-e2a9-403c-9602-bd18429db717-clustermesh-secrets\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184577   38157 reconciler_common.go:299] "Volume detached for volume \"host-proc-sys-kernel\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-host-proc-sys-kernel\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184596   38157 reconciler_common.go:299] "Volume detached for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/4c36c8ca-e2a9-403c-9602-bd18429db717-tmp\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.184613   38157 reconciler_common.go:299] "Volume detached for volume \"cilium-cgroup\" (UniqueName: \"kubernetes.io/host-path/4c36c8ca-e2a9-403c-9602-bd18429db717-cilium-cgroup\") on node \"research21\" DevicePath \"\""
2025-05-16T19:34:42-04:00 k8s.k8sd[35160]: I0516 19:34:42.519229   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:42-04:00 k8s.k8sd[35160]: I0516 19:34:42.583270   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:42-04:00 k8s.k8sd[35160]: I0516 19:34:42.621608   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:42-04:00 k8s.k8sd[35160]: I0516 19:34:42.716989   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:42-04:00 k8s.k8sd[35160]: I0516 19:34:42.803877   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:42-04:00 k8s.k8sd[35160]: I0516 19:34:42.818486   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.819922   38157 scope.go:117] "RemoveContainer" containerID="ce218062c0efd9c7fbd5b314c281b8d5fdbfce063e647e70043f81c730cfc30e"
2025-05-16T19:34:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:42.824806134-04:00" level=info msg="RemoveContainer for \"ce218062c0efd9c7fbd5b314c281b8d5fdbfce063e647e70043f81c730cfc30e\""
2025-05-16T19:34:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:42.829353338-04:00" level=info msg="RemoveContainer for \"ce218062c0efd9c7fbd5b314c281b8d5fdbfce063e647e70043f81c730cfc30e\" returns successfully"
2025-05-16T19:34:42-04:00 k8s.kube-apiserver[37236]: W0516 19:34:42.834409   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:34:42-04:00 k8s.k8sd[35160]: I0516 19:34:42.850430   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:42-04:00 k8s.k8sd[35160]: E0516 19:34:42.870754   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.964594   38157 memory_manager.go:355] "RemoveStaleState removing state" podUID="4c36c8ca-e2a9-403c-9602-bd18429db717" containerName="config"
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: W0516 19:34:42.978986   38157 reflector.go:569] object-"kube-system"/"clustermesh-apiserver-local-cert": failed to list *v1.Secret: secrets "clustermesh-apiserver-local-cert" is forbidden: User "system:node:research21" cannot list resource "secrets" in API group "" in the namespace "kube-system": no relationship found between node 'research21' and this object
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: W0516 19:34:42.979017   38157 reflector.go:569] object-"kube-system"/"cilium-clustermesh": failed to list *v1.Secret: secrets "cilium-clustermesh" is forbidden: User "system:node:research21" cannot list resource "secrets" in API group "" in the namespace "kube-system": no relationship found between node 'research21' and this object
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: E0516 19:34:42.979057   38157 reflector.go:166] "Unhandled Error" err="object-\"kube-system\"/\"clustermesh-apiserver-local-cert\": Failed to watch *v1.Secret: failed to list *v1.Secret: secrets \"clustermesh-apiserver-local-cert\" is forbidden: User \"system:node:research21\" cannot list resource \"secrets\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'research21' and this object" logger="UnhandledError"
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: E0516 19:34:42.979065   38157 reflector.go:166] "Unhandled Error" err="object-\"kube-system\"/\"cilium-clustermesh\": Failed to watch *v1.Secret: failed to list *v1.Secret: secrets \"cilium-clustermesh\" is forbidden: User \"system:node:research21\" cannot list resource \"secrets\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'research21' and this object" logger="UnhandledError"
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: W0516 19:34:42.979200   38157 reflector.go:569] object-"kube-system"/"clustermesh-apiserver-remote-cert": failed to list *v1.Secret: secrets "clustermesh-apiserver-remote-cert" is forbidden: User "system:node:research21" cannot list resource "secrets" in API group "" in the namespace "kube-system": no relationship found between node 'research21' and this object
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: W0516 19:34:42.979231   38157 reflector.go:569] object-"kube-system"/"hubble-server-certs": failed to list *v1.Secret: secrets "hubble-server-certs" is forbidden: User "system:node:research21" cannot list resource "secrets" in API group "" in the namespace "kube-system": no relationship found between node 'research21' and this object
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: E0516 19:34:42.979237   38157 reflector.go:166] "Unhandled Error" err="object-\"kube-system\"/\"clustermesh-apiserver-remote-cert\": Failed to watch *v1.Secret: failed to list *v1.Secret: secrets \"clustermesh-apiserver-remote-cert\" is forbidden: User \"system:node:research21\" cannot list resource \"secrets\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'research21' and this object" logger="UnhandledError"
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: E0516 19:34:42.979264   38157 reflector.go:166] "Unhandled Error" err="object-\"kube-system\"/\"hubble-server-certs\": Failed to watch *v1.Secret: failed to list *v1.Secret: secrets \"hubble-server-certs\" is forbidden: User \"system:node:research21\" cannot list resource \"secrets\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'research21' and this object" logger="UnhandledError"
2025-05-16T19:34:42-04:00 k8s.kubelet[38157]: I0516 19:34:42.979302   38157 status_manager.go:890] "Failed to get status for pod" podUID="d3fe5374-0783-41b1-b28e-513ad99edfb5" pod="kube-system/cilium-vzx55" err="pods \"cilium-vzx55\" is forbidden: User \"system:node:research21\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'research21' and this object"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094340   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cilium-netns\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-cilium-netns\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094417   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/d3fe5374-0783-41b1-b28e-513ad99edfb5-tmp\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094470   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cilium-cgroup\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-cilium-cgroup\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094516   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"clustermesh-secrets\" (UniqueName: \"kubernetes.io/projected/d3fe5374-0783-41b1-b28e-513ad99edfb5-clustermesh-secrets\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094557   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"host-proc-sys-kernel\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-host-proc-sys-kernel\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094591   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"bpf-maps\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-bpf-maps\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094625   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-path\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-cni-path\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094670   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-cni-netd\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-etc-cni-netd\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094705   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-lib-modules\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094738   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"hubble-tls\" (UniqueName: \"kubernetes.io/projected/d3fe5374-0783-41b1-b28e-513ad99edfb5-hubble-tls\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094777   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bwc4s\" (UniqueName: \"kubernetes.io/projected/d3fe5374-0783-41b1-b28e-513ad99edfb5-kube-api-access-bwc4s\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094820   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cilium-run\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-cilium-run\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094855   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"hostproc\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-hostproc\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094893   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-xtables-lock\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.094928   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"host-proc-sys-net\" (UniqueName: \"kubernetes.io/host-path/d3fe5374-0783-41b1-b28e-513ad99edfb5-host-proc-sys-net\") pod \"cilium-vzx55\" (UID: \"d3fe5374-0783-41b1-b28e-513ad99edfb5\") " pod="kube-system/cilium-vzx55"
2025-05-16T19:34:43-04:00 k8s.k8sd[35160]: I0516 19:34:43.107946   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:43-04:00 k8s.k8sd[35160]: I0516 19:34:43.308338   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: I0516 19:34:43.353502   38157 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="4c36c8ca-e2a9-403c-9602-bd18429db717" path="/var/lib/kubelet/pods/4c36c8ca-e2a9-403c-9602-bd18429db717/volumes"
2025-05-16T19:34:43-04:00 k8s.kubelet[38157]: E0516 19:34:43.355834   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:44-04:00 k8s.kubelet[38157]: E0516 19:34:44.196772   38157 projected.go:263] Couldn't get secret kube-system/cilium-clustermesh: failed to sync secret cache: timed out waiting for the condition
2025-05-16T19:34:44-04:00 k8s.kubelet[38157]: E0516 19:34:44.196837   38157 projected.go:194] Error preparing data for projected volume clustermesh-secrets for pod kube-system/cilium-vzx55: failed to sync secret cache: timed out waiting for the condition
2025-05-16T19:34:44-04:00 k8s.kubelet[38157]: E0516 19:34:44.196961   38157 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/d3fe5374-0783-41b1-b28e-513ad99edfb5-clustermesh-secrets podName:d3fe5374-0783-41b1-b28e-513ad99edfb5 nodeName:}" failed. No retries permitted until 2025-05-16 19:34:44.69692095 -0400 EDT m=+77.834517637 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "clustermesh-secrets" (UniqueName: "kubernetes.io/projected/d3fe5374-0783-41b1-b28e-513ad99edfb5-clustermesh-secrets") pod "cilium-vzx55" (UID: "d3fe5374-0783-41b1-b28e-513ad99edfb5") : failed to sync secret cache: timed out waiting for the condition
2025-05-16T19:34:44-04:00 k8s.k8sd[35160]: I0516 19:34:44.208699   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:44-04:00 k8s.k8sd[35160]: I0516 19:34:44.568055   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:44.823011515-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cilium-vzx55,Uid:d3fe5374-0783-41b1-b28e-513ad99edfb5,Namespace:kube-system,Attempt:0,}"
2025-05-16T19:34:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:44.880765160-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:34:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:44.881962285-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:34:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:44.882023396-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:34:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:44.882433091-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725 pid=43332 runtime=io.containerd.runc.v2
2025-05-16T19:34:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:44.986262849-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cilium-vzx55,Uid:d3fe5374-0783-41b1-b28e-513ad99edfb5,Namespace:kube-system,Attempt:0,} returns sandbox id \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\""
2025-05-16T19:34:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:44.990244059-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for container &ContainerMetadata{Name:config,Attempt:0,}"
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.028620711-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for &ContainerMetadata{Name:config,Attempt:0,} returns container id \"649433b656d57c8851f8732f6da4d1842791ec5a715c7f310084cfe9f2be60d8\""
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.031510865-04:00" level=info msg="StartContainer for \"649433b656d57c8851f8732f6da4d1842791ec5a715c7f310084cfe9f2be60d8\""
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.169874339-04:00" level=info msg="StartContainer for \"649433b656d57c8851f8732f6da4d1842791ec5a715c7f310084cfe9f2be60d8\" returns successfully"
2025-05-16T19:34:45-04:00 k8s.kubelet[38157]: E0516 19:34:45.352797   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.383415524-04:00" level=info msg="shim disconnected" id=649433b656d57c8851f8732f6da4d1842791ec5a715c7f310084cfe9f2be60d8
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.383532050-04:00" level=warning msg="cleaning up after shim disconnected" id=649433b656d57c8851f8732f6da4d1842791ec5a715c7f310084cfe9f2be60d8 namespace=k8s.io
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.383553158-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.403046435-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:45-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=43435 runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:45-04:00 k8s.k8sd[35160]: I0516 19:34:45.534489   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.841191126-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for container &ContainerMetadata{Name:mount-cgroup,Attempt:0,}"
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.897445715-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for &ContainerMetadata{Name:mount-cgroup,Attempt:0,} returns container id \"30915727f4c9b67a325c8515dd5e3afa8eceb35862df939050760db81f6af341\""
2025-05-16T19:34:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:45.898582040-04:00" level=info msg="StartContainer for \"30915727f4c9b67a325c8515dd5e3afa8eceb35862df939050760db81f6af341\""
2025-05-16T19:34:45-04:00 k8s.k8sd[35160]: I0516 19:34:45.900661   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:45-04:00 k8s.k8sd[35160]: I0516 19:34:45.948152   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.058258   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:46.098396784-04:00" level=info msg="StartContainer for \"30915727f4c9b67a325c8515dd5e3afa8eceb35862df939050760db81f6af341\" returns successfully"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.132682   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.149001   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.173878   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:46.180343637-04:00" level=info msg="shim disconnected" id=30915727f4c9b67a325c8515dd5e3afa8eceb35862df939050760db81f6af341
2025-05-16T19:34:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:46.180435967-04:00" level=warning msg="cleaning up after shim disconnected" id=30915727f4c9b67a325c8515dd5e3afa8eceb35862df939050760db81f6af341 namespace=k8s.io
2025-05-16T19:34:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:46.180467128-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:46.210630710-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:46-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=43500 runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.219781   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.220303   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.236337   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.237550   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.260141   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.261944   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.281587   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.284067   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.299647   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.300138   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.324496   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.326071   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.343320   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.351443   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.370988   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.396314   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.411492   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.415363   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.439136   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.447249   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.461960   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.487604   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.499103   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.510072   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.524345   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.536922   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.548439   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.561125   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.573182   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.583627   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.603645   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.629525   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.652280   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.680242   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.764292   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.k8sd[35160]: I0516 19:34:46.770103   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:46.851164127-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for container &ContainerMetadata{Name:apply-sysctl-overwrites,Attempt:0,}"
2025-05-16T19:34:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:46.907874099-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for &ContainerMetadata{Name:apply-sysctl-overwrites,Attempt:0,} returns container id \"e1dfaf5755b537c1f74c36bb7410f75b3475ca21702547422540819ee3f445be\""
2025-05-16T19:34:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:46.914702010-04:00" level=info msg="StartContainer for \"e1dfaf5755b537c1f74c36bb7410f75b3475ca21702547422540819ee3f445be\""
2025-05-16T19:34:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:47.084896019-04:00" level=info msg="StartContainer for \"e1dfaf5755b537c1f74c36bb7410f75b3475ca21702547422540819ee3f445be\" returns successfully"
2025-05-16T19:34:47-04:00 k8s.k8sd[35160]: I0516 19:34:47.157024   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:47.214279789-04:00" level=info msg="shim disconnected" id=e1dfaf5755b537c1f74c36bb7410f75b3475ca21702547422540819ee3f445be
2025-05-16T19:34:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:47.214716938-04:00" level=warning msg="cleaning up after shim disconnected" id=e1dfaf5755b537c1f74c36bb7410f75b3475ca21702547422540819ee3f445be namespace=k8s.io
2025-05-16T19:34:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:47.214753254-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:47.237734402-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:47-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=43568 runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:47-04:00 k8s.kubelet[38157]: E0516 19:34:47.348226   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:47.864737255-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for container &ContainerMetadata{Name:mount-bpf-fs,Attempt:0,}"
2025-05-16T19:34:47-04:00 k8s.k8sd[35160]: I0516 19:34:47.880068   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:47.909418378-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for &ContainerMetadata{Name:mount-bpf-fs,Attempt:0,} returns container id \"43a771eba2b1fbdc5d2c987f491941a51498dfe5cbb191be29a76c6d64bc3796\""
2025-05-16T19:34:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:47.913939272-04:00" level=info msg="StartContainer for \"43a771eba2b1fbdc5d2c987f491941a51498dfe5cbb191be29a76c6d64bc3796\""
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: I0516 19:34:48.047318   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:48.156231039-04:00" level=info msg="StartContainer for \"43a771eba2b1fbdc5d2c987f491941a51498dfe5cbb191be29a76c6d64bc3796\" returns successfully"
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: I0516 19:34:48.216534   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: I0516 19:34:48.230871   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:48.242373709-04:00" level=info msg="shim disconnected" id=43a771eba2b1fbdc5d2c987f491941a51498dfe5cbb191be29a76c6d64bc3796
2025-05-16T19:34:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:48.243694136-04:00" level=warning msg="cleaning up after shim disconnected" id=43a771eba2b1fbdc5d2c987f491941a51498dfe5cbb191be29a76c6d64bc3796 namespace=k8s.io
2025-05-16T19:34:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:48.244067288-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:48.270747773-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:48-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=43628 runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: I0516 19:34:48.333517   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: I0516 19:34:48.345972   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: I0516 19:34:48.383034   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:48-04:00 k8s.kube-apiserver[37236]: W0516 19:34:48.398635   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: I0516 19:34:48.412993   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: E0516 19:34:48.438471   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: I0516 19:34:48.644554   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:48-04:00 k8s.k8sd[35160]: I0516 19:34:48.815251   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:48.875731547-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for container &ContainerMetadata{Name:clean-cilium-state,Attempt:0,}"
2025-05-16T19:34:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:48.946137922-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for &ContainerMetadata{Name:clean-cilium-state,Attempt:0,} returns container id \"3b2a4be00f5eb1b8d95c6c7ae70ffbfcba4f84a263e6a1c4541df05494f0b18d\""
2025-05-16T19:34:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:48.953044286-04:00" level=info msg="StartContainer for \"3b2a4be00f5eb1b8d95c6c7ae70ffbfcba4f84a263e6a1c4541df05494f0b18d\""
2025-05-16T19:34:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:49.155806681-04:00" level=info msg="StartContainer for \"3b2a4be00f5eb1b8d95c6c7ae70ffbfcba4f84a263e6a1c4541df05494f0b18d\" returns successfully"
2025-05-16T19:34:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:49.206468345-04:00" level=info msg="shim disconnected" id=3b2a4be00f5eb1b8d95c6c7ae70ffbfcba4f84a263e6a1c4541df05494f0b18d
2025-05-16T19:34:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:49.206557900-04:00" level=warning msg="cleaning up after shim disconnected" id=3b2a4be00f5eb1b8d95c6c7ae70ffbfcba4f84a263e6a1c4541df05494f0b18d namespace=k8s.io
2025-05-16T19:34:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:49.206580100-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:49.239541556-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:49-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=43707 runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:49-04:00 k8s.kubelet[38157]: E0516 19:34:49.348487   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:49-04:00 k8s.k8sd[35160]: I0516 19:34:49.526921   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:49.883797470-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for container &ContainerMetadata{Name:install-cni-binaries,Attempt:0,}"
2025-05-16T19:34:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:49.927095134-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for &ContainerMetadata{Name:install-cni-binaries,Attempt:0,} returns container id \"361ce94943f7ca0e0c6da95d32791d4bc2a2613c60e518b7e7499dff62053c9f\""
2025-05-16T19:34:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:49.929714258-04:00" level=info msg="StartContainer for \"361ce94943f7ca0e0c6da95d32791d4bc2a2613c60e518b7e7499dff62053c9f\""
2025-05-16T19:34:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:50.108420177-04:00" level=info msg="StartContainer for \"361ce94943f7ca0e0c6da95d32791d4bc2a2613c60e518b7e7499dff62053c9f\" returns successfully"
2025-05-16T19:34:50-04:00 k8s.k8sd[35160]: I0516 19:34:50.550981   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:50-04:00 k8s.k8sd[35160]: I0516 19:34:50.629652   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:50.809819395-04:00" level=info msg="shim disconnected" id=361ce94943f7ca0e0c6da95d32791d4bc2a2613c60e518b7e7499dff62053c9f
2025-05-16T19:34:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:50.809912033-04:00" level=warning msg="cleaning up after shim disconnected" id=361ce94943f7ca0e0c6da95d32791d4bc2a2613c60e518b7e7499dff62053c9f namespace=k8s.io
2025-05-16T19:34:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:50.809934805-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:34:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:50.827872247-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:34:50-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=43785 runtime=io.containerd.runc.v2\n"
2025-05-16T19:34:50-04:00 k8s.kubelet[38157]: I0516 19:34:50.889128   38157 kubelet_resources.go:45] "Allocatable" allocatable={"cpu":"4","ephemeral-storage":"477464304Ki","hugepages-1Gi":"0","hugepages-2Mi":"0","memory":"32721880Ki","pods":"110"}
2025-05-16T19:34:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:50.897981899-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for container &ContainerMetadata{Name:cilium-agent,Attempt:0,}"
2025-05-16T19:34:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:50.950784437-04:00" level=info msg="CreateContainer within sandbox \"bf18b36c17196a56ea8b99c59caa22d9c6da77a10b2798f08c5e02e9c2200725\" for &ContainerMetadata{Name:cilium-agent,Attempt:0,} returns container id \"339076dd941685778b81d5ebfd46d72c19fe2bcde8218b151adcb76aa9a331fc\""
2025-05-16T19:34:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:50.955330307-04:00" level=info msg="StartContainer for \"339076dd941685778b81d5ebfd46d72c19fe2bcde8218b151adcb76aa9a331fc\""
2025-05-16T19:34:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:34:51.128341275-04:00" level=info msg="StartContainer for \"339076dd941685778b81d5ebfd46d72c19fe2bcde8218b151adcb76aa9a331fc\" returns successfully"
2025-05-16T19:34:51-04:00 k8s.kubelet[38157]: E0516 19:34:51.350839   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:51-04:00 k8s.kubelet[38157]: I0516 19:34:51.955885   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/cilium-vzx55" podStartSLOduration=9.955855514 podStartE2EDuration="9.955855514s" podCreationTimestamp="2025-05-16 19:34:42 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:34:51.94638942 -0400 EDT m=+85.083986125" watchObservedRunningTime="2025-05-16 19:34:51.955855514 -0400 EDT m=+85.093452198"
2025-05-16T19:34:51-04:00 k8s.k8sd[35160]: I0516 19:34:51.989545   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.418133   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.684827   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.752228   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.757711   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.769504   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.772364   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.783987   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.789356   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.798608   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.804424   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.819685   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.827295   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.839857   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.866673   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.870764   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.894480   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.923412   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.928981   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.950678   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.954530   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.973257   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.975061   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:52-04:00 k8s.k8sd[35160]: I0516 19:34:52.990235   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.003790   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.025116   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.042029   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.042747   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.058486   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.083482   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.086703   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.102267   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.127757   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.138986   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.141826   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.159989   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.187422   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.214861   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.232791   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.254781   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.280304   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.304422   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.kubelet[38157]: E0516 19:34:53.351762   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.385636   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.576013   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: I0516 19:34:53.577268   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:53-04:00 k8s.k8sd[35160]: E0516 19:34:53.854700   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable network: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="network"
2025-05-16T19:34:54-04:00 k8s.k8sd[35160]: I0516 19:34:54.474604   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:54-04:00 k8s.k8sd[35160]: I0516 19:34:54.517371   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:54-04:00 k8s.k8sd[35160]: I0516 19:34:54.616093   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:54-04:00 k8s.k8sd[35160]: I0516 19:34:54.696018   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:54-04:00 k8s.k8sd[35160]: I0516 19:34:54.708889   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:54-04:00 k8s.kube-apiserver[37236]: W0516 19:34:54.714866   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:34:54-04:00 k8s.k8sd[35160]: I0516 19:34:54.728359   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:54-04:00 k8s.k8sd[35160]: E0516 19:34:54.746618   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:34:54-04:00 k8s.k8sd[35160]: I0516 19:34:54.902603   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:55-04:00 k8s.k8sd[35160]: I0516 19:34:55.030944   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:55-04:00 k8s.kubelet[38157]: E0516 19:34:55.352147   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:56-04:00 k8s.k8sd[35160]: I0516 19:34:56.157495   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.kubelet[38157]: E0516 19:34:57.351484   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.495467   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.538347   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.552476   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.562745   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.571855   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.581424   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.595933   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.615638   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.643247   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.664972   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.684406   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.711859   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.727294   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.738392   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.751002   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.761360   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.770751   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.780831   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.791518   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.809255   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.819657   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.833042   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.854827   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.868579   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.889279   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:34:57-04:00 k8s.k8sd[35160]: I0516 19:34:57.948972   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:59-04:00 k8s.k8sd[35160]: I0516 19:34:59.090804   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:59-04:00 k8s.k8sd[35160]: I0516 19:34:59.185925   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:59-04:00 k8s.kubelet[38157]: E0516 19:34:59.348731   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podUID="478e2f54-5815-4945-accc-2453884e1a31"
2025-05-16T19:34:59-04:00 k8s.k8sd[35160]: I0516 19:34:59.403854   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:59-04:00 k8s.k8sd[35160]: I0516 19:34:59.504125   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:59-04:00 k8s.k8sd[35160]: I0516 19:34:59.518514   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:59-04:00 k8s.kube-apiserver[37236]: W0516 19:34:59.527017   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: connection refused
2025-05-16T19:34:59-04:00 k8s.k8sd[35160]: I0516 19:34:59.536213   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd.helm" controller="feature"
2025-05-16T19:34:59-04:00 k8s.k8sd[35160]: E0516 19:34:59.560659   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: connection refused" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:34:59-04:00 k8s.k8sd[35160]: I0516 19:34:59.939354   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:00-04:00 k8s.k8sd[35160]: I0516 19:35:00.100881   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:00-04:00 k8s.k8sd[35160]: I0516 19:35:00.200989   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:00.867816843-04:00" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/.05-cilium.conflist967968045\": REMOVE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
2025-05-16T19:35:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:00.868064370-04:00" level=error msg="failed to reload cni configuration after receiving fs change event(\"/etc/cni/net.d/.05-cilium.conflist3550305901006146277\": WRITE)" error="cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config"
2025-05-16T19:35:00-04:00 k8s.kubelet[38157]: I0516 19:35:00.943652   38157 kubelet_node_status.go:502] "Fast updating node status as it just became ready"
2025-05-16T19:35:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:35:01.327444   36331 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
2025-05-16T19:35:01-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:01.384269290-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:ck-storage-rawfile-csi-node-z9lvt,Uid:478e2f54-5815-4945-accc-2453884e1a31,Namespace:kube-system,Attempt:0,}"
2025-05-16T19:35:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:01.544627   36331 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
2025-05-16T19:35:02-04:00 k8s.k8sd[35160]: I0516 19:35:02.107515   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:02-04:00 k8s.k8sd[35160]: I0516 19:35:02.147232   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.128257   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.186075   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.192252   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.196777   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.211567   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.228190   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.241065   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.263715   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.301556   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.349508   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.370022   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.399801   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.429393   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.447617   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.465715   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.478214   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.495402   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.513008   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.528916   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.544565   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.560426   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.570737   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.581598   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.597551   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.614255   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.649554   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:04-04:00 k8s.k8sd[35160]: I0516 19:35:04.712351   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.377054   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.641196   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.649038   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.659429   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.678587   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.697885   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.723938   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.738900   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.750268   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.764493   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:05.769581   36331 node_lifecycle_controller.go:1057] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.777262   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.802673   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.813237   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.815998   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.837333   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.861861   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.885266   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.899346   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:05-04:00 k8s.k8sd[35160]: I0516 19:35:05.995077   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:06-04:00 k8s.k8sd[35160]: I0516 19:35:06.111342   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:06-04:00 k8s.k8sd[35160]: I0516 19:35:06.125342   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:06-04:00 k8s.kube-apiserver[37236]: W0516 19:35:06.134012   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: operation not permitted
2025-05-16T19:35:06-04:00 k8s.k8sd[35160]: I0516 19:35:06.147911   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:06-04:00 k8s.k8sd[35160]: E0516 19:35:06.216057   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:35:06-04:00 k8s.k8sd[35160]: I0516 19:35:06.260935   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:06-04:00 k8s.k8sd[35160]: I0516 19:35:06.459960   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:06-04:00 k8s.k8sd[35160]: I0516 19:35:06.693580   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:07-04:00 k8s.k8sd[35160]: I0516 19:35:07.853363   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.158143   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.210527   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.220753   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.244597   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.263295   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.275203   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.291463   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.321410   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.357019   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.380923   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.419640   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.441214   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.457168   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.469161   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.479028   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.488487   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.498514   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.509593   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.521889   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.536856   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.549355   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.564950   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.590301   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.641150   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.750883   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:09-04:00 k8s.kube-apiserver[37236]: I0516 19:35:09.755051   37236 controller.go:615] quota admission added evaluator for: ciliumendpoints.cilium.io
2025-05-16T19:35:09-04:00 k8s.k8sd[35160]: I0516 19:35:09.899018   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:11-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:11.018835   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="579.585µs"
2025-05-16T19:35:11-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:11.084727   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="91.119µs"
2025-05-16T19:35:11-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:11.092169   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="84.796µs"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: W0516 19:35:11.100227   38157 reflector.go:569] object-"metallb-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:research21" cannot list resource "configmaps" in API group "" in the namespace "metallb-system": no relationship found between node 'research21' and this object
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: E0516 19:35:11.101182   38157 reflector.go:166] "Unhandled Error" err="object-\"metallb-system\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"kube-root-ca.crt\" is forbidden: User \"system:node:research21\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"metallb-system\": no relationship found between node 'research21' and this object" logger="UnhandledError"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.130986   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d09748e0-ba21-4533-aff7-a6a92c617c59-config-volume\") pod \"coredns-56d5ddcf86-lwtgx\" (UID: \"d09748e0-ba21-4533-aff7-a6a92c617c59\") " pod="kube-system/coredns-56d5ddcf86-lwtgx"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.131057   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-56s48\" (UniqueName: \"kubernetes.io/projected/7cf93103-78ba-4736-9d96-1bf79fe69175-kube-api-access-56s48\") pod \"metrics-server-8694c96fb7-csxm7\" (UID: \"7cf93103-78ba-4736-9d96-1bf79fe69175\") " pod="kube-system/metrics-server-8694c96fb7-csxm7"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.131109   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r4v9k\" (UniqueName: \"kubernetes.io/projected/d4f4f71b-54e6-4625-9650-d34f0be00e4a-kube-api-access-r4v9k\") pod \"ck-storage-rawfile-csi-controller-0\" (UID: \"d4f4f71b-54e6-4625-9650-d34f0be00e4a\") " pod="kube-system/ck-storage-rawfile-csi-controller-0"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.131148   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-f5dgf\" (UniqueName: \"kubernetes.io/projected/d09748e0-ba21-4533-aff7-a6a92c617c59-kube-api-access-f5dgf\") pod \"coredns-56d5ddcf86-lwtgx\" (UID: \"d09748e0-ba21-4533-aff7-a6a92c617c59\") " pod="kube-system/coredns-56d5ddcf86-lwtgx"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.131191   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cert\" (UniqueName: \"kubernetes.io/secret/b3cfbcd2-921b-46d3-9bf3-e0e5aaa18e4f-cert\") pod \"metallb-controller-86cb6b5b76-2bm2k\" (UID: \"b3cfbcd2-921b-46d3-9bf3-e0e5aaa18e4f\") " pod="metallb-system/metallb-controller-86cb6b5b76-2bm2k"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.131232   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"socket-dir\" (UniqueName: \"kubernetes.io/empty-dir/d4f4f71b-54e6-4625-9650-d34f0be00e4a-socket-dir\") pod \"ck-storage-rawfile-csi-controller-0\" (UID: \"d4f4f71b-54e6-4625-9650-d34f0be00e4a\") " pod="kube-system/ck-storage-rawfile-csi-controller-0"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.131270   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6bvt9\" (UniqueName: \"kubernetes.io/projected/b3cfbcd2-921b-46d3-9bf3-e0e5aaa18e4f-kube-api-access-6bvt9\") pod \"metallb-controller-86cb6b5b76-2bm2k\" (UID: \"b3cfbcd2-921b-46d3-9bf3-e0e5aaa18e4f\") " pod="metallb-system/metallb-controller-86cb6b5b76-2bm2k"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.131312   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/empty-dir/7cf93103-78ba-4736-9d96-1bf79fe69175-tmp\") pod \"metrics-server-8694c96fb7-csxm7\" (UID: \"7cf93103-78ba-4736-9d96-1bf79fe69175\") " pod="kube-system/metrics-server-8694c96fb7-csxm7"
2025-05-16T19:35:11-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:11.325447   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="90.105µs"
2025-05-16T19:35:11-04:00 k8s.k8sd[35160]: I0516 19:35:11.347581   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.351714   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vh2ff\" (UniqueName: \"kubernetes.io/projected/abc93faf-9a7f-42ad-82f8-9f288a0660b8-kube-api-access-vh2ff\") pod \"metallb-speaker-lxl5s\" (UID: \"abc93faf-9a7f-42ad-82f8-9f288a0660b8\") " pod="metallb-system/metallb-speaker-lxl5s"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.351790   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"memberlist\" (UniqueName: \"kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist\") pod \"metallb-speaker-lxl5s\" (UID: \"abc93faf-9a7f-42ad-82f8-9f288a0660b8\") " pod="metallb-system/metallb-speaker-lxl5s"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: I0516 19:35:11.351838   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"metallb-excludel2\" (UniqueName: \"kubernetes.io/configmap/abc93faf-9a7f-42ad-82f8-9f288a0660b8-metallb-excludel2\") pod \"metallb-speaker-lxl5s\" (UID: \"abc93faf-9a7f-42ad-82f8-9f288a0660b8\") " pod="metallb-system/metallb-speaker-lxl5s"
2025-05-16T19:35:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:11.458081708-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:ck-storage-rawfile-csi-controller-0,Uid:d4f4f71b-54e6-4625-9650-d34f0be00e4a,Namespace:kube-system,Attempt:0,}"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: E0516 19:35:11.461035   38157 secret.go:189] Couldn't get secret metallb-system/metallb-memberlist: secret "metallb-memberlist" not found
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: E0516 19:35:11.484795   38157 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist podName:abc93faf-9a7f-42ad-82f8-9f288a0660b8 nodeName:}" failed. No retries permitted until 2025-05-16 19:35:11.96111738 -0400 EDT m=+105.098714054 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "memberlist" (UniqueName: "kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist") pod "metallb-speaker-lxl5s" (UID: "abc93faf-9a7f-42ad-82f8-9f288a0660b8") : secret "metallb-memberlist" not found
2025-05-16T19:35:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:11.490044314-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:coredns-56d5ddcf86-lwtgx,Uid:d09748e0-ba21-4533-aff7-a6a92c617c59,Namespace:kube-system,Attempt:0,}"
2025-05-16T19:35:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:11.672661931-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:metrics-server-8694c96fb7-csxm7,Uid:7cf93103-78ba-4736-9d96-1bf79fe69175,Namespace:kube-system,Attempt:0,}"
2025-05-16T19:35:11-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:11.697657   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="653.922µs"
2025-05-16T19:35:11-04:00 k8s.k8sd[35160]: I0516 19:35:11.740069   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:11-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:11.800149   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="97.805µs"
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: E0516 19:35:11.966472   38157 secret.go:189] Couldn't get secret metallb-system/metallb-memberlist: secret "metallb-memberlist" not found
2025-05-16T19:35:11-04:00 k8s.kubelet[38157]: E0516 19:35:11.966583   38157 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist podName:abc93faf-9a7f-42ad-82f8-9f288a0660b8 nodeName:}" failed. No retries permitted until 2025-05-16 19:35:12.966553982 -0400 EDT m=+106.104150669 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "memberlist" (UniqueName: "kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist") pod "metallb-speaker-lxl5s" (UID: "abc93faf-9a7f-42ad-82f8-9f288a0660b8") : secret "metallb-memberlist" not found
2025-05-16T19:35:12-04:00 k8s.k8sd[35160]: I0516 19:35:12.099328   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:12.667012355-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:metallb-controller-86cb6b5b76-2bm2k,Uid:b3cfbcd2-921b-46d3-9bf3-e0e5aaa18e4f,Namespace:metallb-system,Attempt:0,}"
2025-05-16T19:35:12-04:00 k8s.k8sd[35160]: I0516 19:35:12.776859   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:12-04:00 k8s.k8sd[35160]: I0516 19:35:12.888671   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:12-04:00 k8s.kube-apiserver[37236]: W0516 19:35:12.939157   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: operation not permitted
2025-05-16T19:35:12-04:00 k8s.kubelet[38157]: E0516 19:35:12.989592   38157 secret.go:189] Couldn't get secret metallb-system/metallb-memberlist: secret "metallb-memberlist" not found
2025-05-16T19:35:12-04:00 k8s.kubelet[38157]: E0516 19:35:12.992267   38157 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist podName:abc93faf-9a7f-42ad-82f8-9f288a0660b8 nodeName:}" failed. No retries permitted until 2025-05-16 19:35:14.990763258 -0400 EDT m=+108.128359947 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "memberlist" (UniqueName: "kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist") pod "metallb-speaker-lxl5s" (UID: "abc93faf-9a7f-42ad-82f8-9f288a0660b8") : secret "metallb-memberlist" not found
2025-05-16T19:35:13-04:00 k8s.k8sd[35160]: I0516 19:35:13.019244   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:13-04:00 k8s.k8sd[35160]: E0516 19:35:13.103966   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:35:13-04:00 k8s.k8sd[35160]: I0516 19:35:13.552790   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:13-04:00 k8s.k8sd[35160]: I0516 19:35:13.750383   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:15-04:00 k8s.kubelet[38157]: E0516 19:35:15.016840   38157 secret.go:189] Couldn't get secret metallb-system/metallb-memberlist: secret "metallb-memberlist" not found
2025-05-16T19:35:15-04:00 k8s.kubelet[38157]: E0516 19:35:15.016950   38157 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist podName:abc93faf-9a7f-42ad-82f8-9f288a0660b8 nodeName:}" failed. No retries permitted until 2025-05-16 19:35:19.016922597 -0400 EDT m=+112.154519280 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "memberlist" (UniqueName: "kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist") pod "metallb-speaker-lxl5s" (UID: "abc93faf-9a7f-42ad-82f8-9f288a0660b8") : secret "metallb-memberlist" not found
2025-05-16T19:35:15-04:00 k8s.k8sd[35160]: I0516 19:35:15.147442   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.592299   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.641394   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.653312   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.667532   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.682714   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.700997   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.721679   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.747837   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.782351   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.804501   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.823644   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.843438   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.858413   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.872310   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.882780   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.893098   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.902847   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.913436   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.924703   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.934895   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.946345   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.960025   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.974788   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:16-04:00 k8s.k8sd[35160]: I0516 19:35:16.989564   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:17-04:00 k8s.k8sd[35160]: I0516 19:35:17.014584   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:17-04:00 k8s.k8sd[35160]: I0516 19:35:17.078046   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:18-04:00 k8s.k8sd[35160]: I0516 19:35:18.167531   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:18.177965026-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:35:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:18.178182312-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:35:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:18.179697946-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:35:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:18.180309979-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29 pid=44744 runtime=io.containerd.runc.v2
2025-05-16T19:35:18-04:00 k8s.k8sd[35160]: I0516 19:35:18.295955   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:18.312116533-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:ck-storage-rawfile-csi-node-z9lvt,Uid:478e2f54-5815-4945-accc-2453884e1a31,Namespace:kube-system,Attempt:0,} returns sandbox id \"20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29\""
2025-05-16T19:35:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:18.315375990-04:00" level=info msg="PullImage \"ghcr.io/canonical/rawfile-localpv:0.8.0-ck4\""
2025-05-16T19:35:18-04:00 k8s.k8sd[35160]: I0516 19:35:18.414230   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:18-04:00 k8s.k8sd[35160]: I0516 19:35:18.511966   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:18-04:00 k8s.k8sd[35160]: I0516 19:35:18.524229   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:18-04:00 k8s.kube-apiserver[37236]: W0516 19:35:18.531255   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: operation not permitted
2025-05-16T19:35:18-04:00 k8s.k8sd[35160]: I0516 19:35:18.545734   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:18-04:00 k8s.k8sd[35160]: E0516 19:35:18.560491   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:35:18-04:00 k8s.k8sd[35160]: I0516 19:35:18.781952   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:18-04:00 k8s.k8sd[35160]: I0516 19:35:18.970083   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:19-04:00 k8s.kubelet[38157]: E0516 19:35:19.049291   38157 secret.go:189] Couldn't get secret metallb-system/metallb-memberlist: secret "metallb-memberlist" not found
2025-05-16T19:35:19-04:00 k8s.kubelet[38157]: E0516 19:35:19.049390   38157 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist podName:abc93faf-9a7f-42ad-82f8-9f288a0660b8 nodeName:}" failed. No retries permitted until 2025-05-16 19:35:27.04936449 -0400 EDT m=+120.186961173 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "memberlist" (UniqueName: "kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist") pod "metallb-speaker-lxl5s" (UID: "abc93faf-9a7f-42ad-82f8-9f288a0660b8") : secret "metallb-memberlist" not found
2025-05-16T19:35:20-04:00 k8s.k8sd[35160]: I0516 19:35:20.273027   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.626966   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.669998   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.682348   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.696857   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.711025   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.724102   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.741930   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.764352   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.793290   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.813595   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.833083   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.854652   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.884949   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.918851   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.958496   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.979787   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:21-04:00 k8s.k8sd[35160]: I0516 19:35:21.999573   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:22-04:00 k8s.k8sd[35160]: I0516 19:35:22.024114   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:22-04:00 k8s.k8sd[35160]: I0516 19:35:22.042123   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:22-04:00 k8s.k8sd[35160]: I0516 19:35:22.060198   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:22-04:00 k8s.k8sd[35160]: I0516 19:35:22.079730   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:22-04:00 k8s.k8sd[35160]: I0516 19:35:22.097348   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:22-04:00 k8s.k8sd[35160]: I0516 19:35:22.124617   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:22-04:00 k8s.k8sd[35160]: I0516 19:35:22.146709   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:22-04:00 k8s.k8sd[35160]: I0516 19:35:22.171825   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:22-04:00 k8s.k8sd[35160]: I0516 19:35:22.247009   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:23-04:00 k8s.k8sd[35160]: I0516 19:35:23.373631   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:23-04:00 k8s.k8sd[35160]: I0516 19:35:23.465206   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:23-04:00 k8s.k8sd[35160]: I0516 19:35:23.582752   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:23-04:00 k8s.k8sd[35160]: I0516 19:35:23.693234   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:23-04:00 k8s.k8sd[35160]: I0516 19:35:23.708849   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:23-04:00 k8s.kube-apiserver[37236]: W0516 19:35:23.717354   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: operation not permitted
2025-05-16T19:35:23-04:00 k8s.k8sd[35160]: I0516 19:35:23.727373   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:23-04:00 k8s.k8sd[35160]: E0516 19:35:23.750098   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:35:24-04:00 k8s.k8sd[35160]: I0516 19:35:24.042996   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:24-04:00 k8s.k8sd[35160]: I0516 19:35:24.330292   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:26-04:00 k8s.k8sd[35160]: I0516 19:35:26.002066   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.kubelet[38157]: E0516 19:35:27.134890   38157 secret.go:189] Couldn't get secret metallb-system/metallb-memberlist: secret "metallb-memberlist" not found
2025-05-16T19:35:27-04:00 k8s.kubelet[38157]: E0516 19:35:27.135009   38157 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist podName:abc93faf-9a7f-42ad-82f8-9f288a0660b8 nodeName:}" failed. No retries permitted until 2025-05-16 19:35:43.13498155 -0400 EDT m=+136.272578237 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "memberlist" (UniqueName: "kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist") pod "metallb-speaker-lxl5s" (UID: "abc93faf-9a7f-42ad-82f8-9f288a0660b8") : secret "metallb-memberlist" not found
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.208560108-04:00" level=info msg="StopPodSandbox for \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\""
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.208712532-04:00" level=info msg="TearDown network for sandbox \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\" successfully"
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.208739922-04:00" level=info msg="StopPodSandbox for \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\" returns successfully"
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.211773633-04:00" level=info msg="RemovePodSandbox for \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\""
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.211841168-04:00" level=info msg="Forcibly stopping sandbox \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\""
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.211953453-04:00" level=info msg="TearDown network for sandbox \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\" successfully"
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.221150895-04:00" level=info msg="RemovePodSandbox \"b34688fa49fd15a9526b3df49c7297c01eb975745acbc5d8f78c3fae1a74f7d3\" returns successfully"
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.221960134-04:00" level=info msg="StopPodSandbox for \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\""
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.222091790-04:00" level=info msg="TearDown network for sandbox \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\" successfully"
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.222116111-04:00" level=info msg="StopPodSandbox for \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\" returns successfully"
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.222621432-04:00" level=info msg="RemovePodSandbox for \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\""
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.222664508-04:00" level=info msg="Forcibly stopping sandbox \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\""
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.222752265-04:00" level=info msg="TearDown network for sandbox \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\" successfully"
2025-05-16T19:35:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:27.232092916-04:00" level=info msg="RemovePodSandbox \"de7bc94a33fa37d09998ad1ba2264ce142e258960aaf842951e9a43c3b94112b\" returns successfully"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.775547   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.818774   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.828653   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.849206   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.866599   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.889705   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.909752   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.933817   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.966260   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:27-04:00 k8s.k8sd[35160]: I0516 19:35:27.994428   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.026021   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.050421   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.084211   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.112507   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.151799   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.197833   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.227567   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.242975   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.267417   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.280605   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.296154   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.308589   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.331568   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.361879   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.393796   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:28-04:00 k8s.k8sd[35160]: I0516 19:35:28.456990   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:29-04:00 k8s.k8sd[35160]: I0516 19:35:29.596493   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:29-04:00 k8s.k8sd[35160]: I0516 19:35:29.687093   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:29-04:00 k8s.k8sd[35160]: I0516 19:35:29.841048   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:29.932510427-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/rawfile-localpv:0.8.0-ck4,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:29.938572964-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:d2f702e0653e165ab58cc441224d30bb99d8d3c579bae9ad0fe85f1c8fe8afba,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:29.942984743-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/rawfile-localpv@sha256:1c71b15fc1d592a31ffb79d4f3a52ec728a9a1c8bc56d66948d542263ed3b98e,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:29.943889809-04:00" level=info msg="PullImage \"ghcr.io/canonical/rawfile-localpv:0.8.0-ck4\" returns image reference \"sha256:d2f702e0653e165ab58cc441224d30bb99d8d3c579bae9ad0fe85f1c8fe8afba\""
2025-05-16T19:35:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:29.951749276-04:00" level=info msg="CreateContainer within sandbox \"20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29\" for container &ContainerMetadata{Name:csi-driver,Attempt:0,}"
2025-05-16T19:35:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:30.005368257-04:00" level=info msg="CreateContainer within sandbox \"20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29\" for &ContainerMetadata{Name:csi-driver,Attempt:0,} returns container id \"e00b29b9ca63ffaba6a20b67dcbb899ec692f6c8d7b3398d385feb25ce07153e\""
2025-05-16T19:35:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:30.007116645-04:00" level=info msg="StartContainer for \"e00b29b9ca63ffaba6a20b67dcbb899ec692f6c8d7b3398d385feb25ce07153e\""
2025-05-16T19:35:30-04:00 k8s.k8sd[35160]: I0516 19:35:30.026392   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:30-04:00 k8s.k8sd[35160]: I0516 19:35:30.061572   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:30-04:00 k8s.kube-apiserver[37236]: W0516 19:35:30.073165   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: operation not permitted
2025-05-16T19:35:30-04:00 k8s.k8sd[35160]: I0516 19:35:30.092930   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:30-04:00 k8s.k8sd[35160]: E0516 19:35:30.154730   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:35:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:30.314240134-04:00" level=info msg="StartContainer for \"e00b29b9ca63ffaba6a20b67dcbb899ec692f6c8d7b3398d385feb25ce07153e\" returns successfully"
2025-05-16T19:35:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:30.324166369-04:00" level=info msg="PullImage \"ghcr.io/canonical/k8s-snap/sig-storage/csi-node-driver-registrar:v2.10.1\""
2025-05-16T19:35:30-04:00 k8s.k8sd[35160]: I0516 19:35:30.536549   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:30-04:00 k8s.k8sd[35160]: I0516 19:35:30.894748   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:31-04:00 k8s.kube-controller-manager[36331]: E0516 19:35:31.364726   36331 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
2025-05-16T19:35:31-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:31.590694   36331 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
2025-05-16T19:35:32-04:00 k8s.k8sd[35160]: I0516 19:35:32.735476   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.140615809-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/sig-storage/csi-node-driver-registrar:v2.10.1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.151382899-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:36a3d5794c31c5e8100918592731079f59168b2a7aad1631ba0d416fe2668785,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.157275535-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/sig-storage/csi-node-driver-registrar@sha256:f25af73ee708ff9c82595ae99493cdef9295bd96953366cddf36305f82555dac,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.159221870-04:00" level=info msg="PullImage \"ghcr.io/canonical/k8s-snap/sig-storage/csi-node-driver-registrar:v2.10.1\" returns image reference \"sha256:36a3d5794c31c5e8100918592731079f59168b2a7aad1631ba0d416fe2668785\""
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.166582712-04:00" level=info msg="CreateContainer within sandbox \"20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29\" for container &ContainerMetadata{Name:node-driver-registrar,Attempt:0,}"
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.240441499-04:00" level=info msg="CreateContainer within sandbox \"20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29\" for &ContainerMetadata{Name:node-driver-registrar,Attempt:0,} returns container id \"24c62f9e839fe4fe36f0ba6984d6e39ecb7b3fd7c5274e6d1eb333b792730523\""
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.258131645-04:00" level=info msg="StartContainer for \"24c62f9e839fe4fe36f0ba6984d6e39ecb7b3fd7c5274e6d1eb333b792730523\""
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.601533824-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.601735131-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.601762468-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.609992528-04:00" level=info msg="StartContainer for \"24c62f9e839fe4fe36f0ba6984d6e39ecb7b3fd7c5274e6d1eb333b792730523\" returns successfully"
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.611855131-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/3de76bcb280d4fddb6c67d49c42fea80fb510266541f9d2a45cfa9fa2496bc21 pid=45031 runtime=io.containerd.runc.v2
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.632160204-04:00" level=info msg="PullImage \"ghcr.io/canonical/k8s-snap/sig-storage/csi-provisioner:v5.0.1\""
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.685629696-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.688473658-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.694983342-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.715510746-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/905036c25ead8fd8d8061ba4fc23ec4f9a0881578138739d9aa8683eeb24989a pid=45049 runtime=io.containerd.runc.v2
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.782344346-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.782928530-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.783081223-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.783866871-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/c734d8f2524c61ba6eeff3002e11618d5aff534ad9278919e6bec73eb3d3a5dd pid=45075 runtime=io.containerd.runc.v2
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.972922284-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.973429984-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.974072605-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:35:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:33.975638437-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/5d0a47da3f60314eb9d2d1554870f2fe8c4ec1d75deb1f246be3d3d56770a44a pid=45144 runtime=io.containerd.runc.v2
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.150849679-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:metrics-server-8694c96fb7-csxm7,Uid:7cf93103-78ba-4736-9d96-1bf79fe69175,Namespace:kube-system,Attempt:0,} returns sandbox id \"c734d8f2524c61ba6eeff3002e11618d5aff534ad9278919e6bec73eb3d3a5dd\""
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.177710207-04:00" level=info msg="PullImage \"ghcr.io/canonical/metrics-server:0.7.2-ck0\""
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.246743279-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:ck-storage-rawfile-csi-controller-0,Uid:d4f4f71b-54e6-4625-9650-d34f0be00e4a,Namespace:kube-system,Attempt:0,} returns sandbox id \"905036c25ead8fd8d8061ba4fc23ec4f9a0881578138739d9aa8683eeb24989a\""
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.251653678-04:00" level=info msg="PullImage \"ghcr.io/canonical/rawfile-localpv:0.8.0-ck4\""
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.432680283-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:metallb-controller-86cb6b5b76-2bm2k,Uid:b3cfbcd2-921b-46d3-9bf3-e0e5aaa18e4f,Namespace:metallb-system,Attempt:0,} returns sandbox id \"5d0a47da3f60314eb9d2d1554870f2fe8c4ec1d75deb1f246be3d3d56770a44a\""
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.443193508-04:00" level=info msg="PullImage \"ghcr.io/canonical/metallb-controller:v0.14.8-ck0\""
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.485903957-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:coredns-56d5ddcf86-lwtgx,Uid:d09748e0-ba21-4533-aff7-a6a92c617c59,Namespace:kube-system,Attempt:0,} returns sandbox id \"3de76bcb280d4fddb6c67d49c42fea80fb510266541f9d2a45cfa9fa2496bc21\""
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.499010118-04:00" level=info msg="PullImage \"ghcr.io/canonical/coredns:1.11.3-ck0\""
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.537503684-04:00" level=info msg="ImageUpdate event &ImageUpdate{Name:ghcr.io/canonical/rawfile-localpv:0.8.0-ck4,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.585814967-04:00" level=info msg="PullImage \"ghcr.io/canonical/rawfile-localpv:0.8.0-ck4\" returns image reference \"sha256:d2f702e0653e165ab58cc441224d30bb99d8d3c579bae9ad0fe85f1c8fe8afba\""
2025-05-16T19:35:34-04:00 k8s.kubelet[38157]: I0516 19:35:34.589759   38157 csi_plugin.go:100] kubernetes.io/csi: Trying to validate a new CSI Driver with name: rawfile.csi.openebs.io endpoint: /var/lib/kubelet/plugins/rawfile-csi/csi.sock versions: 1.0.0
2025-05-16T19:35:34-04:00 k8s.kubelet[38157]: I0516 19:35:34.589825   38157 csi_plugin.go:113] kubernetes.io/csi: Register new plugin with name: rawfile.csi.openebs.io at endpoint: /var/lib/kubelet/plugins/rawfile-csi/csi.sock
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.699762648-04:00" level=info msg="CreateContainer within sandbox \"905036c25ead8fd8d8061ba4fc23ec4f9a0881578138739d9aa8683eeb24989a\" for container &ContainerMetadata{Name:csi-driver,Attempt:0,}"
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.894101953-04:00" level=info msg="CreateContainer within sandbox \"905036c25ead8fd8d8061ba4fc23ec4f9a0881578138739d9aa8683eeb24989a\" for &ContainerMetadata{Name:csi-driver,Attempt:0,} returns container id \"57b27bfae46475fe5b4a407e2b0fc92cb530db4ac838e9da340ddbbe8a3cf5a0\""
2025-05-16T19:35:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:34.899296994-04:00" level=info msg="StartContainer for \"57b27bfae46475fe5b4a407e2b0fc92cb530db4ac838e9da340ddbbe8a3cf5a0\""
2025-05-16T19:35:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:35.336699027-04:00" level=info msg="StartContainer for \"57b27bfae46475fe5b4a407e2b0fc92cb530db4ac838e9da340ddbbe8a3cf5a0\" returns successfully"
2025-05-16T19:35:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:35.354741591-04:00" level=info msg="PullImage \"ghcr.io/canonical/k8s-snap/sig-storage/csi-resizer:v1.11.1\""
2025-05-16T19:35:35-04:00 k8s.k8sd[35160]: I0516 19:35:35.989163   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.140220   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.157191   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.178449   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.201341   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.225063   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.258416   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.301399   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.338789   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.373264   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.427284   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.499210   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.532311   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.554682   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.581117   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.615153   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.638654   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.662404   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.691916   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.712844   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.735973   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.757332   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.790776   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.831341   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.881316   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:36-04:00 k8s.k8sd[35160]: I0516 19:35:36.974415   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:38-04:00 k8s.k8sd[35160]: I0516 19:35:38.235893   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:38-04:00 k8s.k8sd[35160]: I0516 19:35:38.421783   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:38-04:00 k8s.k8sd[35160]: I0516 19:35:38.700703   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:38-04:00 k8s.k8sd[35160]: I0516 19:35:38.961727   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:38-04:00 k8s.k8sd[35160]: I0516 19:35:38.998970   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:39-04:00 k8s.kube-apiserver[37236]: W0516 19:35:39.025861   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: operation not permitted
2025-05-16T19:35:39-04:00 k8s.k8sd[35160]: I0516 19:35:39.057892   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:39-04:00 k8s.k8sd[35160]: E0516 19:35:39.110580   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:35:39-04:00 k8s.k8sd[35160]: I0516 19:35:39.792130   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:40-04:00 k8s.k8sd[35160]: I0516 19:35:40.184934   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:42-04:00 k8s.k8sd[35160]: I0516 19:35:42.305043   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:43-04:00 k8s.kubelet[38157]: E0516 19:35:43.235711   38157 secret.go:189] Couldn't get secret metallb-system/metallb-memberlist: secret "metallb-memberlist" not found
2025-05-16T19:35:43-04:00 k8s.kubelet[38157]: E0516 19:35:43.235833   38157 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist podName:abc93faf-9a7f-42ad-82f8-9f288a0660b8 nodeName:}" failed. No retries permitted until 2025-05-16 19:36:15.235803357 -0400 EDT m=+168.373400044 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "memberlist" (UniqueName: "kubernetes.io/secret/abc93faf-9a7f-42ad-82f8-9f288a0660b8-memberlist") pod "metallb-speaker-lxl5s" (UID: "abc93faf-9a7f-42ad-82f8-9f288a0660b8") : secret "metallb-memberlist" not found
2025-05-16T19:35:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:43.871829624-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/sig-storage/csi-provisioner:v5.0.1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:43.904110648-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:427403f00b9eac41b7e4d340b99e53e7dd9f673e8b7514b7a2415b7972c4dce0,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:43.942211497-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/sig-storage/csi-provisioner@sha256:405a14e1aa702f7ea133cea459e8395fe40a6125c088c55569e696d48e1bd385,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:43.944916354-04:00" level=info msg="PullImage \"ghcr.io/canonical/k8s-snap/sig-storage/csi-provisioner:v5.0.1\" returns image reference \"sha256:427403f00b9eac41b7e4d340b99e53e7dd9f673e8b7514b7a2415b7972c4dce0\""
2025-05-16T19:35:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:43.987802827-04:00" level=info msg="CreateContainer within sandbox \"20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29\" for container &ContainerMetadata{Name:external-provisioner,Attempt:0,}"
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.067698680-04:00" level=info msg="CreateContainer within sandbox \"20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29\" for &ContainerMetadata{Name:external-provisioner,Attempt:0,} returns container id \"cbaf94e481896932ced45e72b503defc47e47abaf0ae101bf28779b78a171a0a\""
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.086711442-04:00" level=info msg="StartContainer for \"cbaf94e481896932ced45e72b503defc47e47abaf0ae101bf28779b78a171a0a\""
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.405601059-04:00" level=info msg="StartContainer for \"cbaf94e481896932ced45e72b503defc47e47abaf0ae101bf28779b78a171a0a\" returns successfully"
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.454140886-04:00" level=info msg="PullImage \"ghcr.io/canonical/k8s-snap/sig-storage/csi-snapshotter:v8.0.1\""
2025-05-16T19:35:44-04:00 k8s.kube-apiserver[37236]: I0516 19:35:44.671850   37236 controller.go:615] quota admission added evaluator for: csistoragecapacities.storage.k8s.io
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.685951539-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/coredns:1.11.3-ck0,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.702830763-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:a100342dc38d710a03290ee1d3ba2cf31f66e7956bd686a1c1125012f795e6f8,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.714566761-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/coredns@sha256:3dd921a2bb6496885c30155afc31185c490cd4fbb63dcf8b478f91ce92dc3fe9,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.718699880-04:00" level=info msg="PullImage \"ghcr.io/canonical/coredns:1.11.3-ck0\" returns image reference \"sha256:a100342dc38d710a03290ee1d3ba2cf31f66e7956bd686a1c1125012f795e6f8\""
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.729173622-04:00" level=info msg="CreateContainer within sandbox \"3de76bcb280d4fddb6c67d49c42fea80fb510266541f9d2a45cfa9fa2496bc21\" for container &ContainerMetadata{Name:coredns,Attempt:0,}"
2025-05-16T19:35:44-04:00 k8s.k8sd[35160]: I0516 19:35:44.761827   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.771959051-04:00" level=info msg="CreateContainer within sandbox \"3de76bcb280d4fddb6c67d49c42fea80fb510266541f9d2a45cfa9fa2496bc21\" for &ContainerMetadata{Name:coredns,Attempt:0,} returns container id \"5f1bbd1493a59f8b154fb6a8d162506e4ab2b7f4e393f3735e38d98bef643d94\""
2025-05-16T19:35:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:44.783789290-04:00" level=info msg="StartContainer for \"5f1bbd1493a59f8b154fb6a8d162506e4ab2b7f4e393f3735e38d98bef643d94\""
2025-05-16T19:35:44-04:00 k8s.k8sd[35160]: I0516 19:35:44.826430   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:44-04:00 k8s.k8sd[35160]: I0516 19:35:44.841321   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:44-04:00 k8s.k8sd[35160]: I0516 19:35:44.870793   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:44-04:00 k8s.k8sd[35160]: I0516 19:35:44.898274   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:44-04:00 k8s.k8sd[35160]: I0516 19:35:44.930912   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:44-04:00 k8s.k8sd[35160]: I0516 19:35:44.959552   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:44-04:00 k8s.k8sd[35160]: I0516 19:35:44.991790   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.034441   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgppeers.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.047490871-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/metrics-server:0.7.2-ck0,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.064890   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.070382003-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:f3aac59a876bc1bbc33f6429186dcca31b1bda2ddc749556050321dfe821d37a,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.076976078-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/metrics-server@sha256:2b94444cf67479f2fe77e353f64d04aab98a222c057cd40b2000aff9a2fb1682,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.077467986-04:00" level=info msg="PullImage \"ghcr.io/canonical/metrics-server:0.7.2-ck0\" returns image reference \"sha256:f3aac59a876bc1bbc33f6429186dcca31b1bda2ddc749556050321dfe821d37a\""
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.089828805-04:00" level=info msg="CreateContainer within sandbox \"c734d8f2524c61ba6eeff3002e11618d5aff534ad9278919e6bec73eb3d3a5dd\" for container &ContainerMetadata{Name:metrics-server,Attempt:0,}"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.112442   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.140628584-04:00" level=info msg="StartContainer for \"5f1bbd1493a59f8b154fb6a8d162506e4ab2b7f4e393f3735e38d98bef643d94\" returns successfully"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.149355   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.207719382-04:00" level=info msg="CreateContainer within sandbox \"c734d8f2524c61ba6eeff3002e11618d5aff534ad9278919e6bec73eb3d3a5dd\" for &ContainerMetadata{Name:metrics-server,Attempt:0,} returns container id \"f261aecb1020dec6de63a0c6ee5d47ce93981b9642d00757735f695abd325d16\""
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.210618553-04:00" level=info msg="StartContainer for \"f261aecb1020dec6de63a0c6ee5d47ce93981b9642d00757735f695abd325d16\""
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.220015   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.254400   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.283169   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.307743   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.332410   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.359164   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.379024   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.423722   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.448564   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.472352   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.482152001-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/sig-storage/csi-resizer:v1.11.1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.487916533-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:95ba1a4c52f0c78acc2a5a7880fe3ef6e485f0fcb543751434c76a1af8eab263,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.505357   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.510304883-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/sig-storage/csi-resizer@sha256:a541e6cc2d8b011bb21b1d4ffec6b090e85270cce6276ee302d86153eec0af43,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.514694290-04:00" level=info msg="PullImage \"ghcr.io/canonical/k8s-snap/sig-storage/csi-resizer:v1.11.1\" returns image reference \"sha256:95ba1a4c52f0c78acc2a5a7880fe3ef6e485f0fcb543751434c76a1af8eab263\""
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.528116768-04:00" level=info msg="CreateContainer within sandbox \"905036c25ead8fd8d8061ba4fc23ec4f9a0881578138739d9aa8683eeb24989a\" for container &ContainerMetadata{Name:external-resizer,Attempt:0,}"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.541780   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.559673670-04:00" level=info msg="StartContainer for \"f261aecb1020dec6de63a0c6ee5d47ce93981b9642d00757735f695abd325d16\" returns successfully"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.561717841-04:00" level=info msg="CreateContainer within sandbox \"905036c25ead8fd8d8061ba4fc23ec4f9a0881578138739d9aa8683eeb24989a\" for &ContainerMetadata{Name:external-resizer,Attempt:0,} returns container id \"bf0645f30d18cdbcc19c29daaffa14f168531be74827b59c12d9edd1739e9ad1\""
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.582496014-04:00" level=info msg="StartContainer for \"bf0645f30d18cdbcc19c29daaffa14f168531be74827b59c12d9edd1739e9ad1\""
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.626500   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:45.728703   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="2.261988ms"
2025-05-16T19:35:45-04:00 k8s.k8sd[35160]: I0516 19:35:45.765082   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:45-04:00 k8s.kubelet[38157]: I0516 19:35:45.898278   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/metrics-server-8694c96fb7-csxm7" podStartSLOduration=123.994447416 podStartE2EDuration="2m14.898178793s" podCreationTimestamp="2025-05-16 19:33:31 -0400 EDT" firstStartedPulling="2025-05-16 19:35:34.176831805 -0400 EDT m=+127.314428490" lastFinishedPulling="2025-05-16 19:35:45.080563182 -0400 EDT m=+138.218159867" observedRunningTime="2025-05-16 19:35:45.881259289 -0400 EDT m=+139.018855976" watchObservedRunningTime="2025-05-16 19:35:45.898178793 -0400 EDT m=+139.035775485"
2025-05-16T19:35:45-04:00 k8s.kubelet[38157]: I0516 19:35:45.898665   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-56d5ddcf86-lwtgx" podStartSLOduration=124.669006313 podStartE2EDuration="2m14.898650306s" podCreationTimestamp="2025-05-16 19:33:31 -0400 EDT" firstStartedPulling="2025-05-16 19:35:34.493611174 -0400 EDT m=+127.631207859" lastFinishedPulling="2025-05-16 19:35:44.723255165 -0400 EDT m=+137.860851852" observedRunningTime="2025-05-16 19:35:45.732256283 -0400 EDT m=+138.869852956" watchObservedRunningTime="2025-05-16 19:35:45.898650306 -0400 EDT m=+139.036246987"
2025-05-16T19:35:45-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:45.899566   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="74.864µs"
2025-05-16T19:35:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:45.971645043-04:00" level=info msg="StartContainer for \"bf0645f30d18cdbcc19c29daaffa14f168531be74827b59c12d9edd1739e9ad1\" returns successfully"
2025-05-16T19:35:47-04:00 k8s.k8sd[35160]: I0516 19:35:47.103908   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:47-04:00 k8s.k8sd[35160]: I0516 19:35:47.279641   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:47-04:00 k8s.k8sd[35160]: I0516 19:35:47.504561   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:47-04:00 k8s.k8sd[35160]: I0516 19:35:47.688549   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:47-04:00 k8s.k8sd[35160]: I0516 19:35:47.708485   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:47-04:00 k8s.kube-apiserver[37236]: W0516 19:35:47.721605   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: operation not permitted
2025-05-16T19:35:47-04:00 k8s.k8sd[35160]: I0516 19:35:47.733509   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:47-04:00 k8s.k8sd[35160]: E0516 19:35:47.756595   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:35:48-04:00 k8s.k8sd[35160]: I0516 19:35:48.194503   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:48.591884286-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/metallb-controller:v0.14.8-ck0,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:48.602552069-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:b23c8468853b625950e7428092016726c754a5c8b23f825b06762d0d41e2f19a,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:48-04:00 k8s.k8sd[35160]: I0516 19:35:48.608537   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:48.612754132-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/metallb-controller@sha256:2ab3fb5477d7966d95f1084f60c856c633d518c12958dc5c0eee389ed098c3e2,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:48.616981242-04:00" level=info msg="PullImage \"ghcr.io/canonical/metallb-controller:v0.14.8-ck0\" returns image reference \"sha256:b23c8468853b625950e7428092016726c754a5c8b23f825b06762d0d41e2f19a\""
2025-05-16T19:35:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:48.626777504-04:00" level=info msg="CreateContainer within sandbox \"5d0a47da3f60314eb9d2d1554870f2fe8c4ec1d75deb1f246be3d3d56770a44a\" for container &ContainerMetadata{Name:controller,Attempt:0,}"
2025-05-16T19:35:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:48.700972670-04:00" level=info msg="CreateContainer within sandbox \"5d0a47da3f60314eb9d2d1554870f2fe8c4ec1d75deb1f246be3d3d56770a44a\" for &ContainerMetadata{Name:controller,Attempt:0,} returns container id \"c93ba691226c07b5fba347145e3c2984cf74c84be6836bd9a660557d18ebdb2f\""
2025-05-16T19:35:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:48.703418357-04:00" level=info msg="StartContainer for \"c93ba691226c07b5fba347145e3c2984cf74c84be6836bd9a660557d18ebdb2f\""
2025-05-16T19:35:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:49.027148941-04:00" level=info msg="StartContainer for \"c93ba691226c07b5fba347145e3c2984cf74c84be6836bd9a660557d18ebdb2f\" returns successfully"
2025-05-16T19:35:49-04:00 k8s.kube-controller-manager[36331]: I0516 19:35:49.819735   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="87.131µs"
2025-05-16T19:35:49-04:00 k8s.kubelet[38157]: I0516 19:35:49.820423   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/ck-storage-rawfile-csi-controller-0" podStartSLOduration=126.552362735 podStartE2EDuration="2m17.820399772s" podCreationTimestamp="2025-05-16 19:33:32 -0400 EDT" firstStartedPulling="2025-05-16 19:35:34.251006728 -0400 EDT m=+127.388603400" lastFinishedPulling="2025-05-16 19:35:45.519043753 -0400 EDT m=+138.656640437" observedRunningTime="2025-05-16 19:35:46.772045178 -0400 EDT m=+139.909641871" watchObservedRunningTime="2025-05-16 19:35:49.820399772 -0400 EDT m=+142.957996446"
2025-05-16T19:35:49-04:00 k8s.kubelet[38157]: I0516 19:35:49.820639   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="metallb-system/metallb-controller-86cb6b5b76-2bm2k" podStartSLOduration=124.639666849 podStartE2EDuration="2m18.820628527s" podCreationTimestamp="2025-05-16 19:33:31 -0400 EDT" firstStartedPulling="2025-05-16 19:35:34.441904911 -0400 EDT m=+127.579501594" lastFinishedPulling="2025-05-16 19:35:48.622866588 -0400 EDT m=+141.760463272" observedRunningTime="2025-05-16 19:35:49.820385444 -0400 EDT m=+142.957982149" watchObservedRunningTime="2025-05-16 19:35:49.820628527 -0400 EDT m=+142.958225201"
2025-05-16T19:35:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:49.861064959-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/sig-storage/csi-snapshotter:v8.0.1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:49.870706713-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:a011c41a1df07bf5a3c81341ff315b708dd9e448af8eee7213f8d97230d0a524,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:49.879544159-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/k8s-snap/sig-storage/csi-snapshotter@sha256:2e04046334baf9be425bb0fa1d04c2d1720d770825eedbdbcdb10d430da4ad8c,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:35:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:49.882816207-04:00" level=info msg="PullImage \"ghcr.io/canonical/k8s-snap/sig-storage/csi-snapshotter:v8.0.1\" returns image reference \"sha256:a011c41a1df07bf5a3c81341ff315b708dd9e448af8eee7213f8d97230d0a524\""
2025-05-16T19:35:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:49.889926894-04:00" level=info msg="CreateContainer within sandbox \"20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29\" for container &ContainerMetadata{Name:external-snapshotter,Attempt:0,}"
2025-05-16T19:35:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:49.928343625-04:00" level=info msg="CreateContainer within sandbox \"20800e54511449e46ddb138e4c334806383f3bee68592baf7b6bd48a2cb84a29\" for &ContainerMetadata{Name:external-snapshotter,Attempt:0,} returns container id \"f03c6cb84d6458ce297d53d27ad424f38bf81ab953feb1cef13809cf69d4246b\""
2025-05-16T19:35:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:49.933624769-04:00" level=info msg="StartContainer for \"f03c6cb84d6458ce297d53d27ad424f38bf81ab953feb1cef13809cf69d4246b\""
2025-05-16T19:35:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:35:50.136470255-04:00" level=info msg="StartContainer for \"f03c6cb84d6458ce297d53d27ad424f38bf81ab953feb1cef13809cf69d4246b\" returns successfully"
2025-05-16T19:35:50-04:00 k8s.k8sd[35160]: I0516 19:35:50.495026   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:50-04:00 k8s.kubelet[38157]: I0516 19:35:50.799440   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/ck-storage-rawfile-csi-node-z9lvt" podStartSLOduration=108.228123757 podStartE2EDuration="2m19.799414449s" podCreationTimestamp="2025-05-16 19:33:31 -0400 EDT" firstStartedPulling="2025-05-16 19:35:18.314789165 -0400 EDT m=+111.452385836" lastFinishedPulling="2025-05-16 19:35:49.886079843 -0400 EDT m=+143.023676528" observedRunningTime="2025-05-16 19:35:50.793643584 -0400 EDT m=+143.931240273" watchObservedRunningTime="2025-05-16 19:35:50.799414449 -0400 EDT m=+143.937011143"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.705837   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.748567   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.756798   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.766655   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.779589   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.791595   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.804178   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.822372   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.kube-apiserver[37236]: W0516 19:35:51.838027   37236 cacher.go:171] Terminating all watchers from cacher bgppeers.metallb.io
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.844750   35160 helm/client.go:40] "Patch CustomResourceDefinition \"bgppeers.metallb.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.913158   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.932444   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.964839   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.984727   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:51-04:00 k8s.k8sd[35160]: I0516 19:35:51.999915   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.013370   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.023286   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.035091   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.045307   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.055385   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.066389   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.076250   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.085734   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.100023   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.114621   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.135569   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:52-04:00 k8s.k8sd[35160]: I0516 19:35:52.208089   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:53-04:00 k8s.k8sd[35160]: I0516 19:35:53.333731   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:53-04:00 k8s.k8sd[35160]: I0516 19:35:53.405895   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:53-04:00 k8s.k8sd[35160]: I0516 19:35:53.500967   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:53-04:00 k8s.k8sd[35160]: I0516 19:35:53.586575   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:53-04:00 k8s.k8sd[35160]: I0516 19:35:53.604191   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:53-04:00 k8s.kube-apiserver[37236]: W0516 19:35:53.610434   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: operation not permitted
2025-05-16T19:35:53-04:00 k8s.k8sd[35160]: I0516 19:35:53.618269   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:53-04:00 k8s.k8sd[35160]: E0516 19:35:53.633181   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:35:53-04:00 k8s.k8sd[35160]: I0516 19:35:53.892600   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:54-04:00 k8s.k8sd[35160]: I0516 19:35:54.202887   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:55-04:00 k8s.kube-controller-manager[36331]: W0516 19:35:55.132619   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:35:55-04:00 k8s.kube-controller-manager[36331]: E0516 19:35:55.134436   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta2, Resource=bgppeers"
2025-05-16T19:35:55-04:00 k8s.k8sd[35160]: I0516 19:35:55.257509   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.438410   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.477221   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.485826   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.494806   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.507448   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.518463   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.531735   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.547623   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.568373   35160 helm/client.go:40] "Patch CustomResourceDefinition \"bgppeers.metallb.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.609496   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.628796   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.656369   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.679207   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.697657   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.707834   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.719110   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.731974   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.742813   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.755980   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.768201   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.781241   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.794123   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.808118   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.820948   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.844687   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:35:56-04:00 k8s.k8sd[35160]: I0516 19:35:56.896010   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:57-04:00 k8s.kube-apiserver[37236]: W0516 19:35:57.600698   37236 cacher.go:171] Terminating all watchers from cacher bgppeers.metallb.io
2025-05-16T19:35:58-04:00 k8s.k8sd[35160]: I0516 19:35:58.003158   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:58-04:00 k8s.k8sd[35160]: I0516 19:35:58.090908   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:58-04:00 k8s.k8sd[35160]: I0516 19:35:58.189447   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:58-04:00 k8s.k8sd[35160]: I0516 19:35:58.280879   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:58-04:00 k8s.k8sd[35160]: I0516 19:35:58.296016   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:58-04:00 k8s.kube-apiserver[37236]: W0516 19:35:58.303679   37236 dispatcher.go:217] Failed calling webhook, failing closed ipaddresspoolvalidationwebhook.metallb.io: failed calling webhook "ipaddresspoolvalidationwebhook.metallb.io": failed to call webhook: Post "https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s": dial tcp 10.152.183.205:443: connect: operation not permitted
2025-05-16T19:35:58-04:00 k8s.k8sd[35160]: I0516 19:35:58.313061   35160 helm/client.go:40] "warning: Upgrade \"metallb-loadbalancer\" failed: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:58-04:00 k8s.k8sd[35160]: E0516 19:35:58.327911   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable LoadBalancer: failed to apply MetalLB LoadBalancer configuration: failed to upgrade metallb-loadbalancer: failed to create resource: Internal error occurred: failed calling webhook \"ipaddresspoolvalidationwebhook.metallb.io\": failed to call webhook: Post \"https://metallb-webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s\": dial tcp 10.152.183.205:443: connect: operation not permitted" logger="k8sd" controller="feature" feature="load-balancer"
2025-05-16T19:35:58-04:00 k8s.k8sd[35160]: I0516 19:35:58.951169   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:35:59-04:00 k8s.k8sd[35160]: I0516 19:35:59.243559   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:00-04:00 k8s.k8sd[35160]: I0516 19:36:00.302634   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:00-04:00 k8s.kube-controller-manager[36331]: I0516 19:36:00.825390   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="31.264839ms"
2025-05-16T19:36:00-04:00 k8s.kube-controller-manager[36331]: I0516 19:36:00.830315   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="86.057µs"
2025-05-16T19:36:00-04:00 k8s.kube-controller-manager[36331]: W0516 19:36:00.915248   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:36:00-04:00 k8s.kube-controller-manager[36331]: E0516 19:36:00.918328   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta2, Resource=bgppeers"
2025-05-16T19:36:01-04:00 k8s.kube-controller-manager[36331]: E0516 19:36:01.373477   36331 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.509102   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.547040   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.560301   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.573346   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.586430   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.597938   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.kube-controller-manager[36331]: I0516 19:36:01.606507   36331 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.612006   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.629067   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.655353   35160 helm/client.go:40] "Patch CustomResourceDefinition \"bgppeers.metallb.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.697022   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.717152   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.750646   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.768752   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.784206   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.794061   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.805189   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.814848   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.825622   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.835309   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.844725   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.854561   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.864152   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.877205   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.892527   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.914562   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:36:01-04:00 k8s.k8sd[35160]: I0516 19:36:01.964274   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:02-04:00 k8s.kube-apiserver[37236]: W0516 19:36:02.686625   37236 cacher.go:171] Terminating all watchers from cacher bgppeers.metallb.io
2025-05-16T19:36:03-04:00 k8s.k8sd[35160]: I0516 19:36:03.081687   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:03-04:00 k8s.k8sd[35160]: I0516 19:36:03.176712   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:03-04:00 k8s.k8sd[35160]: I0516 19:36:03.275478   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:03-04:00 k8s.k8sd[35160]: I0516 19:36:03.373017   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:03-04:00 k8s.k8sd[35160]: I0516 19:36:03.385954   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:03-04:00 k8s.kube-apiserver[37236]: I0516 19:36:03.405001   37236 controller.go:615] quota admission added evaluator for: ipaddresspools.metallb.io
2025-05-16T19:36:03-04:00 k8s.k8sd[35160]: I0516 19:36:03.414940   35160 helm/client.go:40] "Created a new IPAddressPool called \"metallb-loadbalancer-ck-loadbalancer\" in metallb-system\n" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:03-04:00 k8s.kube-apiserver[37236]: I0516 19:36:03.445194   37236 controller.go:615] quota admission added evaluator for: l2advertisements.metallb.io
2025-05-16T19:36:03-04:00 k8s.k8sd[35160]: I0516 19:36:03.450654   35160 helm/client.go:40] "Created a new L2Advertisement called \"metallb-loadbalancer-ck-loadbalancer\" in metallb-system\n" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:03-04:00 k8s.k8sd[35160]: I0516 19:36:03.461729   35160 helm/client.go:40] "updating status for upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:03-04:00 k8s.k8sd[35160]: I0516 19:36:03.775394   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:04-04:00 k8s.k8sd[35160]: I0516 19:36:04.148709   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:05-04:00 k8s.k8sd[35160]: I0516 19:36:05.267784   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.432360   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.474600   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.483260   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.492282   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.503116   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.512825   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.527257   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.549768   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.574169   35160 helm/client.go:40] "Patch CustomResourceDefinition \"bgppeers.metallb.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.622593   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.648717   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.680192   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.710489   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.740428   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.kube-controller-manager[36331]: I0516 19:36:06.762880   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="22.276657ms"
2025-05-16T19:36:06-04:00 k8s.kube-controller-manager[36331]: I0516 19:36:06.767611   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="81.754µs"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.770900   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.kube-apiserver[37236]: E0516 19:36:06.787214   37236 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.152.183.156:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.152.183.156:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.152.183.156:443: connect: operation not permitted" logger="UnhandledError"
2025-05-16T19:36:06-04:00 k8s.kube-apiserver[37236]: W0516 19:36:06.787540   37236 handler_proxy.go:99] no RequestInfo found in the context
2025-05-16T19:36:06-04:00 k8s.kube-apiserver[37236]: E0516 19:36:06.787627   37236 controller.go:146] "Unhandled Error" err=<
2025-05-16T19:36:06-04:00 k8s.kube-apiserver[37236]: 	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
2025-05-16T19:36:06-04:00 k8s.kube-apiserver[37236]: 	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
2025-05-16T19:36:06-04:00 k8s.kube-apiserver[37236]:  > logger="UnhandledError"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.806410   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.kube-apiserver[37236]: I0516 19:36:06.831075   37236 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.849927   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.887940   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.907521   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.929103   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.946252   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.962733   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:06-04:00 k8s.k8sd[35160]: I0516 19:36:06.985179   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:07-04:00 k8s.k8sd[35160]: I0516 19:36:07.012910   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:07-04:00 k8s.k8sd[35160]: I0516 19:36:07.045375   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:36:07-04:00 k8s.k8sd[35160]: I0516 19:36:07.103957   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:07-04:00 k8s.kube-controller-manager[36331]: W0516 19:36:07.954050   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:36:07-04:00 k8s.kube-controller-manager[36331]: E0516 19:36:07.956317   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta2, Resource=bgppeers"
2025-05-16T19:36:08-04:00 k8s.k8sd[35160]: I0516 19:36:08.237583   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:08-04:00 k8s.k8sd[35160]: I0516 19:36:08.299637   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:08-04:00 k8s.k8sd[35160]: I0516 19:36:08.404999   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:08-04:00 k8s.k8sd[35160]: I0516 19:36:08.490470   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:08-04:00 k8s.k8sd[35160]: I0516 19:36:08.502473   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:08-04:00 k8s.k8sd[35160]: I0516 19:36:08.510835   35160 helm/client.go:40] "Patch IPAddressPool \"metallb-loadbalancer-ck-loadbalancer\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:08-04:00 k8s.k8sd[35160]: I0516 19:36:08.530770   35160 helm/client.go:40] "Patch L2Advertisement \"metallb-loadbalancer-ck-loadbalancer\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:08-04:00 k8s.k8sd[35160]: I0516 19:36:08.557859   35160 helm/client.go:40] "updating status for upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:36:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:15.547606045-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:metallb-speaker-lxl5s,Uid:abc93faf-9a7f-42ad-82f8-9f288a0660b8,Namespace:metallb-system,Attempt:0,}"
2025-05-16T19:36:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:15.604960548-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:36:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:15.605227593-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:36:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:15.605257333-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:36:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:15.605550910-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/d1776f21c952f005d49ca37728693f12f1f8def6bf5112eb5cbc944539e04c57 pid=46171 runtime=io.containerd.runc.v2
2025-05-16T19:36:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:15.774787260-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:metallb-speaker-lxl5s,Uid:abc93faf-9a7f-42ad-82f8-9f288a0660b8,Namespace:metallb-system,Attempt:0,} returns sandbox id \"d1776f21c952f005d49ca37728693f12f1f8def6bf5112eb5cbc944539e04c57\""
2025-05-16T19:36:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:15.779177877-04:00" level=info msg="PullImage \"ghcr.io/canonical/metallb-speaker:v0.14.8-ck0\""
2025-05-16T19:36:17-04:00 k8s.kube-controller-manager[36331]: I0516 19:36:17.795630   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="42.524001ms"
2025-05-16T19:36:17-04:00 k8s.kube-controller-manager[36331]: I0516 19:36:17.804628   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="1.002707ms"
2025-05-16T19:36:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:23.783606336-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/metallb-speaker:v0.14.8-ck0,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:36:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:23.786811091-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:cc696625ae9dd2cf6c76ac057e96559d902ab10598253b2e96f6f2c73711d675,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:36:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:23.790734170-04:00" level=info msg="ImageCreate event &ImageCreate{Name:ghcr.io/canonical/metallb-speaker@sha256:708862968e35e7599b2e3a63eea59f4a220f03ae3583d7280bcb20c6df5aa1ad,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:36:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:23.791745277-04:00" level=info msg="PullImage \"ghcr.io/canonical/metallb-speaker:v0.14.8-ck0\" returns image reference \"sha256:cc696625ae9dd2cf6c76ac057e96559d902ab10598253b2e96f6f2c73711d675\""
2025-05-16T19:36:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:23.795567431-04:00" level=info msg="CreateContainer within sandbox \"d1776f21c952f005d49ca37728693f12f1f8def6bf5112eb5cbc944539e04c57\" for container &ContainerMetadata{Name:speaker,Attempt:0,}"
2025-05-16T19:36:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:23.827235334-04:00" level=info msg="CreateContainer within sandbox \"d1776f21c952f005d49ca37728693f12f1f8def6bf5112eb5cbc944539e04c57\" for &ContainerMetadata{Name:speaker,Attempt:0,} returns container id \"9d30fa44f4dce56b0a348ba10fd481a591a3f1e018da6b0c54266d8eeb320a8d\""
2025-05-16T19:36:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:23.828326817-04:00" level=info msg="StartContainer for \"9d30fa44f4dce56b0a348ba10fd481a591a3f1e018da6b0c54266d8eeb320a8d\""
2025-05-16T19:36:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:36:23.957497791-04:00" level=info msg="StartContainer for \"9d30fa44f4dce56b0a348ba10fd481a591a3f1e018da6b0c54266d8eeb320a8d\" returns successfully"
2025-05-16T19:36:24-04:00 k8s.kubelet[38157]: I0516 19:36:24.910319   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="metallb-system/metallb-speaker-lxl5s" podStartSLOduration=65.893467605 podStartE2EDuration="1m13.910281052s" podCreationTimestamp="2025-05-16 19:35:11 -0400 EDT" firstStartedPulling="2025-05-16 19:36:15.776911683 -0400 EDT m=+168.914508354" lastFinishedPulling="2025-05-16 19:36:23.793725117 -0400 EDT m=+176.931321801" observedRunningTime="2025-05-16 19:36:24.90770489 -0400 EDT m=+178.045301629" watchObservedRunningTime="2025-05-16 19:36:24.910281052 -0400 EDT m=+178.047877744"
2025-05-16T19:36:58-04:00 k8s.kube-apiserver[37236]: I0516 19:36:58.350750   37236 alloc.go:330] "allocated clusterIPs" service="controller-sunbeam-controller/controller-service" clusterIPs={"IPv4":"10.152.183.155"}
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.452810   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"controller-shared-secret\" (UniqueName: \"kubernetes.io/secret/e7a8aac7-83f9-4010-8422-751309e4233b-controller-shared-secret\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") " pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.452882   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/e7a8aac7-83f9-4010-8422-751309e4233b-charm-data\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") " pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.452925   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"controller-server-pem\" (UniqueName: \"kubernetes.io/secret/e7a8aac7-83f9-4010-8422-751309e4233b-controller-server-pem\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") " pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.452964   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"controller-bootstrap-params\" (UniqueName: \"kubernetes.io/configmap/e7a8aac7-83f9-4010-8422-751309e4233b-controller-bootstrap-params\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") " pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.453000   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8n26d\" (UniqueName: \"kubernetes.io/projected/e7a8aac7-83f9-4010-8422-751309e4233b-kube-api-access-8n26d\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") " pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.453103   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"apiserver-scratch\" (UniqueName: \"kubernetes.io/empty-dir/e7a8aac7-83f9-4010-8422-751309e4233b-apiserver-scratch\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") " pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.453174   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"mongo-scratch\" (UniqueName: \"kubernetes.io/empty-dir/e7a8aac7-83f9-4010-8422-751309e4233b-mongo-scratch\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") " pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.453214   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"controller-agent-conf\" (UniqueName: \"kubernetes.io/configmap/e7a8aac7-83f9-4010-8422-751309e4233b-controller-agent-conf\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") " pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.453262   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-16d97cc3-52cb-4746-94b7-f1d86dd7a1af\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-16d97cc3-52cb-4746-94b7-f1d86dd7a1af\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") " pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.kubelet[38157]: I0516 19:37:04.688780   38157 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"pvc-16d97cc3-52cb-4746-94b7-f1d86dd7a1af\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-16d97cc3-52cb-4746-94b7-f1d86dd7a1af\") pod \"controller-0\" (UID: \"e7a8aac7-83f9-4010-8422-751309e4233b\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/rawfile.csi.openebs.io/4dab54c1a2a2583085b68fed0b672348f4fe20de14ae5f4e1675809c89c5070c/globalmount\"" pod="controller-sunbeam-controller/controller-0"
2025-05-16T19:37:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:04.985528396-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:controller-0,Uid:e7a8aac7-83f9-4010-8422-751309e4233b,Namespace:controller-sunbeam-controller,Attempt:0,}"
2025-05-16T19:37:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:05.258447103-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:37:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:05.258650863-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:37:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:05.258690092-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:37:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:05.259867385-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68 pid=47308 runtime=io.containerd.runc.v2
2025-05-16T19:37:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:05.382910380-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:controller-0,Uid:e7a8aac7-83f9-4010-8422-751309e4233b,Namespace:controller-sunbeam-controller,Attempt:0,} returns sandbox id \"20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68\""
2025-05-16T19:37:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:05.391143784-04:00" level=info msg="PullImage \"docker.io/jujusolutions/jujud-operator:3.6.5\""
2025-05-16T19:37:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:24.289806134-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/jujud-operator:3.6.5,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:37:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:24.293312664-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:38193c3535144ec4a242e05dd66cbc9e62f460f6f710cebeb4175cbcf874ab57,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:37:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:24.299060557-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/jujud-operator@sha256:a00b1371c2380e3b1631455d6d7ec3a43d41c077d8443493759683e6f483820e,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:37:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:24.301847363-04:00" level=info msg="PullImage \"docker.io/jujusolutions/jujud-operator:3.6.5\" returns image reference \"sha256:38193c3535144ec4a242e05dd66cbc9e62f460f6f710cebeb4175cbcf874ab57\""
2025-05-16T19:37:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:24.387356846-04:00" level=info msg="CreateContainer within sandbox \"20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:37:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:24.421644701-04:00" level=info msg="CreateContainer within sandbox \"20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"13935a5fe8c6e0929fb949247d6a0d4f672edcb11bd3825eba1d0253aa454514\""
2025-05-16T19:37:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:24.422818655-04:00" level=info msg="StartContainer for \"13935a5fe8c6e0929fb949247d6a0d4f672edcb11bd3825eba1d0253aa454514\""
2025-05-16T19:37:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:24.536867320-04:00" level=info msg="StartContainer for \"13935a5fe8c6e0929fb949247d6a0d4f672edcb11bd3825eba1d0253aa454514\" returns successfully"
2025-05-16T19:37:25-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:25.964794572-04:00" level=info msg="shim disconnected" id=13935a5fe8c6e0929fb949247d6a0d4f672edcb11bd3825eba1d0253aa454514
2025-05-16T19:37:25-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:25.964924988-04:00" level=warning msg="cleaning up after shim disconnected" id=13935a5fe8c6e0929fb949247d6a0d4f672edcb11bd3825eba1d0253aa454514 namespace=k8s.io
2025-05-16T19:37:25-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:25.964957603-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:37:25-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:25.981508013-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:37:25-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=47555 runtime=io.containerd.runc.v2\n"
2025-05-16T19:37:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:26.083615609-04:00" level=info msg="PullImage \"docker.io/jujusolutions/charm-base:ubuntu-24.04\""
2025-05-16T19:37:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:34.440012236-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/charm-base:ubuntu-24.04,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:37:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:34.444127749-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:5235e9e86b9fcaf3dbcad9badb0a984a64d85a431ac5b40a32837c494a35091a,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:37:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:34.449712057-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/charm-base@sha256:9682e1784acf563164562b2b4327b70aee54f61e563cf5cee68af2cd1a9f5706,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:37:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:34.451264879-04:00" level=info msg="PullImage \"docker.io/jujusolutions/charm-base:ubuntu-24.04\" returns image reference \"sha256:5235e9e86b9fcaf3dbcad9badb0a984a64d85a431ac5b40a32837c494a35091a\""
2025-05-16T19:37:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:34.591199414-04:00" level=info msg="CreateContainer within sandbox \"20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:37:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:34.615287884-04:00" level=info msg="CreateContainer within sandbox \"20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"d670efc38c0a393a48eb95338e579f23ea2b9d7e5fadaa0ce57871d7cc0bde0b\""
2025-05-16T19:37:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:34.617969388-04:00" level=info msg="StartContainer for \"d670efc38c0a393a48eb95338e579f23ea2b9d7e5fadaa0ce57871d7cc0bde0b\""
2025-05-16T19:37:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:34.762403456-04:00" level=info msg="StartContainer for \"d670efc38c0a393a48eb95338e579f23ea2b9d7e5fadaa0ce57871d7cc0bde0b\" returns successfully"
2025-05-16T19:37:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:34.764729564-04:00" level=info msg="PullImage \"docker.io/jujusolutions/juju-db:4.4\""
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.531482214-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/juju-db:4.4,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.536438335-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:bc8be43215e8f521cb0d387071d846f9557e801ea876ae4958dc9e16131fe641,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.540669173-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/juju-db@sha256:e74e1d8162e3733b7fd44f27f1d07a40f5f77293d8801e68e1e975f9df95ac45,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.541648423-04:00" level=info msg="PullImage \"docker.io/jujusolutions/juju-db:4.4\" returns image reference \"sha256:bc8be43215e8f521cb0d387071d846f9557e801ea876ae4958dc9e16131fe641\""
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.645751485-04:00" level=info msg="CreateContainer within sandbox \"20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68\" for container &ContainerMetadata{Name:mongodb,Attempt:0,}"
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.677378415-04:00" level=info msg="CreateContainer within sandbox \"20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68\" for &ContainerMetadata{Name:mongodb,Attempt:0,} returns container id \"66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d\""
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.681791767-04:00" level=info msg="StartContainer for \"66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d\""
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.793705382-04:00" level=info msg="StartContainer for \"66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d\" returns successfully"
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.968681131-04:00" level=info msg="CreateContainer within sandbox \"20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68\" for container &ContainerMetadata{Name:api-server,Attempt:0,}"
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.996176242-04:00" level=info msg="CreateContainer within sandbox \"20e51a04e4ac416bc753ee2d6deef3c818d1b32a99b94b5e164dbc52e1e4bd68\" for &ContainerMetadata{Name:api-server,Attempt:0,} returns container id \"17d3715238f8187399c69d26f8153e7620d56ccd759b1cf697ef9a87c746a5ef\""
2025-05-16T19:37:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:50.997668348-04:00" level=info msg="StartContainer for \"17d3715238f8187399c69d26f8153e7620d56ccd759b1cf697ef9a87c746a5ef\""
2025-05-16T19:37:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:37:51.126226210-04:00" level=info msg="StartContainer for \"17d3715238f8187399c69d26f8153e7620d56ccd759b1cf697ef9a87c746a5ef\" returns successfully"
2025-05-16T19:37:51-04:00 k8s.kubelet[38157]: I0516 19:37:51.310493   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="controller-sunbeam-controller/controller-0" podStartSLOduration=3.156889334 podStartE2EDuration="48.310467707s" podCreationTimestamp="2025-05-16 19:37:03 -0400 EDT" firstStartedPulling="2025-05-16 19:37:05.389547426 -0400 EDT m=+218.527144113" lastFinishedPulling="2025-05-16 19:37:50.543125801 -0400 EDT m=+263.680722486" observedRunningTime="2025-05-16 19:37:51.28956169 -0400 EDT m=+264.427158409" watchObservedRunningTime="2025-05-16 19:37:51.310467707 -0400 EDT m=+264.448064381"
2025-05-16T19:38:11-04:00 k8s.kube-apiserver[37236]: I0516 19:38:11.089575   37236 controller.go:615] quota admission added evaluator for: servicel2statuses.metallb.io
2025-05-16T19:38:13-04:00 k8s.kube-apiserver[37236]: I0516 19:38:13.138493   37236 alloc.go:330] "allocated clusterIPs" service="controller-sunbeam-controller/modeloperator" clusterIPs={"IPv4":"10.152.183.228"}
2025-05-16T19:38:13-04:00 k8s.kube-controller-manager[36331]: I0516 19:38:13.220113   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="controller-sunbeam-controller/modeloperator-856d6cf4bf" duration="39.754788ms"
2025-05-16T19:38:13-04:00 k8s.kubelet[38157]: I0516 19:38:13.268388   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wlp6z\" (UniqueName: \"kubernetes.io/projected/75d4fb3a-9a4d-4bf3-a9fb-f452aade5822-kube-api-access-wlp6z\") pod \"modeloperator-856d6cf4bf-57fzb\" (UID: \"75d4fb3a-9a4d-4bf3-a9fb-f452aade5822\") " pod="controller-sunbeam-controller/modeloperator-856d6cf4bf-57fzb"
2025-05-16T19:38:13-04:00 k8s.kubelet[38157]: I0516 19:38:13.268516   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"modeloperator\" (UniqueName: \"kubernetes.io/configmap/75d4fb3a-9a4d-4bf3-a9fb-f452aade5822-modeloperator\") pod \"modeloperator-856d6cf4bf-57fzb\" (UID: \"75d4fb3a-9a4d-4bf3-a9fb-f452aade5822\") " pod="controller-sunbeam-controller/modeloperator-856d6cf4bf-57fzb"
2025-05-16T19:38:13-04:00 k8s.kube-controller-manager[36331]: I0516 19:38:13.269141   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="controller-sunbeam-controller/modeloperator-856d6cf4bf" duration="48.952708ms"
2025-05-16T19:38:13-04:00 k8s.kube-controller-manager[36331]: I0516 19:38:13.287182   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="controller-sunbeam-controller/modeloperator-856d6cf4bf" duration="17.890727ms"
2025-05-16T19:38:13-04:00 k8s.kube-controller-manager[36331]: I0516 19:38:13.287312   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="controller-sunbeam-controller/modeloperator-856d6cf4bf" duration="76.542µs"
2025-05-16T19:38:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:13.530480481-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:modeloperator-856d6cf4bf-57fzb,Uid:75d4fb3a-9a4d-4bf3-a9fb-f452aade5822,Namespace:controller-sunbeam-controller,Attempt:0,}"
2025-05-16T19:38:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:13.843253170-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:38:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:13.844223749-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:38:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:13.844272568-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:38:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:13.844759195-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/cea70f5f5d6abbfbe394b4e30c7ec7421773743ac9604e7eaa6abf7b738bc329 pid=48583 runtime=io.containerd.runc.v2
2025-05-16T19:38:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:14.058591286-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:modeloperator-856d6cf4bf-57fzb,Uid:75d4fb3a-9a4d-4bf3-a9fb-f452aade5822,Namespace:controller-sunbeam-controller,Attempt:0,} returns sandbox id \"cea70f5f5d6abbfbe394b4e30c7ec7421773743ac9604e7eaa6abf7b738bc329\""
2025-05-16T19:38:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:14.081025134-04:00" level=info msg="CreateContainer within sandbox \"cea70f5f5d6abbfbe394b4e30c7ec7421773743ac9604e7eaa6abf7b738bc329\" for container &ContainerMetadata{Name:juju-operator,Attempt:0,}"
2025-05-16T19:38:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:14.120606843-04:00" level=info msg="CreateContainer within sandbox \"cea70f5f5d6abbfbe394b4e30c7ec7421773743ac9604e7eaa6abf7b738bc329\" for &ContainerMetadata{Name:juju-operator,Attempt:0,} returns container id \"0c09502c8ad0ab261b371f7de724d7f9f7da98fa252c27ba7daafd6e072c8bd2\""
2025-05-16T19:38:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:14.124004088-04:00" level=info msg="StartContainer for \"0c09502c8ad0ab261b371f7de724d7f9f7da98fa252c27ba7daafd6e072c8bd2\""
2025-05-16T19:38:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:38:14.278977004-04:00" level=info msg="StartContainer for \"0c09502c8ad0ab261b371f7de724d7f9f7da98fa252c27ba7daafd6e072c8bd2\" returns successfully"
2025-05-16T19:38:15-04:00 k8s.kubelet[38157]: I0516 19:38:15.294967   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="controller-sunbeam-controller/modeloperator-856d6cf4bf-57fzb" podStartSLOduration=2.294937879 podStartE2EDuration="2.294937879s" podCreationTimestamp="2025-05-16 19:38:13 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:38:15.290319296 -0400 EDT m=+288.427915986" watchObservedRunningTime="2025-05-16 19:38:15.294937879 -0400 EDT m=+288.432534553"
2025-05-16T19:38:15-04:00 k8s.kube-controller-manager[36331]: I0516 19:38:15.323431   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="controller-sunbeam-controller/modeloperator-856d6cf4bf" duration="28.544729ms"
2025-05-16T19:38:15-04:00 k8s.kube-controller-manager[36331]: I0516 19:38:15.326842   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="controller-sunbeam-controller/modeloperator-856d6cf4bf" duration="79.239µs"
2025-05-16T19:39:07-04:00 k8s.k8sd[35160]: I0516 19:39:07.439628   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:07-04:00 k8s.k8sd[35160]: I0516 19:39:07.647369   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:07-04:00 k8s.k8sd[35160]: I0516 19:39:07.667676   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:07-04:00 k8s.k8sd[35160]: I0516 19:39:07.729379   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:08-04:00 k8s.k8sd[35160]: I0516 19:39:08.616898   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:09-04:00 k8s.k8sd[35160]: I0516 19:39:09.099711   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:09-04:00 k8s.k8sd[35160]: I0516 19:39:09.187450   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:09-04:00 k8s.k8sd[35160]: I0516 19:39:09.301722   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:11-04:00 k8s.k8sd[35160]: I0516 19:39:11.108738   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:11-04:00 k8s.k8sd[35160]: I0516 19:39:11.160890   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:11-04:00 k8s.k8sd[35160]: I0516 19:39:11.713792   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:12-04:00 k8s.k8sd[35160]: I0516 19:39:12.078594   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:12-04:00 k8s.k8sd[35160]: I0516 19:39:12.335832   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:13-04:00 k8s.k8sd[35160]: I0516 19:39:13.544012   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:13-04:00 k8s.k8sd[35160]: I0516 19:39:13.831556   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:13-04:00 k8s.k8sd[35160]: I0516 19:39:13.915061   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.005118   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.106645   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.165878   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.224376   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.308006   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.438744   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.706646   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.731633   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.815191   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.854284   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:14-04:00 k8s.k8sd[35160]: I0516 19:39:14.942099   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.066885   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.161632   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.266905   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.419798   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.449117   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.585171   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.628574   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.666525   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.713600   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.783683   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:15-04:00 k8s.k8sd[35160]: I0516 19:39:15.812199   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:16-04:00 k8s.k8sd[35160]: I0516 19:39:16.871626   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:16-04:00 k8s.k8sd[35160]: I0516 19:39:16.956425   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.044533   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.104516   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.214470   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.370745   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.528638   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.636255   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.743735   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.779020   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.781931   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:17-04:00 k8s.k8sd[35160]: I0516 19:39:17.965549   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.083995   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.238152   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.290638   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.344808   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.359525   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.388468   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.441551   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.520983   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.565361   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.605641   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:18-04:00 k8s.k8sd[35160]: I0516 19:39:18.939106   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:19-04:00 k8s.k8sd[35160]: I0516 19:39:19.271056   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:19-04:00 k8s.k8sd[35160]: I0516 19:39:19.839975   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.021790   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.090136   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.127316   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.127864   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.165996   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.190285   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.230734   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.280807   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.343572   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.442156   35160 helm/client.go:40] "Patch CustomResourceDefinition \"bgppeers.metallb.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.539289   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.581200   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.643116   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.685948   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.719008   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.750148   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.819650   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.844928   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.882539   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.914118   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.952160   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:20-04:00 k8s.k8sd[35160]: I0516 19:39:20.974610   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.010657   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.062282   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.089930   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.134510   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.210728   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.497062   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.kube-apiserver[37236]: W0516 19:39:21.528376   37236 cacher.go:171] Terminating all watchers from cacher bgppeers.metallb.io
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.537134   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.586014   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.621391   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.662415   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.690596   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.709705   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.730642   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.768512   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.775856   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.805967   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.808844   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.835295   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.878919   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.902499   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.939991   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.980438   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.986147   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:21-04:00 k8s.k8sd[35160]: I0516 19:39:21.993539   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:22-04:00 k8s.k8sd[35160]: I0516 19:39:22.032847   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:22-04:00 k8s.k8sd[35160]: I0516 19:39:22.075473   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:22-04:00 k8s.k8sd[35160]: I0516 19:39:22.132775   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:39:22-04:00 k8s.k8sd[35160]: I0516 19:39:22.208783   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:22-04:00 k8s.k8sd[35160]: I0516 19:39:22.231818   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:22-04:00 k8s.k8sd[35160]: I0516 19:39:22.457736   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:22-04:00 k8s.k8sd[35160]: I0516 19:39:22.650039   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:23-04:00 k8s.k8sd[35160]: I0516 19:39:23.211353   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:23-04:00 k8s.k8sd[35160]: I0516 19:39:23.632997   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:23-04:00 k8s.k8sd[35160]: I0516 19:39:23.658900   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:23-04:00 k8s.k8sd[35160]: I0516 19:39:23.693759   35160 helm/client.go:40] "Patch IPAddressPool \"metallb-loadbalancer-ck-loadbalancer\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:23-04:00 k8s.k8sd[35160]: I0516 19:39:23.816290   35160 helm/client.go:40] "Patch L2Advertisement \"metallb-loadbalancer-ck-loadbalancer\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:23-04:00 k8s.k8sd[35160]: I0516 19:39:23.937778   35160 helm/client.go:40] "updating status for upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:24-04:00 k8s.kube-controller-manager[36331]: W0516 19:39:24.032372   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:39:24-04:00 k8s.kube-controller-manager[36331]: E0516 19:39:24.043875   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta2, Resource=bgppeers"
2025-05-16T19:39:25-04:00 k8s.k8sd[35160]: I0516 19:39:25.507908   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:25-04:00 k8s.k8sd[35160]: I0516 19:39:25.608192   35160 helm/client.go:40] "preparing upgrade for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:25-04:00 k8s.k8sd[35160]: I0516 19:39:25.936748   35160 helm/client.go:40] "preparing upgrade for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:26-04:00 k8s.k8sd[35160]: I0516 19:39:26.107243   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:26-04:00 k8s.k8sd[35160]: I0516 19:39:26.620942   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:26-04:00 k8s.k8sd[35160]: I0516 19:39:26.761526   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:26-04:00 k8s.k8sd[35160]: I0516 19:39:26.963299   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.126294   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.135078   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.172730   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.207021   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.268692   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.310710   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.360122   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.448125   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.529263   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.544619   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.565085   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.609791   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.632049   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.677811   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.708430   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.722014   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: E0516 19:39:27.734554   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to delete Gateway API cilium configuration: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="gateway"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.762062   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.765140   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.848350   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.852508   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.987613   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:27-04:00 k8s.k8sd[35160]: I0516 19:39:27.998321   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.028277   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.069280   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.105888   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.188541   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.225766   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.255759   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.338772   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.364872   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.409025   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.450754   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.521327   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.731909   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: I0516 19:39:28.863260   35160 helm/client.go:40] "performing update for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:28-04:00 k8s.k8sd[35160]: E0516 19:39:28.970106   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable network: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="network"
2025-05-16T19:39:29-04:00 k8s.k8sd[35160]: I0516 19:39:29.038757   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:29-04:00 k8s.k8sd[35160]: I0516 19:39:29.047612   35160 helm/client.go:40] "performing update for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:29-04:00 k8s.k8sd[35160]: I0516 19:39:29.552983   35160 helm/client.go:40] "preparing upgrade for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:30-04:00 k8s.k8sd[35160]: I0516 19:39:30.387624   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:30-04:00 k8s.k8sd[35160]: I0516 19:39:30.478927   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.229242   35160 helm/client.go:40] "creating upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.282297   35160 helm/client.go:40] "creating upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.309533   35160 helm/client.go:40] "checking 9 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.373394   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.412930   35160 helm/client.go:40] "checking 5 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.433062   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server-aggregated-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.450105   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.474617   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.507366   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.553760   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metrics-server:system:auth-delegator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.559952   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-dns-coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.618106   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"system:metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.618903   35160 helm/client.go:40] "Looks like there are no changes for Service \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.659956   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metrics-server-auth-reader\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.800067   35160 helm/client.go:40] "Looks like there are no changes for Deployment \"coredns\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.864396   35160 helm/client.go:40] "Looks like there are no changes for Service \"metrics-server\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.898678   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:31-04:00 k8s.k8sd[35160]: I0516 19:39:31.917487   35160 helm/client.go:40] "updating status for upgraded release for ck-dns" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:32-04:00 k8s.k8sd[35160]: I0516 19:39:32.077925   35160 helm/client.go:40] "Patch Deployment \"metrics-server\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:32-04:00 k8s.k8sd[35160]: I0516 19:39:32.236232   35160 helm/client.go:40] "Patch APIService \"v1beta1.metrics.k8s.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:39:32-04:00 k8s.k8sd[35160]: I0516 19:39:32.382843   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:32-04:00 k8s.k8sd[35160]: I0516 19:39:32.438475   35160 helm/client.go:40] "updating status for upgraded release for metrics-server" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:32-04:00 k8s.k8sd[35160]: I0516 19:39:32.947681   35160 helm/client.go:40] "performing update for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:33-04:00 k8s.k8sd[35160]: I0516 19:39:33.113683   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.224934   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.285145   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.405130   35160 helm/client.go:40] "creating upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.514581   35160 helm/client.go:40] "checking 15 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.577589   35160 helm/client.go:40] "Patch ServiceAccount \"ck-storage-rawfile-csi-driver\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.650638   35160 helm/client.go:40] "Looks like there are no changes for StorageClass \"csi-rawfile-default\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.728803   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.825818   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.886855   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:35-04:00 k8s.k8sd[35160]: I0516 19:39:35.969291   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.030356   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.053026   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-provisioner\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.068715   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.087737   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-broker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.144663   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.150142   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-resizer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.165894   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.184952   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"ck-storage-rawfile-csi-snapshotter\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.194198   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.259208   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.264248   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.327334   35160 helm/client.go:40] "Looks like there are no changes for Service \"ck-storage-rawfile-csi-node\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.344367   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.382181   35160 helm/client.go:40] "Patch DaemonSet \"ck-storage-rawfile-csi-node\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.401718   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.457920   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.462235   35160 helm/client.go:40] "Patch StatefulSet \"ck-storage-rawfile-csi-controller\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.518245   35160 helm/client.go:40] "Looks like there are no changes for CSIDriver \"rawfile.csi.openebs.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.528516   35160 helm/client.go:40] "Patch CustomResourceDefinition \"bgppeers.metallb.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.570116   35160 helm/client.go:40] "updating status for upgraded release for ck-storage" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.625743   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.688871   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.788098   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.848508   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.918323   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:36-04:00 k8s.k8sd[35160]: I0516 19:39:36.979751   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.015573   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.037399   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.067909   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.092134   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.108148   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.126602   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.146509   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.181840   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.220111   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.249238   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.327309   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:37-04:00 k8s.kube-apiserver[37236]: W0516 19:39:37.590491   37236 cacher.go:171] Terminating all watchers from cacher bgppeers.metallb.io
2025-05-16T19:39:37-04:00 k8s.k8sd[35160]: I0516 19:39:37.811280   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:38-04:00 k8s.k8sd[35160]: I0516 19:39:38.264047   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:38-04:00 k8s.k8sd[35160]: I0516 19:39:38.586330   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:38-04:00 k8s.k8sd[35160]: I0516 19:39:38.705753   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:38-04:00 k8s.k8sd[35160]: I0516 19:39:38.927987   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:39-04:00 k8s.k8sd[35160]: I0516 19:39:39.038525   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:39-04:00 k8s.k8sd[35160]: I0516 19:39:39.159747   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:39-04:00 k8s.k8sd[35160]: I0516 19:39:39.183998   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:39-04:00 k8s.k8sd[35160]: I0516 19:39:39.223515   35160 helm/client.go:40] "Patch IPAddressPool \"metallb-loadbalancer-ck-loadbalancer\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:39-04:00 k8s.k8sd[35160]: I0516 19:39:39.285777   35160 helm/client.go:40] "Patch L2Advertisement \"metallb-loadbalancer-ck-loadbalancer\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:39-04:00 k8s.k8sd[35160]: I0516 19:39:39.377860   35160 helm/client.go:40] "updating status for upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:40-04:00 k8s.k8sd[35160]: I0516 19:39:40.246020   35160 helm/client.go:40] "preparing upgrade for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:40-04:00 k8s.k8sd[35160]: I0516 19:39:40.831742   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:40-04:00 k8s.k8sd[35160]: I0516 19:39:40.865000   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.049816   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.312298   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.330787   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.362133   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.kube-controller-manager[36331]: W0516 19:39:41.383506   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.384970   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.kube-controller-manager[36331]: E0516 19:39:41.388089   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta2, Resource=bgppeers"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.413741   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.457676   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.480191   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.510547   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.534989   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.557717   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.581899   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.611330   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.640171   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.685768   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.739243   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:41-04:00 k8s.k8sd[35160]: I0516 19:39:41.850977   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:42-04:00 k8s.k8sd[35160]: I0516 19:39:42.160344   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:42-04:00 k8s.k8sd[35160]: E0516 19:39:42.284089   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable network: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="network"
2025-05-16T19:39:43-04:00 k8s.k8sd[35160]: I0516 19:39:43.636164   35160 helm/client.go:40] "performing update for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:43-04:00 k8s.k8sd[35160]: I0516 19:39:43.673954   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:44-04:00 k8s.k8sd[35160]: E0516 19:39:44.268448   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:39:44-04:00 k8s.k8sd[35160]: I0516 19:39:44.875341   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:45-04:00 k8s.k8sd[35160]: I0516 19:39:45.888932   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.353839   35160 helm/client.go:40] "creating upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.468092   35160 helm/client.go:40] "checking 23 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.510072   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.551844   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.583777   35160 helm/client.go:40] "Looks like there are no changes for Secret \"metallb-webhook-cert\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.621306   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"metallb-excludel2\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.671039   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bfdprofiles.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.735600   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"bgpadvertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.795902   35160 helm/client.go:40] "Patch CustomResourceDefinition \"bgppeers.metallb.io\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.910024   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"communities.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:46-04:00 k8s.k8sd[35160]: I0516 19:39:46.994508   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"ipaddresspools.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.047890   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"l2advertisements.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.110641   35160 helm/client.go:40] "Looks like there are no changes for CustomResourceDefinition \"servicel2statuses.metallb.io\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.174259   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.221620   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.304873   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.332237   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"metallb:speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.361127   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.379977   35160 helm/client.go:40] "Looks like there are no changes for Role \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.402968   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-pod-lister\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.426634   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"metallb-controller\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.455641   35160 helm/client.go:40] "Looks like there are no changes for Service \"metallb-webhook-service\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.497185   35160 helm/client.go:40] "Looks like there are no changes for DaemonSet \"metallb-speaker\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.522850   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.534110   35160 helm/client.go:40] "Patch Deployment \"metallb-controller\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.604514   35160 helm/client.go:40] "Patch ValidatingWebhookConfiguration \"metallb-webhook-configuration\" in namespace " logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.k8sd[35160]: I0516 19:39:47.722407   35160 helm/client.go:40] "updating status for upgraded release for metallb" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:47-04:00 k8s.kube-apiserver[37236]: W0516 19:39:47.876694   37236 cacher.go:171] Terminating all watchers from cacher bgppeers.metallb.io
2025-05-16T19:39:48-04:00 k8s.k8sd[35160]: I0516 19:39:48.340716   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:48-04:00 k8s.k8sd[35160]: I0516 19:39:48.965521   35160 helm/client.go:40] "preparing upgrade for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:49-04:00 k8s.k8sd[35160]: I0516 19:39:49.109640   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:49-04:00 k8s.k8sd[35160]: I0516 19:39:49.336987   35160 helm/client.go:40] "performing update for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:49-04:00 k8s.k8sd[35160]: I0516 19:39:49.563927   35160 helm/client.go:40] "creating upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:49-04:00 k8s.k8sd[35160]: I0516 19:39:49.604020   35160 helm/client.go:40] "checking 2 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:49-04:00 k8s.k8sd[35160]: I0516 19:39:49.635476   35160 helm/client.go:40] "Patch IPAddressPool \"metallb-loadbalancer-ck-loadbalancer\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:49-04:00 k8s.k8sd[35160]: I0516 19:39:49.712243   35160 helm/client.go:40] "Patch L2Advertisement \"metallb-loadbalancer-ck-loadbalancer\" in namespace metallb-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:49-04:00 k8s.k8sd[35160]: I0516 19:39:49.780922   35160 helm/client.go:40] "updating status for upgraded release for metallb-loadbalancer" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:50-04:00 k8s.k8sd[35160]: I0516 19:39:50.326914   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:51-04:00 k8s.k8sd[35160]: I0516 19:39:51.027129   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:51-04:00 k8s.k8sd[35160]: I0516 19:39:51.563743   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:52-04:00 k8s.k8sd[35160]: I0516 19:39:52.464659   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.063089   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.073743   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.086888   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.103649   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.123016   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.154127   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.177132   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.192649   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.197840   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.216477   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.236257   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.251581   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.270054   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.285381   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.341371   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.408437   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.kube-controller-manager[36331]: W0516 19:39:53.409625   36331 reflector.go:362] The watchlist request ended with an error, falling back to the standard LIST/WATCH semantics because making progress is better than deadlocking, err = ListOptions.meta.k8s.io "" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled
2025-05-16T19:39:53-04:00 k8s.kube-controller-manager[36331]: E0516 19:39:53.416786   36331 metadata.go:231] "The watchlist request ended with an error, falling back to the standard LIST semantics" err="ListOptions.meta.k8s.io \"\" is invalid: sendInitialEvents: Forbidden: sendInitialEvents is forbidden for watch unless the WatchList feature gate is enabled" resource="metallb.io/v1beta2, Resource=bgppeers"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: E0516 19:39:53.504856   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to delete Gateway API cilium configuration: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="gateway"
2025-05-16T19:39:53-04:00 k8s.k8sd[35160]: I0516 19:39:53.816197   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:53-04:00 k8s.kube-apiserver[37236]: I0516 19:39:53.980127   37236 alloc.go:330] "allocated clusterIPs" service="openstack/modeloperator" clusterIPs={"IPv4":"10.152.183.164"}
2025-05-16T19:39:54-04:00 k8s.k8sd[35160]: I0516 19:39:54.041030   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:39:54-04:00 k8s.kube-controller-manager[36331]: I0516 19:39:54.250251   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="openstack/modeloperator-6cbd5bbd86" duration="113.877666ms"
2025-05-16T19:39:54-04:00 k8s.kube-controller-manager[36331]: I0516 19:39:54.348426   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="openstack/modeloperator-6cbd5bbd86" duration="98.093603ms"
2025-05-16T19:39:54-04:00 k8s.kubelet[38157]: I0516 19:39:54.443369   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7xf9x\" (UniqueName: \"kubernetes.io/projected/f3d30e53-805b-4c84-9e81-af3ec8feceb4-kube-api-access-7xf9x\") pod \"modeloperator-6cbd5bbd86-qqxwg\" (UID: \"f3d30e53-805b-4c84-9e81-af3ec8feceb4\") " pod="openstack/modeloperator-6cbd5bbd86-qqxwg"
2025-05-16T19:39:54-04:00 k8s.kubelet[38157]: I0516 19:39:54.443470   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"modeloperator\" (UniqueName: \"kubernetes.io/configmap/f3d30e53-805b-4c84-9e81-af3ec8feceb4-modeloperator\") pod \"modeloperator-6cbd5bbd86-qqxwg\" (UID: \"f3d30e53-805b-4c84-9e81-af3ec8feceb4\") " pod="openstack/modeloperator-6cbd5bbd86-qqxwg"
2025-05-16T19:39:54-04:00 k8s.kube-controller-manager[36331]: I0516 19:39:54.565580   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="openstack/modeloperator-6cbd5bbd86" duration="216.581197ms"
2025-05-16T19:39:54-04:00 k8s.kube-controller-manager[36331]: I0516 19:39:54.566791   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="openstack/modeloperator-6cbd5bbd86" duration="142.103µs"
2025-05-16T19:39:54-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:54.650812728-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:modeloperator-6cbd5bbd86-qqxwg,Uid:f3d30e53-805b-4c84-9e81-af3ec8feceb4,Namespace:openstack,Attempt:0,}"
2025-05-16T19:39:55-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:55.520732741-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:39:55-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:55.520940370-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:39:55-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:55.520977132-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:39:55-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:55.521816341-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/885f44c1f12932f073528be6f2305f569802647f8a148016bfb718c7fff4c954 pid=52455 runtime=io.containerd.runc.v2
2025-05-16T19:39:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:56.217649830-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:modeloperator-6cbd5bbd86-qqxwg,Uid:f3d30e53-805b-4c84-9e81-af3ec8feceb4,Namespace:openstack,Attempt:0,} returns sandbox id \"885f44c1f12932f073528be6f2305f569802647f8a148016bfb718c7fff4c954\""
2025-05-16T19:39:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:56.284672312-04:00" level=info msg="CreateContainer within sandbox \"885f44c1f12932f073528be6f2305f569802647f8a148016bfb718c7fff4c954\" for container &ContainerMetadata{Name:juju-operator,Attempt:0,}"
2025-05-16T19:39:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:56.515985862-04:00" level=info msg="CreateContainer within sandbox \"885f44c1f12932f073528be6f2305f569802647f8a148016bfb718c7fff4c954\" for &ContainerMetadata{Name:juju-operator,Attempt:0,} returns container id \"3d3487bc3b4ea406b527d36ccb19d43dd56a98df49cc10fd8d49b912b66ba688\""
2025-05-16T19:39:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:56.522891306-04:00" level=info msg="StartContainer for \"3d3487bc3b4ea406b527d36ccb19d43dd56a98df49cc10fd8d49b912b66ba688\""
2025-05-16T19:39:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:39:57.104716456-04:00" level=info msg="StartContainer for \"3d3487bc3b4ea406b527d36ccb19d43dd56a98df49cc10fd8d49b912b66ba688\" returns successfully"
2025-05-16T19:39:57-04:00 k8s.kubelet[38157]: I0516 19:39:57.845319   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/modeloperator-6cbd5bbd86-qqxwg" podStartSLOduration=3.8452659110000003 podStartE2EDuration="3.845265911s" podCreationTimestamp="2025-05-16 19:39:54 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:39:57.834706745 -0400 EDT m=+390.972303434" watchObservedRunningTime="2025-05-16 19:39:57.845265911 -0400 EDT m=+390.982862583"
2025-05-16T19:39:57-04:00 k8s.kube-controller-manager[36331]: I0516 19:39:57.909011   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="openstack/modeloperator-6cbd5bbd86" duration="63.258331ms"
2025-05-16T19:39:57-04:00 k8s.kube-controller-manager[36331]: I0516 19:39:57.918856   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="openstack/modeloperator-6cbd5bbd86" duration="441.192µs"
2025-05-16T19:39:59-04:00 k8s.k8sd[35160]: I0516 19:39:59.995302   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:00-04:00 k8s.k8sd[35160]: I0516 19:40:00.640095   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:03-04:00 k8s.k8sd[35160]: I0516 19:40:03.024746   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:03-04:00 k8s.k8sd[35160]: I0516 19:40:03.358505   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:04-04:00 k8s.k8sd[35160]: E0516 19:40:04.284873   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:40:04-04:00 k8s.kube-apiserver[37236]: I0516 19:40:04.373834   37236 alloc.go:330] "allocated clusterIPs" service="openstack/ovn-relay" clusterIPs={"IPv4":"10.152.183.71"}
2025-05-16T19:40:05-04:00 k8s.k8sd[35160]: I0516 19:40:05.203185   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:06-04:00 k8s.k8sd[35160]: I0516 19:40:06.114531   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:07-04:00 k8s.k8sd[35160]: I0516 19:40:07.920781   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:08-04:00 k8s.kube-apiserver[37236]: I0516 19:40:08.186774   37236 alloc.go:330] "allocated clusterIPs" service="openstack/rabbitmq" clusterIPs={"IPv4":"10.152.183.232"}
2025-05-16T19:40:08-04:00 k8s.kubelet[38157]: I0516 19:40:08.478348   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8748f\" (UniqueName: \"kubernetes.io/projected/57c928da-a178-4102-95f1-647867ea16c1-kube-api-access-8748f\") pod \"ovn-relay-0\" (UID: \"57c928da-a178-4102-95f1-647867ea16c1\") " pod="openstack/ovn-relay-0"
2025-05-16T19:40:08-04:00 k8s.kubelet[38157]: I0516 19:40:08.478781   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/57c928da-a178-4102-95f1-647867ea16c1-charm-data\") pod \"ovn-relay-0\" (UID: \"57c928da-a178-4102-95f1-647867ea16c1\") " pod="openstack/ovn-relay-0"
2025-05-16T19:40:08-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:08.786493082-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:ovn-relay-0,Uid:57c928da-a178-4102-95f1-647867ea16c1,Namespace:openstack,Attempt:0,}"
2025-05-16T19:40:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:09.444265132-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:40:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:09.454054473-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:40:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:09.454213929-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:40:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:09.456742332-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/2fd80b566f29cec21cd726eaab789529e78ad5d6add321bfd46e04a827926e02 pid=52881 runtime=io.containerd.runc.v2
2025-05-16T19:40:09-04:00 k8s.k8sd[35160]: I0516 19:40:09.870165   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:09.883836209-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:ovn-relay-0,Uid:57c928da-a178-4102-95f1-647867ea16c1,Namespace:openstack,Attempt:0,} returns sandbox id \"2fd80b566f29cec21cd726eaab789529e78ad5d6add321bfd46e04a827926e02\""
2025-05-16T19:40:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:10.133804807-04:00" level=info msg="CreateContainer within sandbox \"2fd80b566f29cec21cd726eaab789529e78ad5d6add321bfd46e04a827926e02\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:40:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:10.164858587-04:00" level=info msg="CreateContainer within sandbox \"2fd80b566f29cec21cd726eaab789529e78ad5d6add321bfd46e04a827926e02\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"5d53e0c6814b82dab9bdeea28147c12d68d5cc88f39d3aa1e60be41c6abf7c77\""
2025-05-16T19:40:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:10.167808218-04:00" level=info msg="StartContainer for \"5d53e0c6814b82dab9bdeea28147c12d68d5cc88f39d3aa1e60be41c6abf7c77\""
2025-05-16T19:40:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:10.452095201-04:00" level=info msg="StartContainer for \"5d53e0c6814b82dab9bdeea28147c12d68d5cc88f39d3aa1e60be41c6abf7c77\" returns successfully"
2025-05-16T19:40:10-04:00 k8s.kube-apiserver[37236]: I0516 19:40:10.562049   37236 alloc.go:330] "allocated clusterIPs" service="openstack/certificate-authority" clusterIPs={"IPv4":"10.152.183.100"}
2025-05-16T19:40:11-04:00 k8s.k8sd[35160]: I0516 19:40:11.858371   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:11.940125432-04:00" level=info msg="shim disconnected" id=5d53e0c6814b82dab9bdeea28147c12d68d5cc88f39d3aa1e60be41c6abf7c77
2025-05-16T19:40:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:11.940220411-04:00" level=warning msg="cleaning up after shim disconnected" id=5d53e0c6814b82dab9bdeea28147c12d68d5cc88f39d3aa1e60be41c6abf7c77 namespace=k8s.io
2025-05-16T19:40:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:11.940243865-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:40:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:11.975886541-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:40:11-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=53010 runtime=io.containerd.runc.v2\n"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.253608   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.268037   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.315810   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.359377   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.396071   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.441183   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.479075   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.514655   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.542858   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.563540   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.584298   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.608369   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.644148   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.681027   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.707898   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.740404   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:12-04:00 k8s.k8sd[35160]: I0516 19:40:12.997965   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:13.161826187-04:00" level=info msg="CreateContainer within sandbox \"2fd80b566f29cec21cd726eaab789529e78ad5d6add321bfd46e04a827926e02\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:40:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:13.216045549-04:00" level=info msg="CreateContainer within sandbox \"2fd80b566f29cec21cd726eaab789529e78ad5d6add321bfd46e04a827926e02\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"5220530a0f217d8a6de71f4607bb534880671f85c3ec3321b5844c8f8edbe03a\""
2025-05-16T19:40:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:13.218738658-04:00" level=info msg="StartContainer for \"5220530a0f217d8a6de71f4607bb534880671f85c3ec3321b5844c8f8edbe03a\""
2025-05-16T19:40:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:13.565684641-04:00" level=info msg="StartContainer for \"5220530a0f217d8a6de71f4607bb534880671f85c3ec3321b5844c8f8edbe03a\" returns successfully"
2025-05-16T19:40:13-04:00 k8s.k8sd[35160]: E0516 19:40:13.566671   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to delete Gateway API cilium configuration: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="gateway"
2025-05-16T19:40:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:13.574676271-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/l0cz4xi5v0335kaxbsyzeg48uiorm1qfybohq/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\""
2025-05-16T19:40:13-04:00 k8s.k8sd[35160]: I0516 19:40:13.724797   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:14-04:00 k8s.kubelet[38157]: I0516 19:40:14.265351   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-d6591fac-7505-4100-ab1f-f5a77373056b\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-d6591fac-7505-4100-ab1f-f5a77373056b\") pod \"rabbitmq-0\" (UID: \"67a08dc4-fcef-4826-8dfc-08f5b202eaa0\") " pod="openstack/rabbitmq-0"
2025-05-16T19:40:14-04:00 k8s.kubelet[38157]: I0516 19:40:14.265440   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6kgmz\" (UniqueName: \"kubernetes.io/projected/67a08dc4-fcef-4826-8dfc-08f5b202eaa0-kube-api-access-6kgmz\") pod \"rabbitmq-0\" (UID: \"67a08dc4-fcef-4826-8dfc-08f5b202eaa0\") " pod="openstack/rabbitmq-0"
2025-05-16T19:40:14-04:00 k8s.kubelet[38157]: I0516 19:40:14.265503   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/67a08dc4-fcef-4826-8dfc-08f5b202eaa0-charm-data\") pod \"rabbitmq-0\" (UID: \"67a08dc4-fcef-4826-8dfc-08f5b202eaa0\") " pod="openstack/rabbitmq-0"
2025-05-16T19:40:14-04:00 k8s.kubelet[38157]: I0516 19:40:14.564180   38157 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"pvc-d6591fac-7505-4100-ab1f-f5a77373056b\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-d6591fac-7505-4100-ab1f-f5a77373056b\") pod \"rabbitmq-0\" (UID: \"67a08dc4-fcef-4826-8dfc-08f5b202eaa0\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/rawfile.csi.openebs.io/caa943321bc0952b1a2ad10ca59f2b4feaea411622cdd2adeee9542628f585bd/globalmount\"" pod="openstack/rabbitmq-0"
2025-05-16T19:40:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:14.710129030-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:rabbitmq-0,Uid:67a08dc4-fcef-4826-8dfc-08f5b202eaa0,Namespace:openstack,Attempt:0,}"
2025-05-16T19:40:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:15.392722722-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:40:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:15.398979772-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:40:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:15.399048156-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:40:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:15.404445402-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/01baab015394f063cbb4f5614d6b9a999c097aeadcd74146e296d6e3ab929bf8 pid=53193 runtime=io.containerd.runc.v2
2025-05-16T19:40:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:15.876202264-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:rabbitmq-0,Uid:67a08dc4-fcef-4826-8dfc-08f5b202eaa0,Namespace:openstack,Attempt:0,} returns sandbox id \"01baab015394f063cbb4f5614d6b9a999c097aeadcd74146e296d6e3ab929bf8\""
2025-05-16T19:40:16-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:16.138832278-04:00" level=info msg="CreateContainer within sandbox \"01baab015394f063cbb4f5614d6b9a999c097aeadcd74146e296d6e3ab929bf8\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:40:16-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:16.215949049-04:00" level=info msg="CreateContainer within sandbox \"01baab015394f063cbb4f5614d6b9a999c097aeadcd74146e296d6e3ab929bf8\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"49a9083bb47e41dbfac7d3884dfaf0dea57df54d74001d69d2edcea49ff89153\""
2025-05-16T19:40:16-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:16.224918472-04:00" level=info msg="StartContainer for \"49a9083bb47e41dbfac7d3884dfaf0dea57df54d74001d69d2edcea49ff89153\""
2025-05-16T19:40:16-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:16.608111302-04:00" level=info msg="StartContainer for \"49a9083bb47e41dbfac7d3884dfaf0dea57df54d74001d69d2edcea49ff89153\" returns successfully"
2025-05-16T19:40:17-04:00 k8s.k8sd[35160]: E0516 19:40:17.596309   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: failed to upgrade ck-network: another operation (install/upgrade/rollback) is in progress" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:40:17-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:17.758081573-04:00" level=info msg="shim disconnected" id=49a9083bb47e41dbfac7d3884dfaf0dea57df54d74001d69d2edcea49ff89153
2025-05-16T19:40:17-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:17.758180624-04:00" level=warning msg="cleaning up after shim disconnected" id=49a9083bb47e41dbfac7d3884dfaf0dea57df54d74001d69d2edcea49ff89153 namespace=k8s.io
2025-05-16T19:40:17-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:17.758201420-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:40:17-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:17.802806633-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:40:17-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=53332 runtime=io.containerd.runc.v2\ntime=\"2025-05-16T19:40:17-04:00\" level=warning msg=\"failed to remove runc container\" error=\"runc did not terminate successfully: exit status 255: \" runtime=io.containerd.runc.v2\n"
2025-05-16T19:40:18-04:00 k8s.kubelet[38157]: I0516 19:40:18.119961   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-cpbhv\" (UniqueName: \"kubernetes.io/projected/19a87989-0dc5-4501-bfb8-9b62e12c0b2a-kube-api-access-cpbhv\") pod \"certificate-authority-0\" (UID: \"19a87989-0dc5-4501-bfb8-9b62e12c0b2a\") " pod="openstack/certificate-authority-0"
2025-05-16T19:40:18-04:00 k8s.kubelet[38157]: I0516 19:40:18.120042   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/19a87989-0dc5-4501-bfb8-9b62e12c0b2a-charm-data\") pod \"certificate-authority-0\" (UID: \"19a87989-0dc5-4501-bfb8-9b62e12c0b2a\") " pod="openstack/certificate-authority-0"
2025-05-16T19:40:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:18.584762886-04:00" level=info msg="CreateContainer within sandbox \"01baab015394f063cbb4f5614d6b9a999c097aeadcd74146e296d6e3ab929bf8\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:40:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:18.612146155-04:00" level=info msg="CreateContainer within sandbox \"01baab015394f063cbb4f5614d6b9a999c097aeadcd74146e296d6e3ab929bf8\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"43a76614f94bae1306882f8da11bce5f4335b879affa5649afd9cb44e1a72630\""
2025-05-16T19:40:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:18.615276041-04:00" level=info msg="StartContainer for \"43a76614f94bae1306882f8da11bce5f4335b879affa5649afd9cb44e1a72630\""
2025-05-16T19:40:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:18.642040209-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:certificate-authority-0,Uid:19a87989-0dc5-4501-bfb8-9b62e12c0b2a,Namespace:openstack,Attempt:0,}"
2025-05-16T19:40:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:19.283212614-04:00" level=info msg="StartContainer for \"43a76614f94bae1306882f8da11bce5f4335b879affa5649afd9cb44e1a72630\" returns successfully"
2025-05-16T19:40:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:19.294175247-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/kvq0v0ar36fu9jx5c3x2f22ov9o27i7r85p9a/rabbitmq-image@sha256:ab0adb77003a3b523d8100e8be916725189ce877d49e8585b140831165472325\""
2025-05-16T19:40:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:19.852855514-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:40:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:19.853032020-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:40:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:19.853083080-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:40:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:19.865440399-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/1f1219d99feb476db59bcd2fb3ff3a7d901ca1d9e8eaa450868a0662a454f40a pid=53475 runtime=io.containerd.runc.v2
2025-05-16T19:40:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:20.680053754-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:certificate-authority-0,Uid:19a87989-0dc5-4501-bfb8-9b62e12c0b2a,Namespace:openstack,Attempt:0,} returns sandbox id \"1f1219d99feb476db59bcd2fb3ff3a7d901ca1d9e8eaa450868a0662a454f40a\""
2025-05-16T19:40:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:21.058596066-04:00" level=info msg="CreateContainer within sandbox \"1f1219d99feb476db59bcd2fb3ff3a7d901ca1d9e8eaa450868a0662a454f40a\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:40:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:21.167036483-04:00" level=info msg="CreateContainer within sandbox \"1f1219d99feb476db59bcd2fb3ff3a7d901ca1d9e8eaa450868a0662a454f40a\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"9de3d607de1a6e56d45cca2ab4d502dadb4e580662c0b3c7192848eb882886f5\""
2025-05-16T19:40:21-04:00 k8s.kube-apiserver[37236]: I0516 19:40:21.181859   37236 alloc.go:330] "allocated clusterIPs" service="openstack/traefik" clusterIPs={"IPv4":"10.152.183.44"}
2025-05-16T19:40:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:21.202318445-04:00" level=info msg="StartContainer for \"9de3d607de1a6e56d45cca2ab4d502dadb4e580662c0b3c7192848eb882886f5\""
2025-05-16T19:40:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:21.755945415-04:00" level=info msg="StartContainer for \"9de3d607de1a6e56d45cca2ab4d502dadb4e580662c0b3c7192848eb882886f5\" returns successfully"
2025-05-16T19:40:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:23.214315627-04:00" level=info msg="shim disconnected" id=9de3d607de1a6e56d45cca2ab4d502dadb4e580662c0b3c7192848eb882886f5
2025-05-16T19:40:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:23.214547454-04:00" level=warning msg="cleaning up after shim disconnected" id=9de3d607de1a6e56d45cca2ab4d502dadb4e580662c0b3c7192848eb882886f5 namespace=k8s.io
2025-05-16T19:40:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:23.214572414-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:40:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:23.346554524-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:40:23-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=53597 runtime=io.containerd.runc.v2\n"
2025-05-16T19:40:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:24.005912230-04:00" level=info msg="CreateContainer within sandbox \"1f1219d99feb476db59bcd2fb3ff3a7d901ca1d9e8eaa450868a0662a454f40a\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:40:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:24.084531721-04:00" level=info msg="CreateContainer within sandbox \"1f1219d99feb476db59bcd2fb3ff3a7d901ca1d9e8eaa450868a0662a454f40a\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"807ba167dc57858c733188d6238a5cfb64fae824c663afec8f9590c3a15064b8\""
2025-05-16T19:40:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:24.093878401-04:00" level=info msg="StartContainer for \"807ba167dc57858c733188d6238a5cfb64fae824c663afec8f9590c3a15064b8\""
2025-05-16T19:40:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:24.587422906-04:00" level=info msg="StartContainer for \"807ba167dc57858c733188d6238a5cfb64fae824c663afec8f9590c3a15064b8\" returns successfully"
2025-05-16T19:40:25-04:00 k8s.k8sd[35160]: I0516 19:40:25.066822   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:25-04:00 k8s.kube-apiserver[37236]: I0516 19:40:25.425956   37236 alloc.go:330] "allocated clusterIPs" service="openstack/traefik-public" clusterIPs={"IPv4":"10.152.183.20"}
2025-05-16T19:40:25-04:00 k8s.kubelet[38157]: I0516 19:40:25.648379   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/certificate-authority-0" podStartSLOduration=8.648354193 podStartE2EDuration="8.648354193s" podCreationTimestamp="2025-05-16 19:40:17 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:40:25.637704106 -0400 EDT m=+418.775300793" watchObservedRunningTime="2025-05-16 19:40:25.648354193 -0400 EDT m=+418.785950885"
2025-05-16T19:40:26-04:00 k8s.kube-apiserver[37236]: I0516 19:40:26.613746   37236 alloc.go:330] "allocated clusterIPs" service="openstack/ovn-central" clusterIPs={"IPv4":"10.152.183.243"}
2025-05-16T19:40:28-04:00 k8s.k8sd[35160]: I0516 19:40:28.155380   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:28.895685951-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/l0cz4xi5v0335kaxbsyzeg48uiorm1qfybohq/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:40:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:28.902781798-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:e3a0e00f3ef851f223f1c05889815764ffad8e819c378aabd06be8a150a40336,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:40:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:28.906157759-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/l0cz4xi5v0335kaxbsyzeg48uiorm1qfybohq/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\" returns image reference \"sha256:e3a0e00f3ef851f223f1c05889815764ffad8e819c378aabd06be8a150a40336\""
2025-05-16T19:40:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:28.994391103-04:00" level=info msg="CreateContainer within sandbox \"2fd80b566f29cec21cd726eaab789529e78ad5d6add321bfd46e04a827926e02\" for container &ContainerMetadata{Name:ovsdb-server,Attempt:0,}"
2025-05-16T19:40:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:29.078717708-04:00" level=info msg="CreateContainer within sandbox \"2fd80b566f29cec21cd726eaab789529e78ad5d6add321bfd46e04a827926e02\" for &ContainerMetadata{Name:ovsdb-server,Attempt:0,} returns container id \"4efb713746e412bfbe4bba7dcb6b7e5dae2292ab516fa9321a7a73c5e80a3089\""
2025-05-16T19:40:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:29.094158822-04:00" level=info msg="StartContainer for \"4efb713746e412bfbe4bba7dcb6b7e5dae2292ab516fa9321a7a73c5e80a3089\""
2025-05-16T19:40:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:29.716832155-04:00" level=info msg="StartContainer for \"4efb713746e412bfbe4bba7dcb6b7e5dae2292ab516fa9321a7a73c5e80a3089\" returns successfully"
2025-05-16T19:40:30-04:00 k8s.k8sd[35160]: I0516 19:40:30.153188   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:30-04:00 k8s.kubelet[38157]: I0516 19:40:30.702043   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/ovn-relay-0" podStartSLOduration=7.364262523 podStartE2EDuration="22.702014176s" podCreationTimestamp="2025-05-16 19:40:08 -0400 EDT" firstStartedPulling="2025-05-16 19:40:13.573750182 -0400 EDT m=+406.711346869" lastFinishedPulling="2025-05-16 19:40:28.911501837 -0400 EDT m=+422.049098522" observedRunningTime="2025-05-16 19:40:30.685241122 -0400 EDT m=+423.822837809" watchObservedRunningTime="2025-05-16 19:40:30.702014176 -0400 EDT m=+423.839610881"
2025-05-16T19:40:31-04:00 k8s.kube-apiserver[37236]: I0516 19:40:31.560104   37236 alloc.go:330] "allocated clusterIPs" service="openstack/mysql" clusterIPs={"IPv4":"10.152.183.152"}
2025-05-16T19:40:31-04:00 k8s.k8sd[35160]: I0516 19:40:31.852162   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:32-04:00 k8s.k8sd[35160]: I0516 19:40:32.870712   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:35-04:00 k8s.k8sd[35160]: I0516 19:40:35.619364   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:35-04:00 k8s.k8sd[35160]: I0516 19:40:35.902469   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:36-04:00 k8s.k8sd[35160]: I0516 19:40:36.284010   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:36-04:00 k8s.k8sd[35160]: I0516 19:40:36.422724   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:36-04:00 k8s.k8sd[35160]: I0516 19:40:36.590945   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:36-04:00 k8s.k8sd[35160]: I0516 19:40:36.697529   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:36-04:00 k8s.k8sd[35160]: I0516 19:40:36.738868   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:36-04:00 k8s.k8sd[35160]: I0516 19:40:36.880178   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:36-04:00 k8s.k8sd[35160]: I0516 19:40:36.982261   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:37-04:00 k8s.k8sd[35160]: I0516 19:40:37.054791   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:37.076907434-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/kvq0v0ar36fu9jx5c3x2f22ov9o27i7r85p9a/rabbitmq-image@sha256:ab0adb77003a3b523d8100e8be916725189ce877d49e8585b140831165472325,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:40:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:37.084365661-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:7136cb57111e5b4b924154c7525c9259aa6c721734d71ab32fa22ef0f29cada5,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:40:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:37.094272887-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/kvq0v0ar36fu9jx5c3x2f22ov9o27i7r85p9a/rabbitmq-image@sha256:ab0adb77003a3b523d8100e8be916725189ce877d49e8585b140831165472325\" returns image reference \"sha256:7136cb57111e5b4b924154c7525c9259aa6c721734d71ab32fa22ef0f29cada5\""
2025-05-16T19:40:37-04:00 k8s.k8sd[35160]: I0516 19:40:37.138430   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:37.184930440-04:00" level=info msg="CreateContainer within sandbox \"01baab015394f063cbb4f5614d6b9a999c097aeadcd74146e296d6e3ab929bf8\" for container &ContainerMetadata{Name:rabbitmq,Attempt:0,}"
2025-05-16T19:40:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:37.311091200-04:00" level=info msg="CreateContainer within sandbox \"01baab015394f063cbb4f5614d6b9a999c097aeadcd74146e296d6e3ab929bf8\" for &ContainerMetadata{Name:rabbitmq,Attempt:0,} returns container id \"76f83203298957557d6a5652939c4e3a1a2c2920bb9e5cb0774e8bd7ae2d61b7\""
2025-05-16T19:40:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:37.319249172-04:00" level=info msg="StartContainer for \"76f83203298957557d6a5652939c4e3a1a2c2920bb9e5cb0774e8bd7ae2d61b7\""
2025-05-16T19:40:37-04:00 k8s.k8sd[35160]: I0516 19:40:37.347687   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:37-04:00 k8s.k8sd[35160]: I0516 19:40:37.429738   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:37-04:00 k8s.k8sd[35160]: I0516 19:40:37.508288   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:37-04:00 k8s.k8sd[35160]: I0516 19:40:37.573338   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:37.740015391-04:00" level=info msg="StartContainer for \"76f83203298957557d6a5652939c4e3a1a2c2920bb9e5cb0774e8bd7ae2d61b7\" returns successfully"
2025-05-16T19:40:37-04:00 k8s.k8sd[35160]: I0516 19:40:37.753401   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:37-04:00 k8s.k8sd[35160]: I0516 19:40:37.871103   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:38-04:00 k8s.kubelet[38157]: I0516 19:40:38.244840   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ktjhc\" (UniqueName: \"kubernetes.io/projected/a99e13cc-ccfb-4b25-85b6-ade546d577c8-kube-api-access-ktjhc\") pod \"traefik-0\" (UID: \"a99e13cc-ccfb-4b25-85b6-ade546d577c8\") " pod="openstack/traefik-0"
2025-05-16T19:40:38-04:00 k8s.kubelet[38157]: I0516 19:40:38.244930   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/a99e13cc-ccfb-4b25-85b6-ade546d577c8-charm-data\") pod \"traefik-0\" (UID: \"a99e13cc-ccfb-4b25-85b6-ade546d577c8\") " pod="openstack/traefik-0"
2025-05-16T19:40:38-04:00 k8s.kubelet[38157]: I0516 19:40:38.245010   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-b570f973-40c0-462d-9a52-804ed452459b\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-b570f973-40c0-462d-9a52-804ed452459b\") pod \"traefik-0\" (UID: \"a99e13cc-ccfb-4b25-85b6-ade546d577c8\") " pod="openstack/traefik-0"
2025-05-16T19:40:38-04:00 k8s.kubelet[38157]: I0516 19:40:38.608195   38157 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"pvc-b570f973-40c0-462d-9a52-804ed452459b\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-b570f973-40c0-462d-9a52-804ed452459b\") pod \"traefik-0\" (UID: \"a99e13cc-ccfb-4b25-85b6-ade546d577c8\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/rawfile.csi.openebs.io/e73516d6e3f68f38a201ed5c5f260f4a76fb8980b443fe29419cdfc3a286864d/globalmount\"" pod="openstack/traefik-0"
2025-05-16T19:40:38-04:00 k8s.k8sd[35160]: I0516 19:40:38.915533   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:38-04:00 k8s.kubelet[38157]: I0516 19:40:38.929946   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/rabbitmq-0" podStartSLOduration=9.127307055 podStartE2EDuration="26.929918892s" podCreationTimestamp="2025-05-16 19:40:12 -0400 EDT" firstStartedPulling="2025-05-16 19:40:19.2935796 -0400 EDT m=+412.431176283" lastFinishedPulling="2025-05-16 19:40:37.096191437 -0400 EDT m=+430.233788120" observedRunningTime="2025-05-16 19:40:38.885715415 -0400 EDT m=+432.023312100" watchObservedRunningTime="2025-05-16 19:40:38.929918892 -0400 EDT m=+432.067515584"
2025-05-16T19:40:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:39.089533660-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:traefik-0,Uid:a99e13cc-ccfb-4b25-85b6-ade546d577c8,Namespace:openstack,Attempt:0,}"
2025-05-16T19:40:40-04:00 k8s.k8sd[35160]: I0516 19:40:40.080831   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:40.259429941-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:40:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:40.260799615-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:40:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:40.260898920-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:40:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:40.268107239-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/147f9a110389e1baa444957ebc3aac74f2c022304c8ff54c8303792015342825 pid=54115 runtime=io.containerd.runc.v2
2025-05-16T19:40:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:40.845414148-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:traefik-0,Uid:a99e13cc-ccfb-4b25-85b6-ade546d577c8,Namespace:openstack,Attempt:0,} returns sandbox id \"147f9a110389e1baa444957ebc3aac74f2c022304c8ff54c8303792015342825\""
2025-05-16T19:40:41-04:00 k8s.k8sd[35160]: E0516 19:40:41.107052   35160 controllers/feature.go:168] "Failed to apply feature configuration" err="failed to apply configuration: failed to enable ingress: failed to upgrade ck-network: release: already exists" logger="k8sd" controller="feature" feature="ingress"
2025-05-16T19:40:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:41.142216636-04:00" level=info msg="CreateContainer within sandbox \"147f9a110389e1baa444957ebc3aac74f2c022304c8ff54c8303792015342825\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:40:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:41.214449639-04:00" level=info msg="CreateContainer within sandbox \"147f9a110389e1baa444957ebc3aac74f2c022304c8ff54c8303792015342825\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"a2ff40ed2d5bb1284d7367ada43cabc9519a5ed7da02074f59aaaf0e38a3be9d\""
2025-05-16T19:40:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:41.218673575-04:00" level=info msg="StartContainer for \"a2ff40ed2d5bb1284d7367ada43cabc9519a5ed7da02074f59aaaf0e38a3be9d\""
2025-05-16T19:40:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:41.555718929-04:00" level=info msg="StartContainer for \"a2ff40ed2d5bb1284d7367ada43cabc9519a5ed7da02074f59aaaf0e38a3be9d\" returns successfully"
2025-05-16T19:40:42-04:00 k8s.kubelet[38157]: E0516 19:40:42.714563   38157 cadvisor_stats_provider.go:522] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda99e13cc_ccfb_4b25_85b6_ade546d577c8.slice/cri-containerd-a2ff40ed2d5bb1284d7367ada43cabc9519a5ed7da02074f59aaaf0e38a3be9d.scope\": RecentStats: unable to find data in memory cache]"
2025-05-16T19:40:43-04:00 k8s.kubelet[38157]: I0516 19:40:43.464557   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fqqt7\" (UniqueName: \"kubernetes.io/projected/79e56497-100a-456a-bede-759ccf83e1fc-kube-api-access-fqqt7\") pod \"traefik-public-0\" (UID: \"79e56497-100a-456a-bede-759ccf83e1fc\") " pod="openstack/traefik-public-0"
2025-05-16T19:40:43-04:00 k8s.kubelet[38157]: I0516 19:40:43.464641   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/79e56497-100a-456a-bede-759ccf83e1fc-charm-data\") pod \"traefik-public-0\" (UID: \"79e56497-100a-456a-bede-759ccf83e1fc\") " pod="openstack/traefik-public-0"
2025-05-16T19:40:43-04:00 k8s.kubelet[38157]: I0516 19:40:43.464698   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-d2880e52-49d9-4bfd-b7a5-342b9207a685\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-d2880e52-49d9-4bfd-b7a5-342b9207a685\") pod \"traefik-public-0\" (UID: \"79e56497-100a-456a-bede-759ccf83e1fc\") " pod="openstack/traefik-public-0"
2025-05-16T19:40:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:43.630556733-04:00" level=info msg="shim disconnected" id=a2ff40ed2d5bb1284d7367ada43cabc9519a5ed7da02074f59aaaf0e38a3be9d
2025-05-16T19:40:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:43.630651738-04:00" level=warning msg="cleaning up after shim disconnected" id=a2ff40ed2d5bb1284d7367ada43cabc9519a5ed7da02074f59aaaf0e38a3be9d namespace=k8s.io
2025-05-16T19:40:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:43.630673314-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:40:43-04:00 k8s.kubelet[38157]: I0516 19:40:43.755538   38157 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"pvc-d2880e52-49d9-4bfd-b7a5-342b9207a685\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-d2880e52-49d9-4bfd-b7a5-342b9207a685\") pod \"traefik-public-0\" (UID: \"79e56497-100a-456a-bede-759ccf83e1fc\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/rawfile.csi.openebs.io/50df95887380087e46bc43e99b3685e08391344cac3b4d5ea9ba7855869a6e0f/globalmount\"" pod="openstack/traefik-public-0"
2025-05-16T19:40:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:43.770232610-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:40:43-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=54306 runtime=io.containerd.runc.v2\n"
2025-05-16T19:40:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:43.874111332-04:00" level=info msg="PullImage \"docker.io/jujusolutions/charm-base:ubuntu-20.04\""
2025-05-16T19:40:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:44.032348673-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:traefik-public-0,Uid:79e56497-100a-456a-bede-759ccf83e1fc,Namespace:openstack,Attempt:0,}"
2025-05-16T19:40:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:44.508752561-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:40:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:44.511139602-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:40:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:44.511196742-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:40:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:44.514261440-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/4ee93c9afe20443c7c7a3f4e39802c14041532615de879c17e4c3140457cf53e pid=54383 runtime=io.containerd.runc.v2
2025-05-16T19:40:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:45.113260469-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:traefik-public-0,Uid:79e56497-100a-456a-bede-759ccf83e1fc,Namespace:openstack,Attempt:0,} returns sandbox id \"4ee93c9afe20443c7c7a3f4e39802c14041532615de879c17e4c3140457cf53e\""
2025-05-16T19:40:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:45.471425670-04:00" level=info msg="CreateContainer within sandbox \"4ee93c9afe20443c7c7a3f4e39802c14041532615de879c17e4c3140457cf53e\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:40:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:45.579720536-04:00" level=info msg="CreateContainer within sandbox \"4ee93c9afe20443c7c7a3f4e39802c14041532615de879c17e4c3140457cf53e\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"302d1bcdedcd35fa5890a727991992a2aeda9f1be42ad4542dbf12f9a54c0b22\""
2025-05-16T19:40:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:45.581783026-04:00" level=info msg="StartContainer for \"302d1bcdedcd35fa5890a727991992a2aeda9f1be42ad4542dbf12f9a54c0b22\""
2025-05-16T19:40:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:45.957034101-04:00" level=info msg="StartContainer for \"302d1bcdedcd35fa5890a727991992a2aeda9f1be42ad4542dbf12f9a54c0b22\" returns successfully"
2025-05-16T19:40:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:48.071619117-04:00" level=info msg="shim disconnected" id=302d1bcdedcd35fa5890a727991992a2aeda9f1be42ad4542dbf12f9a54c0b22
2025-05-16T19:40:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:48.071719270-04:00" level=warning msg="cleaning up after shim disconnected" id=302d1bcdedcd35fa5890a727991992a2aeda9f1be42ad4542dbf12f9a54c0b22 namespace=k8s.io
2025-05-16T19:40:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:48.071744234-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:40:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:48.383291819-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:40:48-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=54590 runtime=io.containerd.runc.v2\n"
2025-05-16T19:40:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:49.121701836-04:00" level=info msg="PullImage \"docker.io/jujusolutions/charm-base:ubuntu-20.04\""
2025-05-16T19:40:50-04:00 k8s.kubelet[38157]: I0516 19:40:50.744537   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-2fe6db2b-18fb-4ffc-a963-a71a4db765f3\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-2fe6db2b-18fb-4ffc-a963-a71a4db765f3\") pod \"mysql-0\" (UID: \"c77b1b39-faa4-41a1-9bca-6728d9485063\") " pod="openstack/mysql-0"
2025-05-16T19:40:50-04:00 k8s.kubelet[38157]: I0516 19:40:50.745312   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/c77b1b39-faa4-41a1-9bca-6728d9485063-charm-data\") pod \"mysql-0\" (UID: \"c77b1b39-faa4-41a1-9bca-6728d9485063\") " pod="openstack/mysql-0"
2025-05-16T19:40:50-04:00 k8s.kubelet[38157]: I0516 19:40:50.746681   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-w6g7b\" (UniqueName: \"kubernetes.io/projected/c77b1b39-faa4-41a1-9bca-6728d9485063-kube-api-access-w6g7b\") pod \"mysql-0\" (UID: \"c77b1b39-faa4-41a1-9bca-6728d9485063\") " pod="openstack/mysql-0"
2025-05-16T19:40:51-04:00 k8s.kubelet[38157]: I0516 19:40:51.053095   38157 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"pvc-2fe6db2b-18fb-4ffc-a963-a71a4db765f3\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-2fe6db2b-18fb-4ffc-a963-a71a4db765f3\") pod \"mysql-0\" (UID: \"c77b1b39-faa4-41a1-9bca-6728d9485063\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/rawfile.csi.openebs.io/56868e5db204bcf7b3fecafd886985c46e6e1094062dc93b4ea2f069d697109f/globalmount\"" pod="openstack/mysql-0"
2025-05-16T19:40:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:51.209778564-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:mysql-0,Uid:c77b1b39-faa4-41a1-9bca-6728d9485063,Namespace:openstack,Attempt:0,}"
2025-05-16T19:40:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:52.256028756-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:40:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:52.256232458-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:40:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:52.256262029-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:40:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:52.257710715-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/5f95dcf1eda955294c05e70845e9c92654d97069740f9bda05715910b222c08d pid=54788 runtime=io.containerd.runc.v2
2025-05-16T19:40:53-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:53.366872371-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:mysql-0,Uid:c77b1b39-faa4-41a1-9bca-6728d9485063,Namespace:openstack,Attempt:0,} returns sandbox id \"5f95dcf1eda955294c05e70845e9c92654d97069740f9bda05715910b222c08d\""
2025-05-16T19:40:53-04:00 k8s.k8sd[35160]: I0516 19:40:53.444690   35160 helm/client.go:40] "preparing upgrade for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:40:53-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:53.764233137-04:00" level=info msg="CreateContainer within sandbox \"5f95dcf1eda955294c05e70845e9c92654d97069740f9bda05715910b222c08d\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:40:53-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:53.903012752-04:00" level=info msg="CreateContainer within sandbox \"5f95dcf1eda955294c05e70845e9c92654d97069740f9bda05715910b222c08d\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"241fefd2da138f47afe2863e87c44b06f247f9a83f9fd098b3049cacde28bf3f\""
2025-05-16T19:40:53-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:53.912093064-04:00" level=info msg="StartContainer for \"241fefd2da138f47afe2863e87c44b06f247f9a83f9fd098b3049cacde28bf3f\""
2025-05-16T19:40:54-04:00 k8s.kubelet[38157]: I0516 19:40:54.149187   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/0ce412b2-e2b3-443c-88fc-05561df70d90-charm-data\") pod \"ovn-central-0\" (UID: \"0ce412b2-e2b3-443c-88fc-05561df70d90\") " pod="openstack/ovn-central-0"
2025-05-16T19:40:54-04:00 k8s.kubelet[38157]: I0516 19:40:54.149294   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nn52b\" (UniqueName: \"kubernetes.io/projected/0ce412b2-e2b3-443c-88fc-05561df70d90-kube-api-access-nn52b\") pod \"ovn-central-0\" (UID: \"0ce412b2-e2b3-443c-88fc-05561df70d90\") " pod="openstack/ovn-central-0"
2025-05-16T19:40:54-04:00 k8s.kubelet[38157]: I0516 19:40:54.149344   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-420e7239-d709-4b77-a0cd-282b00fe8eaa\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-420e7239-d709-4b77-a0cd-282b00fe8eaa\") pod \"ovn-central-0\" (UID: \"0ce412b2-e2b3-443c-88fc-05561df70d90\") " pod="openstack/ovn-central-0"
2025-05-16T19:40:54-04:00 k8s.kubelet[38157]: I0516 19:40:54.482943   38157 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"pvc-420e7239-d709-4b77-a0cd-282b00fe8eaa\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-420e7239-d709-4b77-a0cd-282b00fe8eaa\") pod \"ovn-central-0\" (UID: \"0ce412b2-e2b3-443c-88fc-05561df70d90\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/rawfile.csi.openebs.io/d71c512936522563830b2752b5d8a33d6d690153c222fcbda6854ca0862de726/globalmount\"" pod="openstack/ovn-central-0"
2025-05-16T19:40:54-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:54.824083888-04:00" level=info msg="StartContainer for \"241fefd2da138f47afe2863e87c44b06f247f9a83f9fd098b3049cacde28bf3f\" returns successfully"
2025-05-16T19:40:54-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:54.873032360-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:ovn-central-0,Uid:0ce412b2-e2b3-443c-88fc-05561df70d90,Namespace:openstack,Attempt:0,}"
2025-05-16T19:40:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:56.409917579-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:40:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:56.416762147-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:40:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:56.416802742-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:40:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:56.430967300-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573 pid=55051 runtime=io.containerd.runc.v2
2025-05-16T19:40:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:56.786219429-04:00" level=info msg="shim disconnected" id=241fefd2da138f47afe2863e87c44b06f247f9a83f9fd098b3049cacde28bf3f
2025-05-16T19:40:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:56.786952189-04:00" level=warning msg="cleaning up after shim disconnected" id=241fefd2da138f47afe2863e87c44b06f247f9a83f9fd098b3049cacde28bf3f namespace=k8s.io
2025-05-16T19:40:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:56.786990305-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:40:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:56.976778279-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:40:56-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=55114 runtime=io.containerd.runc.v2\n"
2025-05-16T19:40:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:57.311581172-04:00" level=info msg="PullImage \"docker.io/jujusolutions/charm-base:ubuntu-22.04\""
2025-05-16T19:40:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:57.435180691-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:ovn-central-0,Uid:0ce412b2-e2b3-443c-88fc-05561df70d90,Namespace:openstack,Attempt:0,} returns sandbox id \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\""
2025-05-16T19:40:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:57.869101976-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:40:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:57.967845732-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"df2e7b767ce96cbf67086bc8ea714276c748f00f83210cbc62bfc375100a1692\""
2025-05-16T19:40:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:57.975578115-04:00" level=info msg="StartContainer for \"df2e7b767ce96cbf67086bc8ea714276c748f00f83210cbc62bfc375100a1692\""
2025-05-16T19:40:58-04:00 k8s.containerd[36097]: time="2025-05-16T19:40:58.380826813-04:00" level=info msg="StartContainer for \"df2e7b767ce96cbf67086bc8ea714276c748f00f83210cbc62bfc375100a1692\" returns successfully"
2025-05-16T19:40:59-04:00 k8s.k8sd[35160]: I0516 19:40:59.605091   35160 helm/client.go:40] "merging values from old release to new values" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:00.611869065-04:00" level=info msg="shim disconnected" id=df2e7b767ce96cbf67086bc8ea714276c748f00f83210cbc62bfc375100a1692
2025-05-16T19:41:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:00.623772309-04:00" level=warning msg="cleaning up after shim disconnected" id=df2e7b767ce96cbf67086bc8ea714276c748f00f83210cbc62bfc375100a1692 namespace=k8s.io
2025-05-16T19:41:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:00.623803740-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:41:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:00.835380228-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:41:00-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=55443 runtime=io.containerd.runc.v2\n"
2025-05-16T19:41:02-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:02.280008424-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:41:02-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:02.446743189-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"cad783f37ebb96b1ee4fd4609186402d67977d731b1a188b38c15d1d02be8193\""
2025-05-16T19:41:02-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:02.458157720-04:00" level=info msg="StartContainer for \"cad783f37ebb96b1ee4fd4609186402d67977d731b1a188b38c15d1d02be8193\""
2025-05-16T19:41:02-04:00 k8s.kube-apiserver[37236]: I0516 19:41:02.564533   37236 alloc.go:330] "allocated clusterIPs" service="openstack/rabbitmq-lb" clusterIPs={"IPv4":"10.152.183.150"}
2025-05-16T19:41:02-04:00 k8s.kube-controller-manager[36331]: I0516 19:41:02.775553   36331 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="openstack/rabbitmq-lb" err="EndpointSlice informer cache is out of date"
2025-05-16T19:41:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:03.513294352-04:00" level=info msg="StartContainer for \"cad783f37ebb96b1ee4fd4609186402d67977d731b1a188b38c15d1d02be8193\" returns successfully"
2025-05-16T19:41:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:03.527935237-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-nb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\""
2025-05-16T19:41:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:04.673023239-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-nb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:04.684133554-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-nb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\" returns image reference \"sha256:e3a0e00f3ef851f223f1c05889815764ffad8e819c378aabd06be8a150a40336\""
2025-05-16T19:41:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:04.859888407-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for container &ContainerMetadata{Name:ovn-nb-db-server,Attempt:0,}"
2025-05-16T19:41:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:04.994492243-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for &ContainerMetadata{Name:ovn-nb-db-server,Attempt:0,} returns container id \"0603abdabc2197f78f3f63e6bcaea170f27508a8a56953eccb30d42e477f09bc\""
2025-05-16T19:41:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:05.012927165-04:00" level=info msg="StartContainer for \"0603abdabc2197f78f3f63e6bcaea170f27508a8a56953eccb30d42e477f09bc\""
2025-05-16T19:41:05-04:00 k8s.k8sd[35160]: I0516 19:41:05.973203   35160 helm/client.go:40] "performing update for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:06-04:00 k8s.kube-apiserver[37236]: I0516 19:41:06.057381   37236 alloc.go:330] "allocated clusterIPs" service="openstack/ovn-relay-lb" clusterIPs={"IPv4":"10.152.183.244"}
2025-05-16T19:41:06-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:06.182360700-04:00" level=info msg="StartContainer for \"0603abdabc2197f78f3f63e6bcaea170f27508a8a56953eccb30d42e477f09bc\" returns successfully"
2025-05-16T19:41:06-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:06.190655061-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\""
2025-05-16T19:41:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:07.132493333-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:41:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:07.578599738-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:41:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:07.586616226-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": failed to resolve reference \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:41:07-04:00 k8s.kubelet[38157]: E0516 19:41:07.592238   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": failed to resolve reference \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631"
2025-05-16T19:41:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:07.593987348-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\""
2025-05-16T19:41:08-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:08.990914113-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:08-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:08.995026291-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-northd-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\" returns image reference \"sha256:e3a0e00f3ef851f223f1c05889815764ffad8e819c378aabd06be8a150a40336\""
2025-05-16T19:41:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:09.094903386-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for container &ContainerMetadata{Name:ovn-northd,Attempt:0,}"
2025-05-16T19:41:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:09.173543347-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for &ContainerMetadata{Name:ovn-northd,Attempt:0,} returns container id \"c10458ee303ee5a7bb49960d45bb49434bc8c874dddb87469ce9c58602d6d979\""
2025-05-16T19:41:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:09.181656895-04:00" level=info msg="StartContainer for \"c10458ee303ee5a7bb49960d45bb49434bc8c874dddb87469ce9c58602d6d979\""
2025-05-16T19:41:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:09.989719147-04:00" level=info msg="StartContainer for \"c10458ee303ee5a7bb49960d45bb49434bc8c874dddb87469ce9c58602d6d979\" returns successfully"
2025-05-16T19:41:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:09.999255714-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\""
2025-05-16T19:41:10-04:00 k8s.k8sd[35160]: I0516 19:41:10.414672   35160 helm/client.go:40] "creating upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.017432   35160 helm/client.go:40] "checking 14 resources for changes" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.044363   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.071845   35160 helm/client.go:40] "Looks like there are no changes for ServiceAccount \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.123723   35160 helm/client.go:40] "Looks like there are no changes for Secret \"cilium-ca\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.177103   35160 helm/client.go:40] "Patch Secret \"hubble-server-certs\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.278028   35160 helm/client.go:40] "Looks like there are no changes for ConfigMap \"cilium-config\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:11.345081771-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.368837   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.423867   35160 helm/client.go:40] "Looks like there are no changes for ClusterRole \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.464695   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.503633   35160 helm/client.go:40] "Looks like there are no changes for ClusterRoleBinding \"cilium-operator\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.544695   35160 helm/client.go:40] "Looks like there are no changes for Role \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.609749   35160 helm/client.go:40] "Looks like there are no changes for RoleBinding \"cilium-config-agent\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.670902   35160 helm/client.go:40] "Looks like there are no changes for Service \"hubble-peer\"" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.k8sd[35160]: I0516 19:41:11.796821   35160 helm/client.go:40] "Patch DaemonSet \"cilium\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:11.808179829-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:41:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:11.811117318-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": failed to resolve reference \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:41:11-04:00 k8s.kubelet[38157]: E0516 19:41:11.812629   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": failed to resolve reference \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631"
2025-05-16T19:41:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:11.814755023-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\""
2025-05-16T19:41:12-04:00 k8s.k8sd[35160]: I0516 19:41:12.022602   35160 helm/client.go:40] "Patch Deployment \"cilium-operator\" in namespace kube-system" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:13-04:00 k8s.k8sd[35160]: I0516 19:41:13.086613   35160 helm/client.go:40] "updating status for upgraded release for ck-network" logger="k8sd.helm" controller="feature"
2025-05-16T19:41:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:13.396140911-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:41:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:13.857017480-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:41:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:13.865646808-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": failed to resolve reference \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:41:13-04:00 k8s.kubelet[38157]: E0516 19:41:13.866495   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": failed to resolve reference \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631"
2025-05-16T19:41:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:13.869144945-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\""
2025-05-16T19:41:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:15.051814406-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:15.074243083-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6a0rnzywlucfo4rvn7y2aylcc19uaarnwsrge/ovn-sb-db-server-image@sha256:745e40f55364c53b870bd72ff9ed8d55cdf070738332260c0b917c66cff5a631\" returns image reference \"sha256:e3a0e00f3ef851f223f1c05889815764ffad8e819c378aabd06be8a150a40336\""
2025-05-16T19:41:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:15.227997540-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for container &ContainerMetadata{Name:ovn-sb-db-server,Attempt:0,}"
2025-05-16T19:41:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:15.342570974-04:00" level=info msg="CreateContainer within sandbox \"a32f3c38a1fe6e06ae8e47bb51eebbd437e3c9b9fc0e5fc541742eab5e96f573\" for &ContainerMetadata{Name:ovn-sb-db-server,Attempt:0,} returns container id \"eebc66bf96b1c36798e54390e1af4db97a339d67fc6cad5ba70b8faec290578f\""
2025-05-16T19:41:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:15.363241895-04:00" level=info msg="StartContainer for \"eebc66bf96b1c36798e54390e1af4db97a339d67fc6cad5ba70b8faec290578f\""
2025-05-16T19:41:16-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:16.222630702-04:00" level=info msg="StartContainer for \"eebc66bf96b1c36798e54390e1af4db97a339d67fc6cad5ba70b8faec290578f\" returns successfully"
2025-05-16T19:41:17-04:00 k8s.kubelet[38157]: I0516 19:41:17.303583   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/ovn-central-0" podStartSLOduration=13.750254638 podStartE2EDuration="25.30355197s" podCreationTimestamp="2025-05-16 19:40:52 -0400 EDT" firstStartedPulling="2025-05-16 19:41:03.523216711 -0400 EDT m=+456.660813393" lastFinishedPulling="2025-05-16 19:41:15.076514043 -0400 EDT m=+468.214110725" observedRunningTime="2025-05-16 19:41:17.296162273 -0400 EDT m=+470.433758962" watchObservedRunningTime="2025-05-16 19:41:17.30355197 -0400 EDT m=+470.441148644"
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.109411691-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/charm-base:ubuntu-20.04,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.126915362-04:00" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/jujusolutions/charm-base:ubuntu-20.04,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.134572210-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:bac94bbc9252256ca990f67b68b21c1b382edd0fc0a9a6aa586b1a4825baac83,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.156301440-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/charm-base@sha256:e8a3eb8909d3c86d5ca5944fba8438f3a55ad4302f0bcc1a1b98aa434c98f0da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.162539193-04:00" level=info msg="PullImage \"docker.io/jujusolutions/charm-base:ubuntu-20.04\" returns image reference \"sha256:bac94bbc9252256ca990f67b68b21c1b382edd0fc0a9a6aa586b1a4825baac83\""
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.169865290-04:00" level=info msg="PullImage \"docker.io/jujusolutions/charm-base:ubuntu-20.04\" returns image reference \"sha256:bac94bbc9252256ca990f67b68b21c1b382edd0fc0a9a6aa586b1a4825baac83\""
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.826900620-04:00" level=info msg="CreateContainer within sandbox \"4ee93c9afe20443c7c7a3f4e39802c14041532615de879c17e4c3140457cf53e\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.901819492-04:00" level=info msg="CreateContainer within sandbox \"147f9a110389e1baa444957ebc3aac74f2c022304c8ff54c8303792015342825\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.920612772-04:00" level=info msg="CreateContainer within sandbox \"4ee93c9afe20443c7c7a3f4e39802c14041532615de879c17e4c3140457cf53e\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"cb950eeb31ab129292f7aa5a8be490ac20ce291f753aa5dace46a614cde85c9f\""
2025-05-16T19:41:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:26.932704515-04:00" level=info msg="StartContainer for \"cb950eeb31ab129292f7aa5a8be490ac20ce291f753aa5dace46a614cde85c9f\""
2025-05-16T19:41:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:27.157389417-04:00" level=info msg="CreateContainer within sandbox \"147f9a110389e1baa444957ebc3aac74f2c022304c8ff54c8303792015342825\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"7a19a1f4c0ec3bcc722169b77006a55297f1f701661bcc8b5c55e9be84e4b1d4\""
2025-05-16T19:41:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:27.165759935-04:00" level=info msg="StartContainer for \"7a19a1f4c0ec3bcc722169b77006a55297f1f701661bcc8b5c55e9be84e4b1d4\""
2025-05-16T19:41:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:27.736234906-04:00" level=info msg="StartContainer for \"cb950eeb31ab129292f7aa5a8be490ac20ce291f753aa5dace46a614cde85c9f\" returns successfully"
2025-05-16T19:41:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:27.747890884-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/3cs9bo6mym14ler2zuvzsnqcd8pvucf6cxvlw/traefik-image@sha256:af66fc2c3970bb9a7d1303df391a00b5b09876005343fb9963eaac03304c0a3e\""
2025-05-16T19:41:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:27.837888761-04:00" level=info msg="StartContainer for \"7a19a1f4c0ec3bcc722169b77006a55297f1f701661bcc8b5c55e9be84e4b1d4\" returns successfully"
2025-05-16T19:41:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:27.845940540-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/3cs9bo6mym14ler2zuvzsnqcd8pvucf6cxvlw/traefik-image@sha256:af66fc2c3970bb9a7d1303df391a00b5b09876005343fb9963eaac03304c0a3e\""
2025-05-16T19:41:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:35.322876576-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/charm-base:ubuntu-22.04,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:35.345027482-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:5bf9581322a25d9e840e2e6da70c38ed9d32dbfc8a9d7210e65d950cac789462,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:35.380942332-04:00" level=info msg="ImageCreate event &ImageCreate{Name:docker.io/jujusolutions/charm-base@sha256:5ff8ce2dd5000378cdcc12998cf5b8b6b10d7193e47acb2945f61ae1ebd1a448,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:35.395013381-04:00" level=info msg="PullImage \"docker.io/jujusolutions/charm-base:ubuntu-22.04\" returns image reference \"sha256:5bf9581322a25d9e840e2e6da70c38ed9d32dbfc8a9d7210e65d950cac789462\""
2025-05-16T19:41:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:35.829358387-04:00" level=info msg="CreateContainer within sandbox \"5f95dcf1eda955294c05e70845e9c92654d97069740f9bda05715910b222c08d\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:41:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:35.916401826-04:00" level=info msg="CreateContainer within sandbox \"5f95dcf1eda955294c05e70845e9c92654d97069740f9bda05715910b222c08d\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"4587234ae611792c543b147b67fa22df4fc59588cf81a644ad9b6a8cd1f37197\""
2025-05-16T19:41:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:35.937626776-04:00" level=info msg="StartContainer for \"4587234ae611792c543b147b67fa22df4fc59588cf81a644ad9b6a8cd1f37197\""
2025-05-16T19:41:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:37.299389215-04:00" level=info msg="StartContainer for \"4587234ae611792c543b147b67fa22df4fc59588cf81a644ad9b6a8cd1f37197\" returns successfully"
2025-05-16T19:41:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:37.327965092-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/62ptdfbrjpw3n9tcnswjpart30jauc6wc5wbi/mysql-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9\""
2025-05-16T19:41:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:47.596787090-04:00" level=error msg="ExecSync for \"66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d\" failed" error="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded"
2025-05-16T19:41:47-04:00 k8s.kubelet[38157]: E0516 19:41:47.613145   38157 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded" containerID="66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d" cmd=["mongo","--port=37017","--tls","--tlsAllowInvalidHostnames","--tlsAllowInvalidCertificates","--tlsCertificateKeyFile=/var/lib/juju/server.pem","--eval","db.adminCommand('ping')"]
2025-05-16T19:41:48-04:00 k8s.kube-apiserver[37236]: I0516 19:41:48.995961   37236 alloc.go:330] "allocated clusterIPs" service="openstack/neutron" clusterIPs={"IPv4":"10.152.183.148"}
2025-05-16T19:41:51-04:00 k8s.kube-apiserver[37236]: I0516 19:41:51.764946   37236 alloc.go:330] "allocated clusterIPs" service="openstack/nova-mysql-router" clusterIPs={"IPv4":"10.152.183.197"}
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.496503436-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/3cs9bo6mym14ler2zuvzsnqcd8pvucf6cxvlw/traefik-image@sha256:af66fc2c3970bb9a7d1303df391a00b5b09876005343fb9963eaac03304c0a3e,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.513909059-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:d75a6f5b61d40b037ec81852120c0ea82bd96d1695ff1bca957259f04a552452,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.524448192-04:00" level=info msg="ImageUpdate event &ImageUpdate{Name:registry.jujucharms.com/charm/3cs9bo6mym14ler2zuvzsnqcd8pvucf6cxvlw/traefik-image@sha256:af66fc2c3970bb9a7d1303df391a00b5b09876005343fb9963eaac03304c0a3e,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.539389779-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/3cs9bo6mym14ler2zuvzsnqcd8pvucf6cxvlw/traefik-image@sha256:af66fc2c3970bb9a7d1303df391a00b5b09876005343fb9963eaac03304c0a3e\" returns image reference \"sha256:d75a6f5b61d40b037ec81852120c0ea82bd96d1695ff1bca957259f04a552452\""
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.544972240-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/3cs9bo6mym14ler2zuvzsnqcd8pvucf6cxvlw/traefik-image@sha256:af66fc2c3970bb9a7d1303df391a00b5b09876005343fb9963eaac03304c0a3e\" returns image reference \"sha256:d75a6f5b61d40b037ec81852120c0ea82bd96d1695ff1bca957259f04a552452\""
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.772098566-04:00" level=info msg="CreateContainer within sandbox \"4ee93c9afe20443c7c7a3f4e39802c14041532615de879c17e4c3140457cf53e\" for container &ContainerMetadata{Name:traefik,Attempt:0,}"
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.808668408-04:00" level=info msg="CreateContainer within sandbox \"147f9a110389e1baa444957ebc3aac74f2c022304c8ff54c8303792015342825\" for container &ContainerMetadata{Name:traefik,Attempt:0,}"
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.934093426-04:00" level=info msg="CreateContainer within sandbox \"4ee93c9afe20443c7c7a3f4e39802c14041532615de879c17e4c3140457cf53e\" for &ContainerMetadata{Name:traefik,Attempt:0,} returns container id \"75803645ec1ad66fb2ce8ac91f119fd5d138e5226bf41416641c1331e05f8fbd\""
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.940527076-04:00" level=info msg="StartContainer for \"75803645ec1ad66fb2ce8ac91f119fd5d138e5226bf41416641c1331e05f8fbd\""
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.984172961-04:00" level=info msg="CreateContainer within sandbox \"147f9a110389e1baa444957ebc3aac74f2c022304c8ff54c8303792015342825\" for &ContainerMetadata{Name:traefik,Attempt:0,} returns container id \"9790a79920cb34067f5855459fef09b7bd830812fa9e0735367169cf72f662a8\""
2025-05-16T19:41:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:52.988773283-04:00" level=info msg="StartContainer for \"9790a79920cb34067f5855459fef09b7bd830812fa9e0735367169cf72f662a8\""
2025-05-16T19:41:53-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:53.822682647-04:00" level=info msg="StartContainer for \"75803645ec1ad66fb2ce8ac91f119fd5d138e5226bf41416641c1331e05f8fbd\" returns successfully"
2025-05-16T19:41:54-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:54.005553187-04:00" level=info msg="StartContainer for \"9790a79920cb34067f5855459fef09b7bd830812fa9e0735367169cf72f662a8\" returns successfully"
2025-05-16T19:41:54-04:00 k8s.kubelet[38157]: I0516 19:41:54.488877   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/traefik-0" podStartSLOduration=9.79704556 podStartE2EDuration="1m18.488846121s" podCreationTimestamp="2025-05-16 19:40:36 -0400 EDT" firstStartedPulling="2025-05-16 19:40:43.862420375 -0400 EDT m=+437.000017055" lastFinishedPulling="2025-05-16 19:41:52.554220929 -0400 EDT m=+505.691817616" observedRunningTime="2025-05-16 19:41:54.445107185 -0400 EDT m=+507.582703905" watchObservedRunningTime="2025-05-16 19:41:54.488846121 -0400 EDT m=+507.626442812"
2025-05-16T19:41:54-04:00 k8s.kube-apiserver[37236]: I0516 19:41:54.531438   37236 alloc.go:330] "allocated clusterIPs" service="openstack/neutron-mysql-router" clusterIPs={"IPv4":"10.152.183.49"}
2025-05-16T19:41:54-04:00 k8s.kubelet[38157]: I0516 19:41:54.700089   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/traefik-public-0" podStartSLOduration=9.259903538 podStartE2EDuration="1m12.700055536s" podCreationTimestamp="2025-05-16 19:40:42 -0400 EDT" firstStartedPulling="2025-05-16 19:40:49.108975705 -0400 EDT m=+442.246572387" lastFinishedPulling="2025-05-16 19:41:52.549127695 -0400 EDT m=+505.686724385" observedRunningTime="2025-05-16 19:41:54.664995589 -0400 EDT m=+507.802592276" watchObservedRunningTime="2025-05-16 19:41:54.700055536 -0400 EDT m=+507.837652209"
2025-05-16T19:41:56-04:00 k8s.kube-apiserver[37236]: I0516 19:41:56.847956   37236 alloc.go:330] "allocated clusterIPs" service="openstack/nova-api-mysql-router" clusterIPs={"IPv4":"10.152.183.145"}
2025-05-16T19:41:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:41:57.540306398-04:00" level=error msg="ExecSync for \"66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d\" failed" error="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded"
2025-05-16T19:41:57-04:00 k8s.kubelet[38157]: E0516 19:41:57.542028   38157 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded" containerID="66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d" cmd=["mongo","--port=37017","--tls","--tlsAllowInvalidHostnames","--tlsAllowInvalidCertificates","--tlsCertificateKeyFile=/var/lib/juju/server.pem","--eval","db.adminCommand('ping')"]
2025-05-16T19:42:00-04:00 k8s.kube-apiserver[37236]: I0516 19:42:00.868499   37236 alloc.go:330] "allocated clusterIPs" service="openstack/cinder" clusterIPs={"IPv4":"10.152.183.210"}
2025-05-16T19:42:15-04:00 k8s.kube-apiserver[37236]: I0516 19:42:15.294725   37236 alloc.go:330] "allocated clusterIPs" service="openstack/glance" clusterIPs={"IPv4":"10.152.183.229"}
2025-05-16T19:42:16-04:00 k8s.kube-apiserver[37236]: I0516 19:42:16.375342   37236 alloc.go:330] "allocated clusterIPs" service="openstack/glance-mysql-router" clusterIPs={"IPv4":"10.152.183.233"}
2025-05-16T19:42:17-04:00 k8s.kube-apiserver[37236]: I0516 19:42:17.969208   37236 alloc.go:330] "allocated clusterIPs" service="openstack/nova-cell-mysql-router" clusterIPs={"IPv4":"10.152.183.151"}
2025-05-16T19:42:19-04:00 k8s.kubelet[38157]: I0516 19:42:19.428808   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/9abc2614-ddda-49dc-a030-991694c25e6e-charm-data\") pod \"neutron-0\" (UID: \"9abc2614-ddda-49dc-a030-991694c25e6e\") " pod="openstack/neutron-0"
2025-05-16T19:42:19-04:00 k8s.kubelet[38157]: I0516 19:42:19.428931   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qtjv6\" (UniqueName: \"kubernetes.io/projected/9abc2614-ddda-49dc-a030-991694c25e6e-kube-api-access-qtjv6\") pod \"neutron-0\" (UID: \"9abc2614-ddda-49dc-a030-991694c25e6e\") " pod="openstack/neutron-0"
2025-05-16T19:42:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:19.715192666-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:neutron-0,Uid:9abc2614-ddda-49dc-a030-991694c25e6e,Namespace:openstack,Attempt:0,}"
2025-05-16T19:42:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:20.749802742-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:42:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:20.753105421-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:42:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:20.753150723-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:42:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:20.756008407-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/a3400cfaf9db5145d6586531ed4b08dec08a70b2529629f27143d9f624fccce4 pid=59787 runtime=io.containerd.runc.v2
2025-05-16T19:42:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:21.644686988-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:neutron-0,Uid:9abc2614-ddda-49dc-a030-991694c25e6e,Namespace:openstack,Attempt:0,} returns sandbox id \"a3400cfaf9db5145d6586531ed4b08dec08a70b2529629f27143d9f624fccce4\""
2025-05-16T19:42:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:21.887155973-04:00" level=info msg="CreateContainer within sandbox \"a3400cfaf9db5145d6586531ed4b08dec08a70b2529629f27143d9f624fccce4\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:42:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:22.016229976-04:00" level=info msg="CreateContainer within sandbox \"a3400cfaf9db5145d6586531ed4b08dec08a70b2529629f27143d9f624fccce4\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"89d6089b9e961fb355492917b22c728c7acc45906f7227ce035f6b16bb55d98b\""
2025-05-16T19:42:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:22.019020301-04:00" level=info msg="StartContainer for \"89d6089b9e961fb355492917b22c728c7acc45906f7227ce035f6b16bb55d98b\""
2025-05-16T19:42:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:22.623778707-04:00" level=info msg="StartContainer for \"89d6089b9e961fb355492917b22c728c7acc45906f7227ce035f6b16bb55d98b\" returns successfully"
2025-05-16T19:42:25-04:00 k8s.kubelet[38157]: I0516 19:42:25.753706   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/bc6230b0-45ba-4e2d-bd8b-017e7eb8ed0b-charm-data\") pod \"nova-mysql-router-0\" (UID: \"bc6230b0-45ba-4e2d-bd8b-017e7eb8ed0b\") " pod="openstack/nova-mysql-router-0"
2025-05-16T19:42:25-04:00 k8s.kubelet[38157]: I0516 19:42:25.753798   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b5jzq\" (UniqueName: \"kubernetes.io/projected/bc6230b0-45ba-4e2d-bd8b-017e7eb8ed0b-kube-api-access-b5jzq\") pod \"nova-mysql-router-0\" (UID: \"bc6230b0-45ba-4e2d-bd8b-017e7eb8ed0b\") " pod="openstack/nova-mysql-router-0"
2025-05-16T19:42:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:26.234821348-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nova-mysql-router-0,Uid:bc6230b0-45ba-4e2d-bd8b-017e7eb8ed0b,Namespace:openstack,Attempt:0,}"
2025-05-16T19:42:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:27.446014317-04:00" level=info msg="shim disconnected" id=89d6089b9e961fb355492917b22c728c7acc45906f7227ce035f6b16bb55d98b
2025-05-16T19:42:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:27.446113261-04:00" level=warning msg="cleaning up after shim disconnected" id=89d6089b9e961fb355492917b22c728c7acc45906f7227ce035f6b16bb55d98b namespace=k8s.io
2025-05-16T19:42:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:27.446135793-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:42:27-04:00 k8s.kube-apiserver[37236]: I0516 19:42:27.452563   37236 alloc.go:330] "allocated clusterIPs" service="openstack/cinder-mysql-router" clusterIPs={"IPv4":"10.152.183.58"}
2025-05-16T19:42:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:27.602924779-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:42:27-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=60113 runtime=io.containerd.runc.v2\n"
2025-05-16T19:42:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:27.634600050-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:42:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:27.634781368-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:42:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:27.634859399-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:42:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:27.637909294-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/a4cf6c9b9a9935e3014ad3415f32dd5c83282296ad1576652a11018f7745e31e pid=60149 runtime=io.containerd.runc.v2
2025-05-16T19:42:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:28.503067576-04:00" level=info msg="CreateContainer within sandbox \"a3400cfaf9db5145d6586531ed4b08dec08a70b2529629f27143d9f624fccce4\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:42:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:28.616656721-04:00" level=info msg="CreateContainer within sandbox \"a3400cfaf9db5145d6586531ed4b08dec08a70b2529629f27143d9f624fccce4\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"18311cb1a0b97e546217ca0298de39d8a2b826542f5bcec82bb4f1a8def76f8e\""
2025-05-16T19:42:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:28.626197531-04:00" level=info msg="StartContainer for \"18311cb1a0b97e546217ca0298de39d8a2b826542f5bcec82bb4f1a8def76f8e\""
2025-05-16T19:42:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:29.072491963-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nova-mysql-router-0,Uid:bc6230b0-45ba-4e2d-bd8b-017e7eb8ed0b,Namespace:openstack,Attempt:0,} returns sandbox id \"a4cf6c9b9a9935e3014ad3415f32dd5c83282296ad1576652a11018f7745e31e\""
2025-05-16T19:42:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:29.564354696-04:00" level=info msg="CreateContainer within sandbox \"a4cf6c9b9a9935e3014ad3415f32dd5c83282296ad1576652a11018f7745e31e\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:42:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:29.764911101-04:00" level=info msg="CreateContainer within sandbox \"a4cf6c9b9a9935e3014ad3415f32dd5c83282296ad1576652a11018f7745e31e\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"f879b1c7322f5fe4d9e459ea0fb76ae33d96f9ad2d7d4b018a1d126b285c530b\""
2025-05-16T19:42:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:29.773120713-04:00" level=info msg="StartContainer for \"f879b1c7322f5fe4d9e459ea0fb76ae33d96f9ad2d7d4b018a1d126b285c530b\""
2025-05-16T19:42:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:29.953518337-04:00" level=info msg="StartContainer for \"18311cb1a0b97e546217ca0298de39d8a2b826542f5bcec82bb4f1a8def76f8e\" returns successfully"
2025-05-16T19:42:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:29.965677037-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/hyn7tmdxrcl650kyzozombx0re0xtmqs378xi/neutron-server-image@sha256:e921bf3dc992f4ec21085eaa06a3e16fa33ddd86d0352f6b0e493b0769db2605\""
2025-05-16T19:42:31-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:31.027805523-04:00" level=info msg="StartContainer for \"f879b1c7322f5fe4d9e459ea0fb76ae33d96f9ad2d7d4b018a1d126b285c530b\" returns successfully"
2025-05-16T19:42:32-04:00 k8s.kube-apiserver[37236]: I0516 19:42:32.237061   37236 alloc.go:330] "allocated clusterIPs" service="openstack/keystone-mysql-router" clusterIPs={"IPv4":"10.152.183.74"}
2025-05-16T19:42:32-04:00 k8s.kubelet[38157]: I0516 19:42:32.644328   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sp29w\" (UniqueName: \"kubernetes.io/projected/4f78946b-6907-41fe-8321-f8b5e73d9ad1-kube-api-access-sp29w\") pod \"neutron-mysql-router-0\" (UID: \"4f78946b-6907-41fe-8321-f8b5e73d9ad1\") " pod="openstack/neutron-mysql-router-0"
2025-05-16T19:42:32-04:00 k8s.kubelet[38157]: I0516 19:42:32.644447   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/4f78946b-6907-41fe-8321-f8b5e73d9ad1-charm-data\") pod \"neutron-mysql-router-0\" (UID: \"4f78946b-6907-41fe-8321-f8b5e73d9ad1\") " pod="openstack/neutron-mysql-router-0"
2025-05-16T19:42:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:32.967696449-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:neutron-mysql-router-0,Uid:4f78946b-6907-41fe-8321-f8b5e73d9ad1,Namespace:openstack,Attempt:0,}"
2025-05-16T19:42:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:34.150702114-04:00" level=info msg="shim disconnected" id=f879b1c7322f5fe4d9e459ea0fb76ae33d96f9ad2d7d4b018a1d126b285c530b
2025-05-16T19:42:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:34.150830395-04:00" level=warning msg="cleaning up after shim disconnected" id=f879b1c7322f5fe4d9e459ea0fb76ae33d96f9ad2d7d4b018a1d126b285c530b namespace=k8s.io
2025-05-16T19:42:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:34.150858784-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:42:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:34.135632633-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:42:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:34.135812124-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:42:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:34.135839215-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:42:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:34.137761336-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/2d7f0e01498193ae55c81fb52d8a7e64b550a2d61513a7247e76d6b474156d8b pid=60765 runtime=io.containerd.runc.v2
2025-05-16T19:42:34-04:00 k8s.kube-apiserver[37236]: I0516 19:42:34.196851   37236 alloc.go:330] "allocated clusterIPs" service="openstack/nova" clusterIPs={"IPv4":"10.152.183.234"}
2025-05-16T19:42:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:34.521832729-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:42:34-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=60791 runtime=io.containerd.runc.v2\n"
2025-05-16T19:42:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:35.936100960-04:00" level=info msg="CreateContainer within sandbox \"a4cf6c9b9a9935e3014ad3415f32dd5c83282296ad1576652a11018f7745e31e\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:42:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:35.965266318-04:00" level=info msg="CreateContainer within sandbox \"a4cf6c9b9a9935e3014ad3415f32dd5c83282296ad1576652a11018f7745e31e\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"00042c5082720c312789eac24f29af85cec0899fc64edbe89a01cbb6b7287d42\""
2025-05-16T19:42:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:35.979603026-04:00" level=info msg="StartContainer for \"00042c5082720c312789eac24f29af85cec0899fc64edbe89a01cbb6b7287d42\""
2025-05-16T19:42:36-04:00 k8s.kubelet[38157]: E0516 19:42:36.270115   38157 cadvisor_stats_provider.go:522] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod4f78946b_6907_41fe_8321_f8b5e73d9ad1.slice/cri-containerd-2d7f0e01498193ae55c81fb52d8a7e64b550a2d61513a7247e76d6b474156d8b.scope\": RecentStats: unable to find data in memory cache]"
2025-05-16T19:42:36-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:36.335826584-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:neutron-mysql-router-0,Uid:4f78946b-6907-41fe-8321-f8b5e73d9ad1,Namespace:openstack,Attempt:0,} returns sandbox id \"2d7f0e01498193ae55c81fb52d8a7e64b550a2d61513a7247e76d6b474156d8b\""
2025-05-16T19:42:36-04:00 k8s.kube-apiserver[37236]: I0516 19:42:36.611322   37236 alloc.go:330] "allocated clusterIPs" service="openstack/horizon-mysql-router" clusterIPs={"IPv4":"10.152.183.143"}
2025-05-16T19:42:36-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:36.865162934-04:00" level=info msg="CreateContainer within sandbox \"2d7f0e01498193ae55c81fb52d8a7e64b550a2d61513a7247e76d6b474156d8b\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:42:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:37.042645935-04:00" level=info msg="CreateContainer within sandbox \"2d7f0e01498193ae55c81fb52d8a7e64b550a2d61513a7247e76d6b474156d8b\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"cc426a0d0bb1fcb69310623984a36d4731c0627b83d92fde338e73648a4e5e66\""
2025-05-16T19:42:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:37.048381724-04:00" level=info msg="StartContainer for \"cc426a0d0bb1fcb69310623984a36d4731c0627b83d92fde338e73648a4e5e66\""
2025-05-16T19:42:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:37.362924585-04:00" level=info msg="StartContainer for \"00042c5082720c312789eac24f29af85cec0899fc64edbe89a01cbb6b7287d42\" returns successfully"
2025-05-16T19:42:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:37.435260567-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/g78qli3013qicvevb3oj4z8u0zhjod1agws2d/mysql-router-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9\""
2025-05-16T19:42:38-04:00 k8s.kubelet[38157]: I0516 19:42:38.087953   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lvr7l\" (UniqueName: \"kubernetes.io/projected/c30b40ad-4026-4c33-8251-7eeea63ed6e1-kube-api-access-lvr7l\") pod \"nova-api-mysql-router-0\" (UID: \"c30b40ad-4026-4c33-8251-7eeea63ed6e1\") " pod="openstack/nova-api-mysql-router-0"
2025-05-16T19:42:38-04:00 k8s.kubelet[38157]: I0516 19:42:38.088039   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/c30b40ad-4026-4c33-8251-7eeea63ed6e1-charm-data\") pod \"nova-api-mysql-router-0\" (UID: \"c30b40ad-4026-4c33-8251-7eeea63ed6e1\") " pod="openstack/nova-api-mysql-router-0"
2025-05-16T19:42:38-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:38.096428471-04:00" level=info msg="StartContainer for \"cc426a0d0bb1fcb69310623984a36d4731c0627b83d92fde338e73648a4e5e66\" returns successfully"
2025-05-16T19:42:38-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:38.342096430-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nova-api-mysql-router-0,Uid:c30b40ad-4026-4c33-8251-7eeea63ed6e1,Namespace:openstack,Attempt:0,}"
2025-05-16T19:42:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:39.864910521-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:42:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:39.865196055-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:42:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:39.865245612-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:42:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:39.866067172-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/c3ce9b7bba5dba8e23247d9bd2343813cd468c97434f7fd177d4a92203252591 pid=61208 runtime=io.containerd.runc.v2
2025-05-16T19:42:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:41.270903462-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nova-api-mysql-router-0,Uid:c30b40ad-4026-4c33-8251-7eeea63ed6e1,Namespace:openstack,Attempt:0,} returns sandbox id \"c3ce9b7bba5dba8e23247d9bd2343813cd468c97434f7fd177d4a92203252591\""
2025-05-16T19:42:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:41.685929040-04:00" level=info msg="CreateContainer within sandbox \"c3ce9b7bba5dba8e23247d9bd2343813cd468c97434f7fd177d4a92203252591\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:42:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:41.950101963-04:00" level=info msg="CreateContainer within sandbox \"c3ce9b7bba5dba8e23247d9bd2343813cd468c97434f7fd177d4a92203252591\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"09e944874ade3553fb607004ef8d72f3e108bf43b19ff5a1b531a368b4ca52d1\""
2025-05-16T19:42:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:41.965803555-04:00" level=info msg="StartContainer for \"09e944874ade3553fb607004ef8d72f3e108bf43b19ff5a1b531a368b4ca52d1\""
2025-05-16T19:42:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:42.902405268-04:00" level=info msg="StartContainer for \"09e944874ade3553fb607004ef8d72f3e108bf43b19ff5a1b531a368b4ca52d1\" returns successfully"
2025-05-16T19:42:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:42.911877130-04:00" level=info msg="shim disconnected" id=cc426a0d0bb1fcb69310623984a36d4731c0627b83d92fde338e73648a4e5e66
2025-05-16T19:42:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:42.911972824-04:00" level=warning msg="cleaning up after shim disconnected" id=cc426a0d0bb1fcb69310623984a36d4731c0627b83d92fde338e73648a4e5e66 namespace=k8s.io
2025-05-16T19:42:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:42.911997037-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:42:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:43.009274913-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:42:42-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=61473 runtime=io.containerd.runc.v2\n"
2025-05-16T19:42:43-04:00 k8s.kube-apiserver[37236]: I0516 19:42:43.775146   37236 alloc.go:330] "allocated clusterIPs" service="openstack/placement" clusterIPs={"IPv4":"10.152.183.137"}
2025-05-16T19:42:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:44.579169772-04:00" level=info msg="CreateContainer within sandbox \"2d7f0e01498193ae55c81fb52d8a7e64b550a2d61513a7247e76d6b474156d8b\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:42:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:44.730632476-04:00" level=info msg="CreateContainer within sandbox \"2d7f0e01498193ae55c81fb52d8a7e64b550a2d61513a7247e76d6b474156d8b\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"425ac3622b25f3966476dcdd4bfa29683122744def4d59a3a529030e7eb98b3f\""
2025-05-16T19:42:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:44.731805942-04:00" level=info msg="StartContainer for \"425ac3622b25f3966476dcdd4bfa29683122744def4d59a3a529030e7eb98b3f\""
2025-05-16T19:42:45-04:00 k8s.kube-apiserver[37236]: I0516 19:42:45.850995   37236 alloc.go:330] "allocated clusterIPs" service="openstack/placement-mysql-router" clusterIPs={"IPv4":"10.152.183.173"}
2025-05-16T19:42:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:46.095869960-04:00" level=info msg="StartContainer for \"425ac3622b25f3966476dcdd4bfa29683122744def4d59a3a529030e7eb98b3f\" returns successfully"
2025-05-16T19:42:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:46.110965977-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/g78qli3013qicvevb3oj4z8u0zhjod1agws2d/mysql-router-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9\""
2025-05-16T19:42:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:46.655447335-04:00" level=info msg="shim disconnected" id=09e944874ade3553fb607004ef8d72f3e108bf43b19ff5a1b531a368b4ca52d1
2025-05-16T19:42:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:46.658658746-04:00" level=warning msg="cleaning up after shim disconnected" id=09e944874ade3553fb607004ef8d72f3e108bf43b19ff5a1b531a368b4ca52d1 namespace=k8s.io
2025-05-16T19:42:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:46.659533202-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:42:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:46.723769445-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:42:46-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=61814 runtime=io.containerd.runc.v2\ntime=\"2025-05-16T19:42:46-04:00\" level=warning msg=\"failed to remove runc container\" error=\"runc did not terminate successfully: exit status 255: \" runtime=io.containerd.runc.v2\n"
2025-05-16T19:42:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:47.579319782-04:00" level=info msg="CreateContainer within sandbox \"c3ce9b7bba5dba8e23247d9bd2343813cd468c97434f7fd177d4a92203252591\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:42:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:47.667344664-04:00" level=info msg="CreateContainer within sandbox \"c3ce9b7bba5dba8e23247d9bd2343813cd468c97434f7fd177d4a92203252591\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"009d571c56b834a37f504563a784b31dc6a7da6e06b0815d55d5a8693302bf52\""
2025-05-16T19:42:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:47.674761258-04:00" level=info msg="StartContainer for \"009d571c56b834a37f504563a784b31dc6a7da6e06b0815d55d5a8693302bf52\""
2025-05-16T19:42:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:48.981956091-04:00" level=info msg="StartContainer for \"009d571c56b834a37f504563a784b31dc6a7da6e06b0815d55d5a8693302bf52\" returns successfully"
2025-05-16T19:42:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:48.992826305-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/g78qli3013qicvevb3oj4z8u0zhjod1agws2d/mysql-router-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9\""
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.381351800-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/62ptdfbrjpw3n9tcnswjpart30jauc6wc5wbi/mysql-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.394415109-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/g78qli3013qicvevb3oj4z8u0zhjod1agws2d/mysql-router-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.399568439-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:c83c3a912fbe97b4eaec22a50c4681e07f883d4ad9aae8a70ef6f5c4fef32ae5,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.409746971-04:00" level=info msg="ImageUpdate event &ImageUpdate{Name:registry.jujucharms.com/charm/g78qli3013qicvevb3oj4z8u0zhjod1agws2d/mysql-router-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.420511761-04:00" level=info msg="ImageUpdate event &ImageUpdate{Name:registry.jujucharms.com/charm/g78qli3013qicvevb3oj4z8u0zhjod1agws2d/mysql-router-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.432393679-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/g78qli3013qicvevb3oj4z8u0zhjod1agws2d/mysql-router-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9\" returns image reference \"sha256:c83c3a912fbe97b4eaec22a50c4681e07f883d4ad9aae8a70ef6f5c4fef32ae5\""
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.444633791-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/62ptdfbrjpw3n9tcnswjpart30jauc6wc5wbi/mysql-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9\" returns image reference \"sha256:c83c3a912fbe97b4eaec22a50c4681e07f883d4ad9aae8a70ef6f5c4fef32ae5\""
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.523071090-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/g78qli3013qicvevb3oj4z8u0zhjod1agws2d/mysql-router-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9\" returns image reference \"sha256:c83c3a912fbe97b4eaec22a50c4681e07f883d4ad9aae8a70ef6f5c4fef32ae5\""
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.527742694-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/g78qli3013qicvevb3oj4z8u0zhjod1agws2d/mysql-router-image@sha256:089fc04dd2d6f1559161ddf4720c1e06559aeb731ecae57b050c9c816e9833e9\" returns image reference \"sha256:c83c3a912fbe97b4eaec22a50c4681e07f883d4ad9aae8a70ef6f5c4fef32ae5\""
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.599907814-04:00" level=info msg="CreateContainer within sandbox \"a4cf6c9b9a9935e3014ad3415f32dd5c83282296ad1576652a11018f7745e31e\" for container &ContainerMetadata{Name:mysql-router,Attempt:0,}"
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.757140100-04:00" level=info msg="CreateContainer within sandbox \"a4cf6c9b9a9935e3014ad3415f32dd5c83282296ad1576652a11018f7745e31e\" for &ContainerMetadata{Name:mysql-router,Attempt:0,} returns container id \"840612eb6d522f0c4e55ef4509db7a9d015ec45a29ff90193a76ed7548319741\""
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.767601729-04:00" level=info msg="StartContainer for \"840612eb6d522f0c4e55ef4509db7a9d015ec45a29ff90193a76ed7548319741\""
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.838087608-04:00" level=info msg="CreateContainer within sandbox \"5f95dcf1eda955294c05e70845e9c92654d97069740f9bda05715910b222c08d\" for container &ContainerMetadata{Name:mysql,Attempt:0,}"
2025-05-16T19:42:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:51.980852486-04:00" level=info msg="CreateContainer within sandbox \"c3ce9b7bba5dba8e23247d9bd2343813cd468c97434f7fd177d4a92203252591\" for container &ContainerMetadata{Name:mysql-router,Attempt:0,}"
2025-05-16T19:42:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:52.022221985-04:00" level=info msg="CreateContainer within sandbox \"2d7f0e01498193ae55c81fb52d8a7e64b550a2d61513a7247e76d6b474156d8b\" for container &ContainerMetadata{Name:mysql-router,Attempt:0,}"
2025-05-16T19:42:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:52.033970246-04:00" level=info msg="CreateContainer within sandbox \"5f95dcf1eda955294c05e70845e9c92654d97069740f9bda05715910b222c08d\" for &ContainerMetadata{Name:mysql,Attempt:0,} returns container id \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\""
2025-05-16T19:42:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:52.046334371-04:00" level=info msg="StartContainer for \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\""
2025-05-16T19:42:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:52.229312397-04:00" level=info msg="CreateContainer within sandbox \"c3ce9b7bba5dba8e23247d9bd2343813cd468c97434f7fd177d4a92203252591\" for &ContainerMetadata{Name:mysql-router,Attempt:0,} returns container id \"7d953a1a8c04ffe3d77686bc0b0d82436a7639f3f4e5ae996e892cc6e5d17ba1\""
2025-05-16T19:42:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:52.254478108-04:00" level=info msg="StartContainer for \"7d953a1a8c04ffe3d77686bc0b0d82436a7639f3f4e5ae996e892cc6e5d17ba1\""
2025-05-16T19:42:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:52.339347148-04:00" level=info msg="CreateContainer within sandbox \"2d7f0e01498193ae55c81fb52d8a7e64b550a2d61513a7247e76d6b474156d8b\" for &ContainerMetadata{Name:mysql-router,Attempt:0,} returns container id \"4a3c1b1ade11b4a5ef9e1d5ffaf418762157a565f53291da0f5a22ac188390e5\""
2025-05-16T19:42:52-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:52.367720606-04:00" level=info msg="StartContainer for \"4a3c1b1ade11b4a5ef9e1d5ffaf418762157a565f53291da0f5a22ac188390e5\""
2025-05-16T19:42:53-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:53.402839242-04:00" level=info msg="StartContainer for \"840612eb6d522f0c4e55ef4509db7a9d015ec45a29ff90193a76ed7548319741\" returns successfully"
2025-05-16T19:42:53-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:53.649660496-04:00" level=info msg="StartContainer for \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\" returns successfully"
2025-05-16T19:42:53-04:00 k8s.kubelet[38157]: I0516 19:42:53.886313   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/nova-mysql-router-0" podStartSLOduration=14.861445225 podStartE2EDuration="28.886286388s" podCreationTimestamp="2025-05-16 19:42:25 -0400 EDT" firstStartedPulling="2025-05-16 19:42:37.419987254 -0400 EDT m=+550.557583959" lastFinishedPulling="2025-05-16 19:42:51.444828439 -0400 EDT m=+564.582425122" observedRunningTime="2025-05-16 19:42:53.862400604 -0400 EDT m=+566.999997293" watchObservedRunningTime="2025-05-16 19:42:53.886286388 -0400 EDT m=+567.023883079"
2025-05-16T19:42:54-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:54.077385352-04:00" level=info msg="StartContainer for \"4a3c1b1ade11b4a5ef9e1d5ffaf418762157a565f53291da0f5a22ac188390e5\" returns successfully"
2025-05-16T19:42:54-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:54.375744525-04:00" level=info msg="StartContainer for \"7d953a1a8c04ffe3d77686bc0b0d82436a7639f3f4e5ae996e892cc6e5d17ba1\" returns successfully"
2025-05-16T19:42:54-04:00 k8s.kubelet[38157]: I0516 19:42:54.918345   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/nova-api-mysql-router-0" podStartSLOduration=15.377296446 podStartE2EDuration="17.918320937s" podCreationTimestamp="2025-05-16 19:42:37 -0400 EDT" firstStartedPulling="2025-05-16 19:42:48.991378534 -0400 EDT m=+562.128975217" lastFinishedPulling="2025-05-16 19:42:51.532403026 -0400 EDT m=+564.669999708" observedRunningTime="2025-05-16 19:42:54.820757726 -0400 EDT m=+567.958354429" watchObservedRunningTime="2025-05-16 19:42:54.918320937 -0400 EDT m=+568.055917611"
2025-05-16T19:42:54-04:00 k8s.kubelet[38157]: I0516 19:42:54.925338   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/mysql-0" podStartSLOduration=11.755477468 podStartE2EDuration="2m5.925311641s" podCreationTimestamp="2025-05-16 19:40:49 -0400 EDT" firstStartedPulling="2025-05-16 19:40:57.30476821 -0400 EDT m=+450.442364893" lastFinishedPulling="2025-05-16 19:42:51.474602383 -0400 EDT m=+564.612199066" observedRunningTime="2025-05-16 19:42:54.916924491 -0400 EDT m=+568.054521178" watchObservedRunningTime="2025-05-16 19:42:54.925311641 -0400 EDT m=+568.062908315"
2025-05-16T19:42:55-04:00 k8s.kubelet[38157]: I0516 19:42:55.391843   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/neutron-mysql-router-0" podStartSLOduration=17.95401482 podStartE2EDuration="23.391812255s" podCreationTimestamp="2025-05-16 19:42:32 -0400 EDT" firstStartedPulling="2025-05-16 19:42:46.10503959 -0400 EDT m=+559.242636261" lastFinishedPulling="2025-05-16 19:42:51.542837013 -0400 EDT m=+564.680433696" observedRunningTime="2025-05-16 19:42:55.331381586 -0400 EDT m=+568.468978271" watchObservedRunningTime="2025-05-16 19:42:55.391812255 -0400 EDT m=+568.529408946"
2025-05-16T19:42:56-04:00 k8s.kube-apiserver[37236]: I0516 19:42:56.437771   37236 alloc.go:330] "allocated clusterIPs" service="openstack/keystone" clusterIPs={"IPv4":"10.152.183.132"}
2025-05-16T19:42:56-04:00 k8s.kubelet[38157]: I0516 19:42:56.605270   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/a77e5ac2-1ca0-46b0-ac8d-e5940d37bbd0-charm-data\") pod \"cinder-0\" (UID: \"a77e5ac2-1ca0-46b0-ac8d-e5940d37bbd0\") " pod="openstack/cinder-0"
2025-05-16T19:42:56-04:00 k8s.kubelet[38157]: I0516 19:42:56.605393   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-c2txh\" (UniqueName: \"kubernetes.io/projected/a77e5ac2-1ca0-46b0-ac8d-e5940d37bbd0-kube-api-access-c2txh\") pod \"cinder-0\" (UID: \"a77e5ac2-1ca0-46b0-ac8d-e5940d37bbd0\") " pod="openstack/cinder-0"
2025-05-16T19:42:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:57.190762836-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cinder-0,Uid:a77e5ac2-1ca0-46b0-ac8d-e5940d37bbd0,Namespace:openstack,Attempt:0,}"
2025-05-16T19:42:58-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:58.500048657-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:42:58-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:58.500409160-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:42:58-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:58.500443447-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:42:58-04:00 k8s.containerd[36097]: time="2025-05-16T19:42:58.500866159-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429 pid=62644 runtime=io.containerd.runc.v2
2025-05-16T19:43:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:00.038548188-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cinder-0,Uid:a77e5ac2-1ca0-46b0-ac8d-e5940d37bbd0,Namespace:openstack,Attempt:0,} returns sandbox id \"5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429\""
2025-05-16T19:43:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:00.527967281-04:00" level=info msg="CreateContainer within sandbox \"5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:43:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:00.655551130-04:00" level=info msg="CreateContainer within sandbox \"5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"b2bd1aeff98aec7c58bea94bbff80f94257c554007449cf4a708e5acdbf06536\""
2025-05-16T19:43:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:00.667156811-04:00" level=info msg="StartContainer for \"b2bd1aeff98aec7c58bea94bbff80f94257c554007449cf4a708e5acdbf06536\""
2025-05-16T19:43:01-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:01.849374818-04:00" level=info msg="StartContainer for \"b2bd1aeff98aec7c58bea94bbff80f94257c554007449cf4a708e5acdbf06536\" returns successfully"
2025-05-16T19:43:03-04:00 k8s.kube-apiserver[37236]: I0516 19:43:03.776716   37236 alloc.go:330] "allocated clusterIPs" service="openstack/horizon" clusterIPs={"IPv4":"10.152.183.142"}
2025-05-16T19:43:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:05.429978534-04:00" level=info msg="shim disconnected" id=b2bd1aeff98aec7c58bea94bbff80f94257c554007449cf4a708e5acdbf06536
2025-05-16T19:43:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:05.430102652-04:00" level=warning msg="cleaning up after shim disconnected" id=b2bd1aeff98aec7c58bea94bbff80f94257c554007449cf4a708e5acdbf06536 namespace=k8s.io
2025-05-16T19:43:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:05.430129855-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:43:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:05.583808925-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:43:05-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=63206 runtime=io.containerd.runc.v2\n"
2025-05-16T19:43:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:07.298951480-04:00" level=info msg="CreateContainer within sandbox \"5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:43:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:07.553292091-04:00" level=info msg="CreateContainer within sandbox \"5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"aa95c6c2bba1a4e582a56dbbb0ae12e0c66690be02f48a7405899c427480c1cc\""
2025-05-16T19:43:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:07.554496690-04:00" level=info msg="StartContainer for \"aa95c6c2bba1a4e582a56dbbb0ae12e0c66690be02f48a7405899c427480c1cc\""
2025-05-16T19:43:07-04:00 k8s.kubelet[38157]: I0516 19:43:07.847659   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/ece72cab-8356-4a2e-959e-dc3cd63858d9-charm-data\") pod \"glance-0\" (UID: \"ece72cab-8356-4a2e-959e-dc3cd63858d9\") " pod="openstack/glance-0"
2025-05-16T19:43:07-04:00 k8s.kubelet[38157]: I0516 19:43:07.847749   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tpdbb\" (UniqueName: \"kubernetes.io/projected/ece72cab-8356-4a2e-959e-dc3cd63858d9-kube-api-access-tpdbb\") pod \"glance-0\" (UID: \"ece72cab-8356-4a2e-959e-dc3cd63858d9\") " pod="openstack/glance-0"
2025-05-16T19:43:07-04:00 k8s.kubelet[38157]: I0516 19:43:07.847816   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-89ac7384-54c8-41da-a0eb-83af78f85047\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-89ac7384-54c8-41da-a0eb-83af78f85047\") pod \"glance-0\" (UID: \"ece72cab-8356-4a2e-959e-dc3cd63858d9\") " pod="openstack/glance-0"
2025-05-16T19:43:08-04:00 k8s.kubelet[38157]: I0516 19:43:08.293905   38157 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"pvc-89ac7384-54c8-41da-a0eb-83af78f85047\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-89ac7384-54c8-41da-a0eb-83af78f85047\") pod \"glance-0\" (UID: \"ece72cab-8356-4a2e-959e-dc3cd63858d9\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/rawfile.csi.openebs.io/49e05df2e1703f8edce471128b72dd8265f23232ebeae70c6d16e63558363963/globalmount\"" pod="openstack/glance-0"
2025-05-16T19:43:08-04:00 k8s.kubelet[38157]: I0516 19:43:08.837883   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/42a0e6ce-20da-407d-9055-f1e40fc28e29-charm-data\") pod \"glance-mysql-router-0\" (UID: \"42a0e6ce-20da-407d-9055-f1e40fc28e29\") " pod="openstack/glance-mysql-router-0"
2025-05-16T19:43:08-04:00 k8s.kubelet[38157]: I0516 19:43:08.838005   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-k6n5s\" (UniqueName: \"kubernetes.io/projected/42a0e6ce-20da-407d-9055-f1e40fc28e29-kube-api-access-k6n5s\") pod \"glance-mysql-router-0\" (UID: \"42a0e6ce-20da-407d-9055-f1e40fc28e29\") " pod="openstack/glance-mysql-router-0"
2025-05-16T19:43:08-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:08.856800256-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:glance-0,Uid:ece72cab-8356-4a2e-959e-dc3cd63858d9,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:09.305728195-04:00" level=info msg="StartContainer for \"aa95c6c2bba1a4e582a56dbbb0ae12e0c66690be02f48a7405899c427480c1cc\" returns successfully"
2025-05-16T19:43:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:09.316212239-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-api-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\""
2025-05-16T19:43:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:09.439479701-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:glance-mysql-router-0,Uid:42a0e6ce-20da-407d-9055-f1e40fc28e29,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:10.635834492-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:43:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:10.636016144-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:43:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:10.636052228-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:43:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:10.638246136-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/20a108e18861d6591906dfb50c9a837645face6ea7713c7804d5d538ad2be428 pid=63722 runtime=io.containerd.runc.v2
2025-05-16T19:43:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:10.925200074-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:43:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:10.930906375-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:43:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:10.931096691-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:43:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:10.950795522-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/b3cd5efd2448999b13b0f8e3e28f7bf6815d2d8ae45f9016a58e5922e7d5f6cd pid=63754 runtime=io.containerd.runc.v2
2025-05-16T19:43:11-04:00 k8s.kubelet[38157]: I0516 19:43:11.822547   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/2c5039ac-3636-41a3-978e-5225ff4c3882-charm-data\") pod \"nova-cell-mysql-router-0\" (UID: \"2c5039ac-3636-41a3-978e-5225ff4c3882\") " pod="openstack/nova-cell-mysql-router-0"
2025-05-16T19:43:11-04:00 k8s.kubelet[38157]: I0516 19:43:11.826520   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5gwtd\" (UniqueName: \"kubernetes.io/projected/2c5039ac-3636-41a3-978e-5225ff4c3882-kube-api-access-5gwtd\") pod \"nova-cell-mysql-router-0\" (UID: \"2c5039ac-3636-41a3-978e-5225ff4c3882\") " pod="openstack/nova-cell-mysql-router-0"
2025-05-16T19:43:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:12.468733688-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nova-cell-mysql-router-0,Uid:2c5039ac-3636-41a3-978e-5225ff4c3882,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:13.056259058-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:glance-0,Uid:ece72cab-8356-4a2e-959e-dc3cd63858d9,Namespace:openstack,Attempt:0,} returns sandbox id \"20a108e18861d6591906dfb50c9a837645face6ea7713c7804d5d538ad2be428\""
2025-05-16T19:43:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:13.693072063-04:00" level=info msg="CreateContainer within sandbox \"20a108e18861d6591906dfb50c9a837645face6ea7713c7804d5d538ad2be428\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:43:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:13.895967807-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:43:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:13.906812099-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:43:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:13.906904742-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:43:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:13.913746910-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/08e70b136586805efcda98a1d224eb86727d68e46aa58666ea618cd3dda558ce pid=63879 runtime=io.containerd.runc.v2
2025-05-16T19:43:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:13.973032151-04:00" level=info msg="CreateContainer within sandbox \"20a108e18861d6591906dfb50c9a837645face6ea7713c7804d5d538ad2be428\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"7f0aaeb0a8332a6bff865ffef953f6bdffa64f1982f55d5413fa774ede7e4d7b\""
2025-05-16T19:43:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:13.982412237-04:00" level=info msg="StartContainer for \"7f0aaeb0a8332a6bff865ffef953f6bdffa64f1982f55d5413fa774ede7e4d7b\""
2025-05-16T19:43:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:14.416001782-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:glance-mysql-router-0,Uid:42a0e6ce-20da-407d-9055-f1e40fc28e29,Namespace:openstack,Attempt:0,} returns sandbox id \"b3cd5efd2448999b13b0f8e3e28f7bf6815d2d8ae45f9016a58e5922e7d5f6cd\""
2025-05-16T19:43:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:14.775691677-04:00" level=info msg="CreateContainer within sandbox \"b3cd5efd2448999b13b0f8e3e28f7bf6815d2d8ae45f9016a58e5922e7d5f6cd\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:43:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:15.059388328-04:00" level=info msg="CreateContainer within sandbox \"b3cd5efd2448999b13b0f8e3e28f7bf6815d2d8ae45f9016a58e5922e7d5f6cd\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"d8d0f5f77c0ba834eca70f42a883f8bd492d6ad136637621ba6b2ae40f204d2d\""
2025-05-16T19:43:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:15.070445803-04:00" level=info msg="StartContainer for \"d8d0f5f77c0ba834eca70f42a883f8bd492d6ad136637621ba6b2ae40f204d2d\""
2025-05-16T19:43:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:15.780120438-04:00" level=info msg="StartContainer for \"7f0aaeb0a8332a6bff865ffef953f6bdffa64f1982f55d5413fa774ede7e4d7b\" returns successfully"
2025-05-16T19:43:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:15.898903115-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nova-cell-mysql-router-0,Uid:2c5039ac-3636-41a3-978e-5225ff4c3882,Namespace:openstack,Attempt:0,} returns sandbox id \"08e70b136586805efcda98a1d224eb86727d68e46aa58666ea618cd3dda558ce\""
2025-05-16T19:43:16-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:16.355644846-04:00" level=info msg="CreateContainer within sandbox \"08e70b136586805efcda98a1d224eb86727d68e46aa58666ea618cd3dda558ce\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:43:16-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:16.618404254-04:00" level=info msg="CreateContainer within sandbox \"08e70b136586805efcda98a1d224eb86727d68e46aa58666ea618cd3dda558ce\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"cade0436e1a63ddf06237544d89b9704fc840a419e70dca5569baf7912bfa5f0\""
2025-05-16T19:43:16-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:16.629873535-04:00" level=info msg="StartContainer for \"cade0436e1a63ddf06237544d89b9704fc840a419e70dca5569baf7912bfa5f0\""
2025-05-16T19:43:17-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:17.641999615-04:00" level=info msg="StartContainer for \"d8d0f5f77c0ba834eca70f42a883f8bd492d6ad136637621ba6b2ae40f204d2d\" returns successfully"
2025-05-16T19:43:17-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:17.999676386-04:00" level=info msg="StartContainer for \"cade0436e1a63ddf06237544d89b9704fc840a419e70dca5569baf7912bfa5f0\" returns successfully"
2025-05-16T19:43:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:19.324069694-04:00" level=info msg="shim disconnected" id=7f0aaeb0a8332a6bff865ffef953f6bdffa64f1982f55d5413fa774ede7e4d7b
2025-05-16T19:43:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:19.324172490-04:00" level=warning msg="cleaning up after shim disconnected" id=7f0aaeb0a8332a6bff865ffef953f6bdffa64f1982f55d5413fa774ede7e4d7b namespace=k8s.io
2025-05-16T19:43:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:19.324194908-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:43:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:19.655896506-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:43:19-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=64344 runtime=io.containerd.runc.v2\ntime=\"2025-05-16T19:43:19-04:00\" level=warning msg=\"failed to remove runc container\" error=\"runc did not terminate successfully: exit status 255: \" runtime=io.containerd.runc.v2\n"
2025-05-16T19:43:20-04:00 k8s.kubelet[38157]: I0516 19:43:20.114811   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/4a9e5f3c-87a0-4a5c-8b18-52ebe3baf44a-charm-data\") pod \"cinder-mysql-router-0\" (UID: \"4a9e5f3c-87a0-4a5c-8b18-52ebe3baf44a\") " pod="openstack/cinder-mysql-router-0"
2025-05-16T19:43:20-04:00 k8s.kubelet[38157]: I0516 19:43:20.114922   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-779cx\" (UniqueName: \"kubernetes.io/projected/4a9e5f3c-87a0-4a5c-8b18-52ebe3baf44a-kube-api-access-779cx\") pod \"cinder-mysql-router-0\" (UID: \"4a9e5f3c-87a0-4a5c-8b18-52ebe3baf44a\") " pod="openstack/cinder-mysql-router-0"
2025-05-16T19:43:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:20.732302583-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cinder-mysql-router-0,Uid:4a9e5f3c-87a0-4a5c-8b18-52ebe3baf44a,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:21.158550413-04:00" level=info msg="CreateContainer within sandbox \"20a108e18861d6591906dfb50c9a837645face6ea7713c7804d5d538ad2be428\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:43:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:21.702735508-04:00" level=info msg="CreateContainer within sandbox \"20a108e18861d6591906dfb50c9a837645face6ea7713c7804d5d538ad2be428\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"b7835cd7e8cc4f95498036986620c75df8f5ecb19f888b3a2d5d16e9f7fa3e7f\""
2025-05-16T19:43:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:21.752909247-04:00" level=info msg="StartContainer for \"b7835cd7e8cc4f95498036986620c75df8f5ecb19f888b3a2d5d16e9f7fa3e7f\""
2025-05-16T19:43:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:22.950847061-04:00" level=info msg="shim disconnected" id=d8d0f5f77c0ba834eca70f42a883f8bd492d6ad136637621ba6b2ae40f204d2d
2025-05-16T19:43:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:22.958147703-04:00" level=warning msg="cleaning up after shim disconnected" id=d8d0f5f77c0ba834eca70f42a883f8bd492d6ad136637621ba6b2ae40f204d2d namespace=k8s.io
2025-05-16T19:43:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:22.961950849-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:43:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:22.964064394-04:00" level=info msg="shim disconnected" id=cade0436e1a63ddf06237544d89b9704fc840a419e70dca5569baf7912bfa5f0
2025-05-16T19:43:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:22.964139871-04:00" level=warning msg="cleaning up after shim disconnected" id=cade0436e1a63ddf06237544d89b9704fc840a419e70dca5569baf7912bfa5f0 namespace=k8s.io
2025-05-16T19:43:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:22.964166615-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:43:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:23.162545528-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:43:23-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=64621 runtime=io.containerd.runc.v2\n"
2025-05-16T19:43:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:23.196931682-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:43:23-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=64619 runtime=io.containerd.runc.v2\n"
2025-05-16T19:43:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:23.425859180-04:00" level=info msg="StartContainer for \"b7835cd7e8cc4f95498036986620c75df8f5ecb19f888b3a2d5d16e9f7fa3e7f\" returns successfully"
2025-05-16T19:43:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:23.454920550-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/7bcl96bshrf84qkak5j60i932a8se43skz3l1/glance-api-image@sha256:6d9fc517d5e904f24cb005a86ebb3a273c261cb39cffd1cc5b865c4944327e3d\""
2025-05-16T19:43:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:23.777208546-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:43:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:23.777400575-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:43:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:23.777444436-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:43:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:23.815962481-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/eb9aa39ecaf30399d71210f506aec137c64cda1a39615b2ed14ff7a22ae2cd7e pid=64716 runtime=io.containerd.runc.v2
2025-05-16T19:43:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:24.463891324-04:00" level=info msg="CreateContainer within sandbox \"08e70b136586805efcda98a1d224eb86727d68e46aa58666ea618cd3dda558ce\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:43:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:24.607632792-04:00" level=info msg="CreateContainer within sandbox \"b3cd5efd2448999b13b0f8e3e28f7bf6815d2d8ae45f9016a58e5922e7d5f6cd\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:43:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:24.658157321-04:00" level=info msg="CreateContainer within sandbox \"08e70b136586805efcda98a1d224eb86727d68e46aa58666ea618cd3dda558ce\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"c55539bf035f3fd9451a1cc800afdfffa4aea304d5b5b3aca58c7dde23e1ecc4\""
2025-05-16T19:43:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:24.663829301-04:00" level=info msg="StartContainer for \"c55539bf035f3fd9451a1cc800afdfffa4aea304d5b5b3aca58c7dde23e1ecc4\""
2025-05-16T19:43:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:24.921069880-04:00" level=info msg="CreateContainer within sandbox \"b3cd5efd2448999b13b0f8e3e28f7bf6815d2d8ae45f9016a58e5922e7d5f6cd\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"ceb17e8b447bb9028b9278d0d57b8ae332980354006b72a1de6b234e9a95d9b6\""
2025-05-16T19:43:24-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:24.935785713-04:00" level=info msg="StartContainer for \"ceb17e8b447bb9028b9278d0d57b8ae332980354006b72a1de6b234e9a95d9b6\""
2025-05-16T19:43:26-04:00 k8s.kubelet[38157]: I0516 19:43:26.416892   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sgdvh\" (UniqueName: \"kubernetes.io/projected/978123ca-9a04-46e6-817d-bd32c889ff8e-kube-api-access-sgdvh\") pod \"keystone-mysql-router-0\" (UID: \"978123ca-9a04-46e6-817d-bd32c889ff8e\") " pod="openstack/keystone-mysql-router-0"
2025-05-16T19:43:26-04:00 k8s.kubelet[38157]: I0516 19:43:26.417010   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/978123ca-9a04-46e6-817d-bd32c889ff8e-charm-data\") pod \"keystone-mysql-router-0\" (UID: \"978123ca-9a04-46e6-817d-bd32c889ff8e\") " pod="openstack/keystone-mysql-router-0"
2025-05-16T19:43:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:27.060202430-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:keystone-mysql-router-0,Uid:978123ca-9a04-46e6-817d-bd32c889ff8e,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:27.694564324-04:00" level=info msg="StartContainer for \"c55539bf035f3fd9451a1cc800afdfffa4aea304d5b5b3aca58c7dde23e1ecc4\" returns successfully"
2025-05-16T19:43:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:28.202258486-04:00" level=info msg="CreateContainer within sandbox \"08e70b136586805efcda98a1d224eb86727d68e46aa58666ea618cd3dda558ce\" for container &ContainerMetadata{Name:mysql-router,Attempt:0,}"
2025-05-16T19:43:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:28.562484034-04:00" level=info msg="CreateContainer within sandbox \"08e70b136586805efcda98a1d224eb86727d68e46aa58666ea618cd3dda558ce\" for &ContainerMetadata{Name:mysql-router,Attempt:0,} returns container id \"3ce9993c3a2f97702df021c3e24c297b7b44a2b8ac1a46b78ecf9cc24b017ed7\""
2025-05-16T19:43:28-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:28.603045039-04:00" level=info msg="StartContainer for \"3ce9993c3a2f97702df021c3e24c297b7b44a2b8ac1a46b78ecf9cc24b017ed7\""
2025-05-16T19:43:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:29.097688427-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:cinder-mysql-router-0,Uid:4a9e5f3c-87a0-4a5c-8b18-52ebe3baf44a,Namespace:openstack,Attempt:0,} returns sandbox id \"eb9aa39ecaf30399d71210f506aec137c64cda1a39615b2ed14ff7a22ae2cd7e\""
2025-05-16T19:43:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:29.156552435-04:00" level=info msg="StartContainer for \"ceb17e8b447bb9028b9278d0d57b8ae332980354006b72a1de6b234e9a95d9b6\" returns successfully"
2025-05-16T19:43:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:29.263846737-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:43:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:29.264055426-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:43:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:29.264085097-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:43:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:29.273654801-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/6924f336fc1cddb2c11f29f23bd06258480c5b6aff6a86565ef5d09321862ad4 pid=65362 runtime=io.containerd.runc.v2
2025-05-16T19:43:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:29.809134209-04:00" level=info msg="CreateContainer within sandbox \"b3cd5efd2448999b13b0f8e3e28f7bf6815d2d8ae45f9016a58e5922e7d5f6cd\" for container &ContainerMetadata{Name:mysql-router,Attempt:0,}"
2025-05-16T19:43:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:30.049952752-04:00" level=info msg="CreateContainer within sandbox \"eb9aa39ecaf30399d71210f506aec137c64cda1a39615b2ed14ff7a22ae2cd7e\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:43:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:30.260943816-04:00" level=info msg="CreateContainer within sandbox \"b3cd5efd2448999b13b0f8e3e28f7bf6815d2d8ae45f9016a58e5922e7d5f6cd\" for &ContainerMetadata{Name:mysql-router,Attempt:0,} returns container id \"8731f37a7e3a10589c5028ac4b674fc8e07b5f5a63db329645eb98fecc5beccd\""
2025-05-16T19:43:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:30.302074865-04:00" level=info msg="StartContainer for \"8731f37a7e3a10589c5028ac4b674fc8e07b5f5a63db329645eb98fecc5beccd\""
2025-05-16T19:43:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:30.720573555-04:00" level=info msg="CreateContainer within sandbox \"eb9aa39ecaf30399d71210f506aec137c64cda1a39615b2ed14ff7a22ae2cd7e\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"8270733b958422400434c4d4824efec0d79aaf5492b4beeb348b0ebdcbb52e59\""
2025-05-16T19:43:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:30.724717160-04:00" level=info msg="StartContainer for \"8270733b958422400434c4d4824efec0d79aaf5492b4beeb348b0ebdcbb52e59\""
2025-05-16T19:43:31-04:00 k8s.kubelet[38157]: I0516 19:43:31.787199   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/94a90393-441a-4405-9c48-331369cd9170-charm-data\") pod \"horizon-mysql-router-0\" (UID: \"94a90393-441a-4405-9c48-331369cd9170\") " pod="openstack/horizon-mysql-router-0"
2025-05-16T19:43:31-04:00 k8s.kubelet[38157]: I0516 19:43:31.787333   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pl5pd\" (UniqueName: \"kubernetes.io/projected/94a90393-441a-4405-9c48-331369cd9170-kube-api-access-pl5pd\") pod \"horizon-mysql-router-0\" (UID: \"94a90393-441a-4405-9c48-331369cd9170\") " pod="openstack/horizon-mysql-router-0"
2025-05-16T19:43:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:32.151985709-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:horizon-mysql-router-0,Uid:94a90393-441a-4405-9c48-331369cd9170,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:32.967823415-04:00" level=error msg="get state for 3ce9993c3a2f97702df021c3e24c297b7b44a2b8ac1a46b78ecf9cc24b017ed7" error="context deadline exceeded: unknown"
2025-05-16T19:43:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:32.967875362-04:00" level=warning msg="unknown status" status=0
2025-05-16T19:43:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:33.008362650-04:00" level=info msg="StartContainer for \"8270733b958422400434c4d4824efec0d79aaf5492b4beeb348b0ebdcbb52e59\" returns successfully"
2025-05-16T19:43:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:33.099532879-04:00" level=info msg="StartContainer for \"8731f37a7e3a10589c5028ac4b674fc8e07b5f5a63db329645eb98fecc5beccd\" returns successfully"
2025-05-16T19:43:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:33.243955729-04:00" level=info msg="StartContainer for \"3ce9993c3a2f97702df021c3e24c297b7b44a2b8ac1a46b78ecf9cc24b017ed7\" returns successfully"
2025-05-16T19:43:34-04:00 k8s.kubelet[38157]: I0516 19:43:34.150436   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/glance-mysql-router-0" podStartSLOduration=26.150407308 podStartE2EDuration="26.150407308s" podCreationTimestamp="2025-05-16 19:43:08 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:43:34.141172761 -0400 EDT m=+607.278769463" watchObservedRunningTime="2025-05-16 19:43:34.150407308 -0400 EDT m=+607.288003999"
2025-05-16T19:43:34-04:00 k8s.kubelet[38157]: I0516 19:43:34.237673   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/nova-cell-mysql-router-0" podStartSLOduration=23.237639716 podStartE2EDuration="23.237639716s" podCreationTimestamp="2025-05-16 19:43:11 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:43:34.214366379 -0400 EDT m=+607.351963079" watchObservedRunningTime="2025-05-16 19:43:34.237639716 -0400 EDT m=+607.375236390"
2025-05-16T19:43:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:34.267930742-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:43:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:34.268118622-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:43:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:34.268154494-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:43:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:34.270833381-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/b3641f24e5bf41fbea358e4ba274a53907970a99ba1d140a6a18050b9a18a352 pid=65902 runtime=io.containerd.runc.v2
2025-05-16T19:43:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:35.062821386-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:keystone-mysql-router-0,Uid:978123ca-9a04-46e6-817d-bd32c889ff8e,Namespace:openstack,Attempt:0,} returns sandbox id \"6924f336fc1cddb2c11f29f23bd06258480c5b6aff6a86565ef5d09321862ad4\""
2025-05-16T19:43:36-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:36.709625814-04:00" level=info msg="CreateContainer within sandbox \"6924f336fc1cddb2c11f29f23bd06258480c5b6aff6a86565ef5d09321862ad4\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:43:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:37.144171064-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:horizon-mysql-router-0,Uid:94a90393-441a-4405-9c48-331369cd9170,Namespace:openstack,Attempt:0,} returns sandbox id \"b3641f24e5bf41fbea358e4ba274a53907970a99ba1d140a6a18050b9a18a352\""
2025-05-16T19:43:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:37.343303866-04:00" level=info msg="CreateContainer within sandbox \"6924f336fc1cddb2c11f29f23bd06258480c5b6aff6a86565ef5d09321862ad4\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"ab9a9769b04321e6598425b81682fa897b822304d4305819a5d294dbdc120b06\""
2025-05-16T19:43:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:37.405757059-04:00" level=info msg="StartContainer for \"ab9a9769b04321e6598425b81682fa897b822304d4305819a5d294dbdc120b06\""
2025-05-16T19:43:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:37.747833643-04:00" level=info msg="CreateContainer within sandbox \"b3641f24e5bf41fbea358e4ba274a53907970a99ba1d140a6a18050b9a18a352\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:43:38-04:00 k8s.kubelet[38157]: I0516 19:43:38.241848   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tlwfn\" (UniqueName: \"kubernetes.io/projected/d657b5da-8fe8-4251-98f6-ed7f7127e753-kube-api-access-tlwfn\") pod \"placement-0\" (UID: \"d657b5da-8fe8-4251-98f6-ed7f7127e753\") " pod="openstack/placement-0"
2025-05-16T19:43:38-04:00 k8s.kubelet[38157]: I0516 19:43:38.242008   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/d657b5da-8fe8-4251-98f6-ed7f7127e753-charm-data\") pod \"placement-0\" (UID: \"d657b5da-8fe8-4251-98f6-ed7f7127e753\") " pod="openstack/placement-0"
2025-05-16T19:43:38-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:38.268861555-04:00" level=info msg="CreateContainer within sandbox \"b3641f24e5bf41fbea358e4ba274a53907970a99ba1d140a6a18050b9a18a352\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"49d8f72e386e0eb2fc047eebc08e88ca370a50d53b83d0ec86a6dd70b8e2ba78\""
2025-05-16T19:43:38-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:38.323222827-04:00" level=info msg="StartContainer for \"49d8f72e386e0eb2fc047eebc08e88ca370a50d53b83d0ec86a6dd70b8e2ba78\""
2025-05-16T19:43:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:39.086531636-04:00" level=info msg="shim disconnected" id=8270733b958422400434c4d4824efec0d79aaf5492b4beeb348b0ebdcbb52e59
2025-05-16T19:43:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:39.086718090-04:00" level=warning msg="cleaning up after shim disconnected" id=8270733b958422400434c4d4824efec0d79aaf5492b4beeb348b0ebdcbb52e59 namespace=k8s.io
2025-05-16T19:43:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:39.086741923-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:43:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:39.192970338-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:placement-0,Uid:d657b5da-8fe8-4251-98f6-ed7f7127e753,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:39.295323981-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:43:39-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=66507 runtime=io.containerd.runc.v2\n"
2025-05-16T19:43:39-04:00 k8s.kubelet[38157]: I0516 19:43:39.673927   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/a89c8beb-fed1-421c-83bc-7183ff8d9332-charm-data\") pod \"placement-mysql-router-0\" (UID: \"a89c8beb-fed1-421c-83bc-7183ff8d9332\") " pod="openstack/placement-mysql-router-0"
2025-05-16T19:43:39-04:00 k8s.kubelet[38157]: I0516 19:43:39.674035   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8vm5p\" (UniqueName: \"kubernetes.io/projected/a89c8beb-fed1-421c-83bc-7183ff8d9332-kube-api-access-8vm5p\") pod \"placement-mysql-router-0\" (UID: \"a89c8beb-fed1-421c-83bc-7183ff8d9332\") " pod="openstack/placement-mysql-router-0"
2025-05-16T19:43:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:40.420636147-04:00" level=info msg="StartContainer for \"ab9a9769b04321e6598425b81682fa897b822304d4305819a5d294dbdc120b06\" returns successfully"
2025-05-16T19:43:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:41.416355177-04:00" level=info msg="CreateContainer within sandbox \"eb9aa39ecaf30399d71210f506aec137c64cda1a39615b2ed14ff7a22ae2cd7e\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:43:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:41.467759076-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:placement-mysql-router-0,Uid:a89c8beb-fed1-421c-83bc-7183ff8d9332,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:41-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:41.758221970-04:00" level=info msg="StartContainer for \"49d8f72e386e0eb2fc047eebc08e88ca370a50d53b83d0ec86a6dd70b8e2ba78\" returns successfully"
2025-05-16T19:43:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:42.223295952-04:00" level=info msg="CreateContainer within sandbox \"eb9aa39ecaf30399d71210f506aec137c64cda1a39615b2ed14ff7a22ae2cd7e\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"149be516e520f20842f77fd9915129d255d07a2c0e1ea78f5f90b749fd29878c\""
2025-05-16T19:43:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:42.243588335-04:00" level=info msg="StartContainer for \"149be516e520f20842f77fd9915129d255d07a2c0e1ea78f5f90b749fd29878c\""
2025-05-16T19:43:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:43.051831394-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:43:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:43.052027358-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:43:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:43.052056786-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:43:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:43.058244052-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/80325f6e83b40192e60116c67d7c9e833bc9a44789c40a5428cefb7f554096f8 pid=66744 runtime=io.containerd.runc.v2
2025-05-16T19:43:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:44.310838024-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:43:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:44.311056740-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:43:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:44.311094471-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:43:44-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:44.317144592-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/382325185a49f284e06152707b766e939198f81ee0f4df7b2d249dd602e57865 pid=66825 runtime=io.containerd.runc.v2
2025-05-16T19:43:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:45.192051410-04:00" level=info msg="StartContainer for \"149be516e520f20842f77fd9915129d255d07a2c0e1ea78f5f90b749fd29878c\" returns successfully"
2025-05-16T19:43:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:45.561040687-04:00" level=info msg="CreateContainer within sandbox \"eb9aa39ecaf30399d71210f506aec137c64cda1a39615b2ed14ff7a22ae2cd7e\" for container &ContainerMetadata{Name:mysql-router,Attempt:0,}"
2025-05-16T19:43:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:45.887856333-04:00" level=info msg="CreateContainer within sandbox \"eb9aa39ecaf30399d71210f506aec137c64cda1a39615b2ed14ff7a22ae2cd7e\" for &ContainerMetadata{Name:mysql-router,Attempt:0,} returns container id \"f29d019b2c768cc3b57273f5a4204f8ad5ce36a6643f6c56e6d8207b7e6754a9\""
2025-05-16T19:43:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:45.913886792-04:00" level=info msg="StartContainer for \"f29d019b2c768cc3b57273f5a4204f8ad5ce36a6643f6c56e6d8207b7e6754a9\""
2025-05-16T19:43:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:46.502534782-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:placement-0,Uid:d657b5da-8fe8-4251-98f6-ed7f7127e753,Namespace:openstack,Attempt:0,} returns sandbox id \"80325f6e83b40192e60116c67d7c9e833bc9a44789c40a5428cefb7f554096f8\""
2025-05-16T19:43:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:47.005494568-04:00" level=info msg="CreateContainer within sandbox \"80325f6e83b40192e60116c67d7c9e833bc9a44789c40a5428cefb7f554096f8\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:43:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:47.177825670-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:placement-mysql-router-0,Uid:a89c8beb-fed1-421c-83bc-7183ff8d9332,Namespace:openstack,Attempt:0,} returns sandbox id \"382325185a49f284e06152707b766e939198f81ee0f4df7b2d249dd602e57865\""
2025-05-16T19:43:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:47.387625503-04:00" level=info msg="CreateContainer within sandbox \"80325f6e83b40192e60116c67d7c9e833bc9a44789c40a5428cefb7f554096f8\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"ab43b18214ee6a418ab5dfa19d306f232488c426d3d9a96cdff49a8973f86a75\""
2025-05-16T19:43:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:47.403789928-04:00" level=info msg="StartContainer for \"ab43b18214ee6a418ab5dfa19d306f232488c426d3d9a96cdff49a8973f86a75\""
2025-05-16T19:43:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:47.865913771-04:00" level=info msg="CreateContainer within sandbox \"382325185a49f284e06152707b766e939198f81ee0f4df7b2d249dd602e57865\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:43:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:48.396136098-04:00" level=info msg="CreateContainer within sandbox \"382325185a49f284e06152707b766e939198f81ee0f4df7b2d249dd602e57865\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"64c0795997c7afa51ca2400a136334e41554d5fda1757282ebea1bf5d50d2136\""
2025-05-16T19:43:48-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:48.410787113-04:00" level=info msg="StartContainer for \"64c0795997c7afa51ca2400a136334e41554d5fda1757282ebea1bf5d50d2136\""
2025-05-16T19:43:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:49.164892434-04:00" level=info msg="shim disconnected" id=ab9a9769b04321e6598425b81682fa897b822304d4305819a5d294dbdc120b06
2025-05-16T19:43:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:49.164990563-04:00" level=warning msg="cleaning up after shim disconnected" id=ab9a9769b04321e6598425b81682fa897b822304d4305819a5d294dbdc120b06 namespace=k8s.io
2025-05-16T19:43:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:49.165012551-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:43:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:49.604005886-04:00" level=info msg="StartContainer for \"ab43b18214ee6a418ab5dfa19d306f232488c426d3d9a96cdff49a8973f86a75\" returns successfully"
2025-05-16T19:43:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:49.722494620-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:43:49-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=67228 runtime=io.containerd.runc.v2\n"
2025-05-16T19:43:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:50.431241281-04:00" level=error msg="collecting metrics for ab9a9769b04321e6598425b81682fa897b822304d4305819a5d294dbdc120b06" error="ttrpc: closed: unknown"
2025-05-16T19:43:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:50.461824840-04:00" level=info msg="StartContainer for \"f29d019b2c768cc3b57273f5a4204f8ad5ce36a6643f6c56e6d8207b7e6754a9\" returns successfully"
2025-05-16T19:43:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:51.090116178-04:00" level=info msg="shim disconnected" id=49d8f72e386e0eb2fc047eebc08e88ca370a50d53b83d0ec86a6dd70b8e2ba78
2025-05-16T19:43:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:51.096233022-04:00" level=warning msg="cleaning up after shim disconnected" id=49d8f72e386e0eb2fc047eebc08e88ca370a50d53b83d0ec86a6dd70b8e2ba78 namespace=k8s.io
2025-05-16T19:43:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:51.113404446-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:43:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:51.314128876-04:00" level=info msg="StartContainer for \"64c0795997c7afa51ca2400a136334e41554d5fda1757282ebea1bf5d50d2136\" returns successfully"
2025-05-16T19:43:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:51.684376038-04:00" level=error msg="collecting metrics for 49d8f72e386e0eb2fc047eebc08e88ca370a50d53b83d0ec86a6dd70b8e2ba78" error="ttrpc: closed: unknown"
2025-05-16T19:43:51-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:51.805018366-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:43:51-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=67407 runtime=io.containerd.runc.v2\n"
2025-05-16T19:43:52-04:00 k8s.kubelet[38157]: I0516 19:43:52.360924   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/cinder-mysql-router-0" podStartSLOduration=33.360895735 podStartE2EDuration="33.360895735s" podCreationTimestamp="2025-05-16 19:43:19 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:43:52.335527148 -0400 EDT m=+625.473123833" watchObservedRunningTime="2025-05-16 19:43:52.360895735 -0400 EDT m=+625.498492409"
2025-05-16T19:43:53-04:00 k8s.kubelet[38157]: I0516 19:43:53.915190   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-a6e89598-2ce1-43cb-8a45-f13a5e222a93\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-a6e89598-2ce1-43cb-8a45-f13a5e222a93\") pod \"keystone-0\" (UID: \"a4d99034-ebd2-404e-a805-9551caf25ee0\") " pod="openstack/keystone-0"
2025-05-16T19:43:53-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:53.992949642-04:00" level=info msg="CreateContainer within sandbox \"6924f336fc1cddb2c11f29f23bd06258480c5b6aff6a86565ef5d09321862ad4\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:43:54-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:54.207986455-04:00" level=info msg="CreateContainer within sandbox \"6924f336fc1cddb2c11f29f23bd06258480c5b6aff6a86565ef5d09321862ad4\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"4342cf5392e89bc2f05b568c1631681b52d3c9b3f09a1b5886ad950c7a3eff54\""
2025-05-16T19:43:54-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:54.217678318-04:00" level=info msg="StartContainer for \"4342cf5392e89bc2f05b568c1631681b52d3c9b3f09a1b5886ad950c7a3eff54\""
2025-05-16T19:43:54-04:00 k8s.kubelet[38157]: I0516 19:43:54.666343   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/a4d99034-ebd2-404e-a805-9551caf25ee0-charm-data\") pod \"keystone-0\" (UID: \"a4d99034-ebd2-404e-a805-9551caf25ee0\") " pod="openstack/keystone-0"
2025-05-16T19:43:54-04:00 k8s.kubelet[38157]: I0516 19:43:54.666430   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-cf2ee45a-9ea1-4f37-afa6-d7fb8e8bf7b5\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-cf2ee45a-9ea1-4f37-afa6-d7fb8e8bf7b5\") pod \"keystone-0\" (UID: \"a4d99034-ebd2-404e-a805-9551caf25ee0\") " pod="openstack/keystone-0"
2025-05-16T19:43:54-04:00 k8s.kubelet[38157]: I0516 19:43:54.678307   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bmcz4\" (UniqueName: \"kubernetes.io/projected/a4d99034-ebd2-404e-a805-9551caf25ee0-kube-api-access-bmcz4\") pod \"keystone-0\" (UID: \"a4d99034-ebd2-404e-a805-9551caf25ee0\") " pod="openstack/keystone-0"
2025-05-16T19:43:54-04:00 k8s.kubelet[38157]: I0516 19:43:54.849748   38157 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"pvc-a6e89598-2ce1-43cb-8a45-f13a5e222a93\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-a6e89598-2ce1-43cb-8a45-f13a5e222a93\") pod \"keystone-0\" (UID: \"a4d99034-ebd2-404e-a805-9551caf25ee0\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/rawfile.csi.openebs.io/5ac734e32095add9451aa4e10b619ae9fe3bfcd1c4a28647c4eae8c332d0d8ea/globalmount\"" pod="openstack/keystone-0"
2025-05-16T19:43:55-04:00 k8s.kubelet[38157]: I0516 19:43:55.697333   38157 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"pvc-cf2ee45a-9ea1-4f37-afa6-d7fb8e8bf7b5\" (UniqueName: \"kubernetes.io/csi/rawfile.csi.openebs.io^pvc-cf2ee45a-9ea1-4f37-afa6-d7fb8e8bf7b5\") pod \"keystone-0\" (UID: \"a4d99034-ebd2-404e-a805-9551caf25ee0\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/rawfile.csi.openebs.io/ace920904de4ef2ee53d4a22d9f5c5743d2e002748274b30c8b3f8bde8dc16b6/globalmount\"" pod="openstack/keystone-0"
2025-05-16T19:43:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:56.396038490-04:00" level=info msg="CreateContainer within sandbox \"b3641f24e5bf41fbea358e4ba274a53907970a99ba1d140a6a18050b9a18a352\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:43:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:57.096931645-04:00" level=info msg="CreateContainer within sandbox \"b3641f24e5bf41fbea358e4ba274a53907970a99ba1d140a6a18050b9a18a352\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"023b1b7f2ce84df7ccde7664ecab28133f00c72c2ea8badfbdab62af0c4b90b2\""
2025-05-16T19:43:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:57.105277438-04:00" level=info msg="StartContainer for \"023b1b7f2ce84df7ccde7664ecab28133f00c72c2ea8badfbdab62af0c4b90b2\""
2025-05-16T19:43:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:57.676738215-04:00" level=info msg="shim disconnected" id=ab43b18214ee6a418ab5dfa19d306f232488c426d3d9a96cdff49a8973f86a75
2025-05-16T19:43:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:57.676841282-04:00" level=warning msg="cleaning up after shim disconnected" id=ab43b18214ee6a418ab5dfa19d306f232488c426d3d9a96cdff49a8973f86a75 namespace=k8s.io
2025-05-16T19:43:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:57.676862226-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:43:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:57.692153520-04:00" level=info msg="shim disconnected" id=64c0795997c7afa51ca2400a136334e41554d5fda1757282ebea1bf5d50d2136
2025-05-16T19:43:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:57.693032995-04:00" level=warning msg="cleaning up after shim disconnected" id=64c0795997c7afa51ca2400a136334e41554d5fda1757282ebea1bf5d50d2136 namespace=k8s.io
2025-05-16T19:43:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:57.694137371-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:43:57-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:57.833555512-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:43:57-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=67910 runtime=io.containerd.runc.v2\n"
2025-05-16T19:43:58-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:58.207656063-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:keystone-0,Uid:a4d99034-ebd2-404e-a805-9551caf25ee0,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:58-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:58.300889689-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:43:58-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=67936 runtime=io.containerd.runc.v2\ntime=\"2025-05-16T19:43:58-04:00\" level=warning msg=\"failed to remove runc container\" error=\"runc did not terminate successfully: exit status 255: \" runtime=io.containerd.runc.v2\n"
2025-05-16T19:43:59-04:00 k8s.kubelet[38157]: I0516 19:43:59.046721   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ltkmc\" (UniqueName: \"kubernetes.io/projected/2bb4341e-6200-4667-9114-612847755fe1-kube-api-access-ltkmc\") pod \"horizon-0\" (UID: \"2bb4341e-6200-4667-9114-612847755fe1\") " pod="openstack/horizon-0"
2025-05-16T19:43:59-04:00 k8s.kubelet[38157]: I0516 19:43:59.051094   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/2bb4341e-6200-4667-9114-612847755fe1-charm-data\") pod \"horizon-0\" (UID: \"2bb4341e-6200-4667-9114-612847755fe1\") " pod="openstack/horizon-0"
2025-05-16T19:43:59-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:59.645448638-04:00" level=info msg="StartContainer for \"4342cf5392e89bc2f05b568c1631681b52d3c9b3f09a1b5886ad950c7a3eff54\" returns successfully"
2025-05-16T19:43:59-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:59.869443437-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:horizon-0,Uid:2bb4341e-6200-4667-9114-612847755fe1,Namespace:openstack,Attempt:0,}"
2025-05-16T19:43:59-04:00 k8s.containerd[36097]: time="2025-05-16T19:43:59.878302666-04:00" level=info msg="StartContainer for \"023b1b7f2ce84df7ccde7664ecab28133f00c72c2ea8badfbdab62af0c4b90b2\" returns successfully"
2025-05-16T19:44:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:00.621505303-04:00" level=info msg="CreateContainer within sandbox \"b3641f24e5bf41fbea358e4ba274a53907970a99ba1d140a6a18050b9a18a352\" for container &ContainerMetadata{Name:mysql-router,Attempt:0,}"
2025-05-16T19:44:01-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:01.438350971-04:00" level=info msg="CreateContainer within sandbox \"6924f336fc1cddb2c11f29f23bd06258480c5b6aff6a86565ef5d09321862ad4\" for container &ContainerMetadata{Name:mysql-router,Attempt:0,}"
2025-05-16T19:44:02-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:02.296571759-04:00" level=info msg="CreateContainer within sandbox \"6924f336fc1cddb2c11f29f23bd06258480c5b6aff6a86565ef5d09321862ad4\" for &ContainerMetadata{Name:mysql-router,Attempt:0,} returns container id \"756fa239773dbf7c3aa7afd3dab198d2bab54a9018530a731d50b3f4847dda4b\""
2025-05-16T19:44:02-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:02.341192958-04:00" level=info msg="StartContainer for \"756fa239773dbf7c3aa7afd3dab198d2bab54a9018530a731d50b3f4847dda4b\""
2025-05-16T19:44:02-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:02.521011990-04:00" level=info msg="CreateContainer within sandbox \"b3641f24e5bf41fbea358e4ba274a53907970a99ba1d140a6a18050b9a18a352\" for &ContainerMetadata{Name:mysql-router,Attempt:0,} returns container id \"6a701b1501f2f8ad78786f2e03e1478206b18999532d5c7cd10278dcddafd33b\""
2025-05-16T19:44:02-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:02.624863676-04:00" level=info msg="StartContainer for \"6a701b1501f2f8ad78786f2e03e1478206b18999532d5c7cd10278dcddafd33b\""
2025-05-16T19:44:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:03.074662420-04:00" level=info msg="CreateContainer within sandbox \"80325f6e83b40192e60116c67d7c9e833bc9a44789c40a5428cefb7f554096f8\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:44:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:03.161388485-04:00" level=info msg="CreateContainer within sandbox \"382325185a49f284e06152707b766e939198f81ee0f4df7b2d249dd602e57865\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:44:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:03.271213581-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:44:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:03.271382317-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:44:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:03.271410480-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:44:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:03.380781308-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/ac342f4b6832f02c2e15baa6d02104947a555fc2ba1d656d5d29717b8de1cf90 pid=68318 runtime=io.containerd.runc.v2
2025-05-16T19:44:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:04.465624051-04:00" level=info msg="CreateContainer within sandbox \"382325185a49f284e06152707b766e939198f81ee0f4df7b2d249dd602e57865\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"9894ff838e35133b76cca2d687e411c8f894ca93fafc25cd9f57ad42c245c761\""
2025-05-16T19:44:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:04.519750567-04:00" level=info msg="StartContainer for \"9894ff838e35133b76cca2d687e411c8f894ca93fafc25cd9f57ad42c245c761\""
2025-05-16T19:44:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:04.616468906-04:00" level=info msg="CreateContainer within sandbox \"80325f6e83b40192e60116c67d7c9e833bc9a44789c40a5428cefb7f554096f8\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"3f074d205c29159968a2e38cd370ec5252500d67823dad6019ab50de6f0280d6\""
2025-05-16T19:44:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:04.717820085-04:00" level=info msg="StartContainer for \"3f074d205c29159968a2e38cd370ec5252500d67823dad6019ab50de6f0280d6\""
2025-05-16T19:44:06-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:06.884306874-04:00" level=error msg="get state for 6a701b1501f2f8ad78786f2e03e1478206b18999532d5c7cd10278dcddafd33b" error="context deadline exceeded: unknown"
2025-05-16T19:44:06-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:06.884374979-04:00" level=warning msg="unknown status" status=0
2025-05-16T19:44:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:07.426746785-04:00" level=info msg="StartContainer for \"756fa239773dbf7c3aa7afd3dab198d2bab54a9018530a731d50b3f4847dda4b\" returns successfully"
2025-05-16T19:44:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:07.464328992-04:00" level=info msg="StartContainer for \"6a701b1501f2f8ad78786f2e03e1478206b18999532d5c7cd10278dcddafd33b\" returns successfully"
2025-05-16T19:44:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:07.797057481-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:44:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:07.797264462-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:44:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:07.797293164-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:44:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:07.801241325-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/a2c1acae2482e31326012dd01ee609a9353fa623555d556e6afc2dbdc6464da9 pid=68793 runtime=io.containerd.runc.v2
2025-05-16T19:44:08-04:00 k8s.kubelet[38157]: E0516 19:44:08.416212   38157 kubelet.go:2579] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.042s"
2025-05-16T19:44:08-04:00 k8s.kubelet[38157]: I0516 19:44:08.532868   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"charm-data\" (UniqueName: \"kubernetes.io/empty-dir/0689787d-3910-4f26-84a4-d98f21f25a2b-charm-data\") pod \"nova-0\" (UID: \"0689787d-3910-4f26-84a4-d98f21f25a2b\") " pod="openstack/nova-0"
2025-05-16T19:44:08-04:00 k8s.kubelet[38157]: I0516 19:44:08.543158   38157 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mvntj\" (UniqueName: \"kubernetes.io/projected/0689787d-3910-4f26-84a4-d98f21f25a2b-kube-api-access-mvntj\") pod \"nova-0\" (UID: \"0689787d-3910-4f26-84a4-d98f21f25a2b\") " pod="openstack/nova-0"
2025-05-16T19:44:08-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:08.647323399-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:keystone-0,Uid:a4d99034-ebd2-404e-a805-9551caf25ee0,Namespace:openstack,Attempt:0,} returns sandbox id \"ac342f4b6832f02c2e15baa6d02104947a555fc2ba1d656d5d29717b8de1cf90\""
2025-05-16T19:44:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:09.563742436-04:00" level=info msg="StartContainer for \"9894ff838e35133b76cca2d687e411c8f894ca93fafc25cd9f57ad42c245c761\" returns successfully"
2025-05-16T19:44:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:09.986976901-04:00" level=info msg="StartContainer for \"3f074d205c29159968a2e38cd370ec5252500d67823dad6019ab50de6f0280d6\" returns successfully"
2025-05-16T19:44:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:10.118950683-04:00" level=info msg="CreateContainer within sandbox \"ac342f4b6832f02c2e15baa6d02104947a555fc2ba1d656d5d29717b8de1cf90\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:44:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:10.192646758-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6ee1n7u7d1hsnllu86zrjwud33wbyqdrp8cg5/placement-api-image@sha256:0f15a0a2f78deb59facafcebd5c6c2c7c1ad0f868764fcfb3c3a260cfdbadd8d\""
2025-05-16T19:44:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:11.427276140-04:00" level=info msg="CreateContainer within sandbox \"382325185a49f284e06152707b766e939198f81ee0f4df7b2d249dd602e57865\" for container &ContainerMetadata{Name:mysql-router,Attempt:0,}"
2025-05-16T19:44:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:11.785576326-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nova-0,Uid:0689787d-3910-4f26-84a4-d98f21f25a2b,Namespace:openstack,Attempt:0,}"
2025-05-16T19:44:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:12.256091487-04:00" level=info msg="CreateContainer within sandbox \"ac342f4b6832f02c2e15baa6d02104947a555fc2ba1d656d5d29717b8de1cf90\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"949cbe8ece1ea7105a1168b0c4f9d9a069da707cf83d324bcabe00e1ead9a3c0\""
2025-05-16T19:44:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:12.256327438-04:00" level=error msg="get state for a2c1acae2482e31326012dd01ee609a9353fa623555d556e6afc2dbdc6464da9" error="context deadline exceeded: unknown"
2025-05-16T19:44:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:12.256358036-04:00" level=warning msg="unknown status" status=0
2025-05-16T19:44:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:12.307862264-04:00" level=info msg="StartContainer for \"949cbe8ece1ea7105a1168b0c4f9d9a069da707cf83d324bcabe00e1ead9a3c0\""
2025-05-16T19:44:12-04:00 k8s.kubelet[38157]: I0516 19:44:12.408421   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/keystone-mysql-router-0" podStartSLOduration=46.408393773 podStartE2EDuration="46.408393773s" podCreationTimestamp="2025-05-16 19:43:26 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:44:12.257618741 -0400 EDT m=+645.395215444" watchObservedRunningTime="2025-05-16 19:44:12.408393773 -0400 EDT m=+645.545990447"
2025-05-16T19:44:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:12.876719514-04:00" level=info msg="CreateContainer within sandbox \"382325185a49f284e06152707b766e939198f81ee0f4df7b2d249dd602e57865\" for &ContainerMetadata{Name:mysql-router,Attempt:0,} returns container id \"6932d3037423ecd09990f98393853908f7a4353408b671716fec2b83b0fe4c4c\""
2025-05-16T19:44:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:12.896708369-04:00" level=info msg="StartContainer for \"6932d3037423ecd09990f98393853908f7a4353408b671716fec2b83b0fe4c4c\""
2025-05-16T19:44:13-04:00 k8s.kubelet[38157]: I0516 19:44:13.638867   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/horizon-mysql-router-0" podStartSLOduration=42.638833296 podStartE2EDuration="42.638833296s" podCreationTimestamp="2025-05-16 19:43:31 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:44:13.589897269 -0400 EDT m=+646.727493956" watchObservedRunningTime="2025-05-16 19:44:13.638833296 -0400 EDT m=+646.776430007"
2025-05-16T19:44:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:13.649821815-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:horizon-0,Uid:2bb4341e-6200-4667-9114-612847755fe1,Namespace:openstack,Attempt:0,} returns sandbox id \"a2c1acae2482e31326012dd01ee609a9353fa623555d556e6afc2dbdc6464da9\""
2025-05-16T19:44:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:14.422124726-04:00" level=info msg="CreateContainer within sandbox \"a2c1acae2482e31326012dd01ee609a9353fa623555d556e6afc2dbdc6464da9\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:44:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:15.351679051-04:00" level=info msg="CreateContainer within sandbox \"a2c1acae2482e31326012dd01ee609a9353fa623555d556e6afc2dbdc6464da9\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"b12d15261e38a21a326f7b5c6dc764ffdc89ff204eb706dd94d9b6e198e68442\""
2025-05-16T19:44:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:15.381702613-04:00" level=info msg="StartContainer for \"b12d15261e38a21a326f7b5c6dc764ffdc89ff204eb706dd94d9b6e198e68442\""
2025-05-16T19:44:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:15.716926397-04:00" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
2025-05-16T19:44:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:15.717159944-04:00" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
2025-05-16T19:44:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:15.717198160-04:00" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
2025-05-16T19:44:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:15.721023286-04:00" level=info msg="starting signal loop" namespace=k8s.io path=/run/containerd/io.containerd.runtime.v2.task/k8s.io/40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30 pid=69468 runtime=io.containerd.runc.v2
2025-05-16T19:44:17-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:17.772352294-04:00" level=info msg="StartContainer for \"6932d3037423ecd09990f98393853908f7a4353408b671716fec2b83b0fe4c4c\" returns successfully"
2025-05-16T19:44:18-04:00 k8s.kubelet[38157]: I0516 19:44:18.165016   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/placement-mysql-router-0" podStartSLOduration=39.164982539 podStartE2EDuration="39.164982539s" podCreationTimestamp="2025-05-16 19:43:39 -0400 EDT" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-16 19:44:18.102878467 -0400 EDT m=+651.240475160" watchObservedRunningTime="2025-05-16 19:44:18.164982539 -0400 EDT m=+651.302579224"
2025-05-16T19:44:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:18.473736742-04:00" level=info msg="StartContainer for \"b12d15261e38a21a326f7b5c6dc764ffdc89ff204eb706dd94d9b6e198e68442\" returns successfully"
2025-05-16T19:44:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:18.644668119-04:00" level=info msg="StartContainer for \"949cbe8ece1ea7105a1168b0c4f9d9a069da707cf83d324bcabe00e1ead9a3c0\" returns successfully"
2025-05-16T19:44:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:20.538881019-04:00" level=error msg="get state for 40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30" error="context deadline exceeded: unknown"
2025-05-16T19:44:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:20.541717771-04:00" level=warning msg="unknown status" status=0
2025-05-16T19:44:21-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:21.771949167-04:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:nova-0,Uid:0689787d-3910-4f26-84a4-d98f21f25a2b,Namespace:openstack,Attempt:0,} returns sandbox id \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\""
2025-05-16T19:44:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:22.653620304-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for container &ContainerMetadata{Name:charm-init,Attempt:0,}"
2025-05-16T19:44:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:23.107436040-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for &ContainerMetadata{Name:charm-init,Attempt:0,} returns container id \"f4cb08385b73951f37a437584c4c8c5ebb6d19084cc5588a76fead309617a24e\""
2025-05-16T19:44:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:23.120322439-04:00" level=info msg="StartContainer for \"f4cb08385b73951f37a437584c4c8c5ebb6d19084cc5588a76fead309617a24e\""
2025-05-16T19:44:25-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:25.579658995-04:00" level=info msg="shim disconnected" id=949cbe8ece1ea7105a1168b0c4f9d9a069da707cf83d324bcabe00e1ead9a3c0
2025-05-16T19:44:25-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:25.579771247-04:00" level=warning msg="cleaning up after shim disconnected" id=949cbe8ece1ea7105a1168b0c4f9d9a069da707cf83d324bcabe00e1ead9a3c0 namespace=k8s.io
2025-05-16T19:44:25-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:25.579794298-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:44:25-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:25.793510814-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:44:25-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=70092 runtime=io.containerd.runc.v2\ntime=\"2025-05-16T19:44:25-04:00\" level=warning msg=\"failed to remove runc container\" error=\"runc did not terminate successfully: exit status 255: \" runtime=io.containerd.runc.v2\n"
2025-05-16T19:44:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:26.162699333-04:00" level=error msg="get state for f4cb08385b73951f37a437584c4c8c5ebb6d19084cc5588a76fead309617a24e" error="context deadline exceeded: unknown"
2025-05-16T19:44:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:26.163198446-04:00" level=warning msg="unknown status" status=0
2025-05-16T19:44:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:26.276662399-04:00" level=info msg="shim disconnected" id=b12d15261e38a21a326f7b5c6dc764ffdc89ff204eb706dd94d9b6e198e68442
2025-05-16T19:44:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:26.276754438-04:00" level=warning msg="cleaning up after shim disconnected" id=b12d15261e38a21a326f7b5c6dc764ffdc89ff204eb706dd94d9b6e198e68442 namespace=k8s.io
2025-05-16T19:44:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:26.276776927-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:44:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:26.399950890-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:44:26-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=70151 runtime=io.containerd.runc.v2\n"
2025-05-16T19:44:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:26.891046449-04:00" level=info msg="StartContainer for \"f4cb08385b73951f37a437584c4c8c5ebb6d19084cc5588a76fead309617a24e\" returns successfully"
2025-05-16T19:44:27-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:27.681583709-04:00" level=error msg="ExecSync for \"66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d\" failed" error="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded"
2025-05-16T19:44:27-04:00 k8s.kubelet[38157]: E0516 19:44:27.715678   38157 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded" containerID="66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d" cmd=["mongo","--port=37017","--tls","--tlsAllowInvalidHostnames","--tlsAllowInvalidCertificates","--tlsCertificateKeyFile=/var/lib/juju/server.pem","--eval","db.adminCommand('ping')"]
2025-05-16T19:44:28-04:00 k8s.kubelet[38157]: E0516 19:44:28.555194   38157 cadvisor_stats_provider.go:522] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda4d99034_ebd2_404e_a805_9551caf25ee0.slice/cri-containerd-949cbe8ece1ea7105a1168b0c4f9d9a069da707cf83d324bcabe00e1ead9a3c0.scope\": RecentStats: unable to find data in memory cache], [\"/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod2bb4341e_6200_4667_9114_612847755fe1.slice/cri-containerd-b12d15261e38a21a326f7b5c6dc764ffdc89ff204eb706dd94d9b6e198e68442.scope\": RecentStats: unable to find data in memory cache]"
2025-05-16T19:44:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:29.206135333-04:00" level=info msg="CreateContainer within sandbox \"ac342f4b6832f02c2e15baa6d02104947a555fc2ba1d656d5d29717b8de1cf90\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:44:29-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:29.666523982-04:00" level=info msg="CreateContainer within sandbox \"a2c1acae2482e31326012dd01ee609a9353fa623555d556e6afc2dbdc6464da9\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:44:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:30.167247216-04:00" level=info msg="CreateContainer within sandbox \"ac342f4b6832f02c2e15baa6d02104947a555fc2ba1d656d5d29717b8de1cf90\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"5e4846ad561028137d183c127021960f4cc48c6156e8ac5819a62d116b1d9bb0\""
2025-05-16T19:44:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:30.174669492-04:00" level=info msg="CreateContainer within sandbox \"a2c1acae2482e31326012dd01ee609a9353fa623555d556e6afc2dbdc6464da9\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"5138ea34e16ba97252e6b8b9f271e7bd6826c8ddbacd73b2c0e6566d503569c6\""
2025-05-16T19:44:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:30.176197467-04:00" level=info msg="StartContainer for \"5138ea34e16ba97252e6b8b9f271e7bd6826c8ddbacd73b2c0e6566d503569c6\""
2025-05-16T19:44:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:30.333807235-04:00" level=info msg="StartContainer for \"5e4846ad561028137d183c127021960f4cc48c6156e8ac5819a62d116b1d9bb0\""
2025-05-16T19:44:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:32.551696629-04:00" level=info msg="StartContainer for \"5e4846ad561028137d183c127021960f4cc48c6156e8ac5819a62d116b1d9bb0\" returns successfully"
2025-05-16T19:44:32-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:32.558128146-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/68mxyvezznh1q7itiq4fg25lv5j5bod890rug/keystone-image@sha256:afcef76b44a294cd4626d14bba2766bfee697e5333bc15ce8d6c302372ad2f73\""
2025-05-16T19:44:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:33.183764836-04:00" level=info msg="StartContainer for \"5138ea34e16ba97252e6b8b9f271e7bd6826c8ddbacd73b2c0e6566d503569c6\" returns successfully"
2025-05-16T19:44:33-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:33.217654789-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/ctws3sfxf7i2j12tvw9obbkcq9q0576tozepc/horizon-image@sha256:45a98212b968b641749c57ce3f1dc8340885b97a2db8cf64cbfd6c55c6bebf41\""
2025-05-16T19:44:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:34.145863901-04:00" level=info msg="shim disconnected" id=f4cb08385b73951f37a437584c4c8c5ebb6d19084cc5588a76fead309617a24e
2025-05-16T19:44:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:34.150227869-04:00" level=warning msg="cleaning up after shim disconnected" id=f4cb08385b73951f37a437584c4c8c5ebb6d19084cc5588a76fead309617a24e namespace=k8s.io
2025-05-16T19:44:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:34.150281969-04:00" level=info msg="cleaning up dead shim"
2025-05-16T19:44:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:34.387215145-04:00" level=warning msg="cleanup warnings time=\"2025-05-16T19:44:34-04:00\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=70632 runtime=io.containerd.runc.v2\n"
2025-05-16T19:44:36-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:36.416666862-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for container &ContainerMetadata{Name:charm,Attempt:0,}"
2025-05-16T19:44:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:37.141255786-04:00" level=error msg="collecting metrics for f4cb08385b73951f37a437584c4c8c5ebb6d19084cc5588a76fead309617a24e" error="ttrpc: closed: unknown"
2025-05-16T19:44:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:37.791730980-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for &ContainerMetadata{Name:charm,Attempt:0,} returns container id \"7a9bdced281136d3434e19a3b5af2a12749fb3d5d1c72cfe543494cc665281c4\""
2025-05-16T19:44:37-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:37.807901116-04:00" level=info msg="StartContainer for \"7a9bdced281136d3434e19a3b5af2a12749fb3d5d1c72cfe543494cc665281c4\""
2025-05-16T19:44:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:39.789644048-04:00" level=info msg="StartContainer for \"7a9bdced281136d3434e19a3b5af2a12749fb3d5d1c72cfe543494cc665281c4\" returns successfully"
2025-05-16T19:44:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:44:39.864424928-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-api-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:47:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:47:46.925774420-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/hyn7tmdxrcl650kyzozombx0re0xtmqs378xi/neutron-server-image@sha256:e921bf3dc992f4ec21085eaa06a3e16fa33ddd86d0352f6b0e493b0769db2605,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:47:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:47:46.961710965-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:56ccd03467afeabe4d904e77cbc145d2789575b6793624d103d70cef1c1a1d74,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:47:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:47:46.979281340-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/hyn7tmdxrcl650kyzozombx0re0xtmqs378xi/neutron-server-image@sha256:e921bf3dc992f4ec21085eaa06a3e16fa33ddd86d0352f6b0e493b0769db2605\" returns image reference \"sha256:56ccd03467afeabe4d904e77cbc145d2789575b6793624d103d70cef1c1a1d74\""
2025-05-16T19:47:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:47:47.154938624-04:00" level=info msg="CreateContainer within sandbox \"a3400cfaf9db5145d6586531ed4b08dec08a70b2529629f27143d9f624fccce4\" for container &ContainerMetadata{Name:neutron-server,Attempt:0,}"
2025-05-16T19:47:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:47:47.727260004-04:00" level=info msg="CreateContainer within sandbox \"a3400cfaf9db5145d6586531ed4b08dec08a70b2529629f27143d9f624fccce4\" for &ContainerMetadata{Name:neutron-server,Attempt:0,} returns container id \"22c8583b4ccaada60e35dc04881a7a100270f86bc082b1fdcffd0df6f7842efe\""
2025-05-16T19:47:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:47:47.757995020-04:00" level=info msg="StartContainer for \"22c8583b4ccaada60e35dc04881a7a100270f86bc082b1fdcffd0df6f7842efe\""
2025-05-16T19:47:49-04:00 k8s.containerd[36097]: time="2025-05-16T19:47:49.737378343-04:00" level=info msg="StartContainer for \"22c8583b4ccaada60e35dc04881a7a100270f86bc082b1fdcffd0df6f7842efe\" returns successfully"
2025-05-16T19:47:50-04:00 k8s.kubelet[38157]: I0516 19:47:50.380999   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/neutron-0" podStartSLOduration=14.346839123 podStartE2EDuration="5m31.380967751s" podCreationTimestamp="2025-05-16 19:42:19 -0400 EDT" firstStartedPulling="2025-05-16 19:42:29.964590274 -0400 EDT m=+543.102186959" lastFinishedPulling="2025-05-16 19:47:46.9987189 -0400 EDT m=+860.136315587" observedRunningTime="2025-05-16 19:47:50.340629141 -0400 EDT m=+863.478225846" watchObservedRunningTime="2025-05-16 19:47:50.380967751 -0400 EDT m=+863.518564424"
2025-05-16T19:48:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:07.289781435-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/6ee1n7u7d1hsnllu86zrjwud33wbyqdrp8cg5/placement-api-image@sha256:0f15a0a2f78deb59facafcebd5c6c2c7c1ad0f868764fcfb3c3a260cfdbadd8d,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:48:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:07.368889082-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:ff20d6d2070bd173cca0b6e1a4a00a51943bf88e94f6963c8a64d87f4c045411,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:48:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:07.374487337-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/6ee1n7u7d1hsnllu86zrjwud33wbyqdrp8cg5/placement-api-image@sha256:0f15a0a2f78deb59facafcebd5c6c2c7c1ad0f868764fcfb3c3a260cfdbadd8d\" returns image reference \"sha256:ff20d6d2070bd173cca0b6e1a4a00a51943bf88e94f6963c8a64d87f4c045411\""
2025-05-16T19:48:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:07.644155291-04:00" level=info msg="CreateContainer within sandbox \"80325f6e83b40192e60116c67d7c9e833bc9a44789c40a5428cefb7f554096f8\" for container &ContainerMetadata{Name:placement-api,Attempt:0,}"
2025-05-16T19:48:08-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:08.007991393-04:00" level=info msg="CreateContainer within sandbox \"80325f6e83b40192e60116c67d7c9e833bc9a44789c40a5428cefb7f554096f8\" for &ContainerMetadata{Name:placement-api,Attempt:0,} returns container id \"07e62df8c0dc9c69553c0a62812ce057a063e4f1a0c35c6d24b08640c93ffdbe\""
2025-05-16T19:48:08-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:08.012887551-04:00" level=info msg="StartContainer for \"07e62df8c0dc9c69553c0a62812ce057a063e4f1a0c35c6d24b08640c93ffdbe\""
2025-05-16T19:48:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:10.933127695-04:00" level=error msg="get state for 07e62df8c0dc9c69553c0a62812ce057a063e4f1a0c35c6d24b08640c93ffdbe" error="context deadline exceeded: unknown"
2025-05-16T19:48:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:10.933195291-04:00" level=warning msg="unknown status" status=0
2025-05-16T19:48:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:12.806998960-04:00" level=info msg="StartContainer for \"07e62df8c0dc9c69553c0a62812ce057a063e4f1a0c35c6d24b08640c93ffdbe\" returns successfully"
2025-05-16T19:48:14-04:00 k8s.kubelet[38157]: I0516 19:48:14.363295   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/placement-0" podStartSLOduration=40.109000924 podStartE2EDuration="4m37.363265553s" podCreationTimestamp="2025-05-16 19:43:37 -0400 EDT" firstStartedPulling="2025-05-16 19:44:10.142839145 -0400 EDT m=+643.280435827" lastFinishedPulling="2025-05-16 19:48:07.397103771 -0400 EDT m=+880.534700456" observedRunningTime="2025-05-16 19:48:13.595786798 -0400 EDT m=+886.733383497" watchObservedRunningTime="2025-05-16 19:48:14.363265553 -0400 EDT m=+887.500862244"
2025-05-16T19:48:19-04:00 k8s.kube-apiserver[37236]: W0516 19:48:19.713507   37236 controller.go:139] slow openapi aggregation of "ciliumnetworkpolicies.cilium.io": 1.136252641s
2025-05-16T19:48:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:34.605028621-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/68mxyvezznh1q7itiq4fg25lv5j5bod890rug/keystone-image@sha256:afcef76b44a294cd4626d14bba2766bfee697e5333bc15ce8d6c302372ad2f73,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:48:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:34.636184009-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:623d99f4a40e8c94ef465067e09c4a60d778e509b4200cc70c9cf9fe4e696daf,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:48:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:34.647564628-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/68mxyvezznh1q7itiq4fg25lv5j5bod890rug/keystone-image@sha256:afcef76b44a294cd4626d14bba2766bfee697e5333bc15ce8d6c302372ad2f73\" returns image reference \"sha256:623d99f4a40e8c94ef465067e09c4a60d778e509b4200cc70c9cf9fe4e696daf\""
2025-05-16T19:48:34-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:34.767320114-04:00" level=info msg="CreateContainer within sandbox \"ac342f4b6832f02c2e15baa6d02104947a555fc2ba1d656d5d29717b8de1cf90\" for container &ContainerMetadata{Name:keystone,Attempt:0,}"
2025-05-16T19:48:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:35.176946534-04:00" level=info msg="CreateContainer within sandbox \"ac342f4b6832f02c2e15baa6d02104947a555fc2ba1d656d5d29717b8de1cf90\" for &ContainerMetadata{Name:keystone,Attempt:0,} returns container id \"c94055e69dd1ab1eb1329c5dd89207301caa04e28cc9333c51e17979dfa5ab6f\""
2025-05-16T19:48:35-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:35.185953491-04:00" level=info msg="StartContainer for \"c94055e69dd1ab1eb1329c5dd89207301caa04e28cc9333c51e17979dfa5ab6f\""
2025-05-16T19:48:38-04:00 k8s.containerd[36097]: time="2025-05-16T19:48:38.813990223-04:00" level=info msg="StartContainer for \"c94055e69dd1ab1eb1329c5dd89207301caa04e28cc9333c51e17979dfa5ab6f\" returns successfully"
2025-05-16T19:48:39-04:00 k8s.kubelet[38157]: I0516 19:48:39.667969   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/keystone-0" podStartSLOduration=47.549784421 podStartE2EDuration="4m49.667942209s" podCreationTimestamp="2025-05-16 19:43:50 -0400 EDT" firstStartedPulling="2025-05-16 19:44:32.557038165 -0400 EDT m=+665.694634847" lastFinishedPulling="2025-05-16 19:48:34.67519595 -0400 EDT m=+907.812792635" observedRunningTime="2025-05-16 19:48:39.643028989 -0400 EDT m=+912.780625676" watchObservedRunningTime="2025-05-16 19:48:39.667942209 -0400 EDT m=+912.805538883"
2025-05-16T19:49:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:39.494991177-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-api-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:49:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:39.516680618-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:611e7d27b62e23254bd660d0117f8edc3171e7811b8c9728c329c95a5d5ab628,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:49:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:39.517851946-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-api-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\" returns image reference \"sha256:611e7d27b62e23254bd660d0117f8edc3171e7811b8c9728c329c95a5d5ab628\""
2025-05-16T19:49:39-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:39.706061754-04:00" level=info msg="CreateContainer within sandbox \"5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429\" for container &ContainerMetadata{Name:cinder-api,Attempt:0,}"
2025-05-16T19:49:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:40.100431433-04:00" level=info msg="CreateContainer within sandbox \"5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429\" for &ContainerMetadata{Name:cinder-api,Attempt:0,} returns container id \"97403c2ee444c507e396dec6af1b2c039db5a8b34b07881a055f48c30cc5dfef\""
2025-05-16T19:49:40-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:40.113880173-04:00" level=info msg="StartContainer for \"97403c2ee444c507e396dec6af1b2c039db5a8b34b07881a055f48c30cc5dfef\""
2025-05-16T19:49:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:42.875689585-04:00" level=error msg="get state for 97403c2ee444c507e396dec6af1b2c039db5a8b34b07881a055f48c30cc5dfef" error="context deadline exceeded: unknown"
2025-05-16T19:49:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:42.876404632-04:00" level=warning msg="unknown status" status=0
2025-05-16T19:49:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:43.557802605-04:00" level=info msg="StartContainer for \"97403c2ee444c507e396dec6af1b2c039db5a8b34b07881a055f48c30cc5dfef\" returns successfully"
2025-05-16T19:49:43-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:43.619524463-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\""
2025-05-16T19:49:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:45.316805371-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:49:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:45.776702854-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:49:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:45.851615689-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\": failed to resolve reference \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:49:45-04:00 k8s.kubelet[38157]: E0516 19:49:45.853571   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\": failed to resolve reference \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264"
2025-05-16T19:49:45-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:45.862834405-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\""
2025-05-16T19:49:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:46.944629272-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:49:46-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:46.960213896-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/ebs3142hrttofbph7b3nwcdnxsnf9mj4cqefz/cinder-scheduler-image@sha256:45e3776f1090a77572e0a72bcc4ac26043e6f79f4a7b578bec8577215d773264\" returns image reference \"sha256:611e7d27b62e23254bd660d0117f8edc3171e7811b8c9728c329c95a5d5ab628\""
2025-05-16T19:49:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:47.096762129-04:00" level=info msg="CreateContainer within sandbox \"5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429\" for container &ContainerMetadata{Name:cinder-scheduler,Attempt:0,}"
2025-05-16T19:49:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:47.506365819-04:00" level=info msg="CreateContainer within sandbox \"5421430abdedf265fe0a7147b9464dd7d9126fcc017e85a532be14a21a139429\" for &ContainerMetadata{Name:cinder-scheduler,Attempt:0,} returns container id \"7793a4dadaf1f7003303e27d8d4d403f605f6b985f5f8b435dd040117475aefc\""
2025-05-16T19:49:47-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:47.511725220-04:00" level=info msg="StartContainer for \"7793a4dadaf1f7003303e27d8d4d403f605f6b985f5f8b435dd040117475aefc\""
2025-05-16T19:49:50-04:00 k8s.containerd[36097]: time="2025-05-16T19:49:50.152220176-04:00" level=info msg="StartContainer for \"7793a4dadaf1f7003303e27d8d4d403f605f6b985f5f8b435dd040117475aefc\" returns successfully"
2025-05-16T19:49:50-04:00 k8s.kubelet[38157]: I0516 19:49:50.698529   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/cinder-0" podStartSLOduration=18.047579856 podStartE2EDuration="6m55.698495163s" podCreationTimestamp="2025-05-16 19:42:55 -0400 EDT" firstStartedPulling="2025-05-16 19:43:09.313547228 -0400 EDT m=+582.451143913" lastFinishedPulling="2025-05-16 19:49:46.964462533 -0400 EDT m=+980.102059220" observedRunningTime="2025-05-16 19:49:50.665915004 -0400 EDT m=+983.803511695" watchObservedRunningTime="2025-05-16 19:49:50.698495163 -0400 EDT m=+983.836091855"
2025-05-16T19:50:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:00.887910984-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/7bcl96bshrf84qkak5j60i932a8se43skz3l1/glance-api-image@sha256:6d9fc517d5e904f24cb005a86ebb3a273c261cb39cffd1cc5b865c4944327e3d,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:50:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:00.905287840-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:0e11ae2e1eabf78785ad9d277ad8f614afbb2f1c7fe54a3d76996c3da58f4cf4,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:50:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:00.906524758-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/7bcl96bshrf84qkak5j60i932a8se43skz3l1/glance-api-image@sha256:6d9fc517d5e904f24cb005a86ebb3a273c261cb39cffd1cc5b865c4944327e3d\" returns image reference \"sha256:0e11ae2e1eabf78785ad9d277ad8f614afbb2f1c7fe54a3d76996c3da58f4cf4\""
2025-05-16T19:50:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:00.980064607-04:00" level=info msg="CreateContainer within sandbox \"20a108e18861d6591906dfb50c9a837645face6ea7713c7804d5d538ad2be428\" for container &ContainerMetadata{Name:glance-api,Attempt:0,}"
2025-05-16T19:50:01-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:01.395079130-04:00" level=info msg="CreateContainer within sandbox \"20a108e18861d6591906dfb50c9a837645face6ea7713c7804d5d538ad2be428\" for &ContainerMetadata{Name:glance-api,Attempt:0,} returns container id \"58040d95ffcfb71aff12064b3781296caecaa51efbaa3e5bd4f2a7eb6c8a658b\""
2025-05-16T19:50:01-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:01.405704960-04:00" level=info msg="StartContainer for \"58040d95ffcfb71aff12064b3781296caecaa51efbaa3e5bd4f2a7eb6c8a658b\""
2025-05-16T19:50:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:04.152053454-04:00" level=error msg="get state for 58040d95ffcfb71aff12064b3781296caecaa51efbaa3e5bd4f2a7eb6c8a658b" error="context deadline exceeded: unknown"
2025-05-16T19:50:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:04.152138616-04:00" level=warning msg="unknown status" status=0
2025-05-16T19:50:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:04.600140276-04:00" level=info msg="StartContainer for \"58040d95ffcfb71aff12064b3781296caecaa51efbaa3e5bd4f2a7eb6c8a658b\" returns successfully"
2025-05-16T19:50:05-04:00 k8s.kubelet[38157]: I0516 19:50:05.049617   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/glance-0" podStartSLOduration=21.587699117 podStartE2EDuration="6m59.049589969s" podCreationTimestamp="2025-05-16 19:43:06 -0400 EDT" firstStartedPulling="2025-05-16 19:43:23.446614379 -0400 EDT m=+596.584211061" lastFinishedPulling="2025-05-16 19:50:00.908505229 -0400 EDT m=+994.046101913" observedRunningTime="2025-05-16 19:50:05.047423608 -0400 EDT m=+998.185020282" watchObservedRunningTime="2025-05-16 19:50:05.049589969 -0400 EDT m=+998.187186653"
2025-05-16T19:50:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:09.873573817-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-api-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:50:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:09.920018311-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/ctws3sfxf7i2j12tvw9obbkcq9q0576tozepc/horizon-image@sha256:45a98212b968b641749c57ce3f1dc8340885b97a2db8cf64cbfd6c55c6bebf41,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:50:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:09.931136354-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:38aadeed93d1559105e7435dbe45ec0cd5ab525e93bd82a528ceee098b769fe1,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:50:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:09.948265242-04:00" level=info msg="ImageCreate event &ImageCreate{Name:sha256:e1eed125b5193a3d74e24f1e470e3ba34c868eeafe2cc8ae2b6add2ddc3d9e6d,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:50:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:09.950928855-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-api-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" returns image reference \"sha256:38aadeed93d1559105e7435dbe45ec0cd5ab525e93bd82a528ceee098b769fe1\""
2025-05-16T19:50:09-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:09.962940696-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/ctws3sfxf7i2j12tvw9obbkcq9q0576tozepc/horizon-image@sha256:45a98212b968b641749c57ce3f1dc8340885b97a2db8cf64cbfd6c55c6bebf41\" returns image reference \"sha256:e1eed125b5193a3d74e24f1e470e3ba34c868eeafe2cc8ae2b6add2ddc3d9e6d\""
2025-05-16T19:50:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:10.083937363-04:00" level=info msg="CreateContainer within sandbox \"a2c1acae2482e31326012dd01ee609a9353fa623555d556e6afc2dbdc6464da9\" for container &ContainerMetadata{Name:horizon,Attempt:0,}"
2025-05-16T19:50:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:10.092869447-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for container &ContainerMetadata{Name:nova-api,Attempt:0,}"
2025-05-16T19:50:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:10.280537129-04:00" level=info msg="CreateContainer within sandbox \"a2c1acae2482e31326012dd01ee609a9353fa623555d556e6afc2dbdc6464da9\" for &ContainerMetadata{Name:horizon,Attempt:0,} returns container id \"d94ebb878486667382cdce8cdc87a1cb4a1fb3bb135678541a2a320b8934a447\""
2025-05-16T19:50:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:10.292238279-04:00" level=info msg="StartContainer for \"d94ebb878486667382cdce8cdc87a1cb4a1fb3bb135678541a2a320b8934a447\""
2025-05-16T19:50:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:10.347529715-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for &ContainerMetadata{Name:nova-api,Attempt:0,} returns container id \"8551d40af4ddb025749eb1aee1f1932be74057a3afaeb7b6dd1e8f34a1f113d7\""
2025-05-16T19:50:10-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:10.348054113-04:00" level=info msg="StartContainer for \"8551d40af4ddb025749eb1aee1f1932be74057a3afaeb7b6dd1e8f34a1f113d7\""
2025-05-16T19:50:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:11.803100000-04:00" level=info msg="StartContainer for \"8551d40af4ddb025749eb1aee1f1932be74057a3afaeb7b6dd1e8f34a1f113d7\" returns successfully"
2025-05-16T19:50:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:11.822087454-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:50:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:12.503372333-04:00" level=info msg="StartContainer for \"d94ebb878486667382cdce8cdc87a1cb4a1fb3bb135678541a2a320b8934a447\" returns successfully"
2025-05-16T19:50:12-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:12.706133686-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:50:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:13.146820113-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:50:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:13.152727342-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:50:13-04:00 k8s.kubelet[38157]: E0516 19:50:13.154038   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:50:13-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:13.157984696-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:50:13-04:00 k8s.kubelet[38157]: I0516 19:50:13.280826   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/horizon-0" podStartSLOduration=39.527075943 podStartE2EDuration="6m16.280795908s" podCreationTimestamp="2025-05-16 19:43:57 -0400 EDT" firstStartedPulling="2025-05-16 19:44:33.215647288 -0400 EDT m=+666.353243975" lastFinishedPulling="2025-05-16 19:50:09.969367253 -0400 EDT m=+1003.106963940" observedRunningTime="2025-05-16 19:50:13.275755079 -0400 EDT m=+1006.413351782" watchObservedRunningTime="2025-05-16 19:50:13.280795908 -0400 EDT m=+1006.418392581"
2025-05-16T19:50:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:14.847823779-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:50:14-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:14.857991091-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-conductor-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" returns image reference \"sha256:38aadeed93d1559105e7435dbe45ec0cd5ab525e93bd82a528ceee098b769fe1\""
2025-05-16T19:50:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:15.054090311-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for container &ContainerMetadata{Name:nova-conductor,Attempt:0,}"
2025-05-16T19:50:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:15.169942028-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for &ContainerMetadata{Name:nova-conductor,Attempt:0,} returns container id \"f1fa49d1ee856b754183436a2ee344859b78e231c49d578989c4ac97f775aaf3\""
2025-05-16T19:50:15-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:15.190515405-04:00" level=info msg="StartContainer for \"f1fa49d1ee856b754183436a2ee344859b78e231c49d578989c4ac97f775aaf3\""
2025-05-16T19:50:17-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:17.328382495-04:00" level=info msg="StartContainer for \"f1fa49d1ee856b754183436a2ee344859b78e231c49d578989c4ac97f775aaf3\" returns successfully"
2025-05-16T19:50:17-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:17.332081823-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:50:18-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:18.572414808-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:50:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:19.003819604-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:50:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:19.017571468-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:50:19-04:00 k8s.kubelet[38157]: E0516 19:50:19.022762   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:50:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:19.027183370-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:50:19-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:19.961095292-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:50:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:20.821763999-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:50:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:20.829175548-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:50:20-04:00 k8s.kubelet[38157]: E0516 19:50:20.830852   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:50:20-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:20.831404740-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:50:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:22.270309743-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:50:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:22.281606357-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-scheduler-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" returns image reference \"sha256:38aadeed93d1559105e7435dbe45ec0cd5ab525e93bd82a528ceee098b769fe1\""
2025-05-16T19:50:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:22.395937700-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for container &ContainerMetadata{Name:nova-scheduler,Attempt:0,}"
2025-05-16T19:50:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:22.491115023-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for &ContainerMetadata{Name:nova-scheduler,Attempt:0,} returns container id \"8936b9af9a59bfbf152900d7b7dd71862158ee39df44c0815cd8d1b64d8ae7de\""
2025-05-16T19:50:22-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:22.494205704-04:00" level=info msg="StartContainer for \"8936b9af9a59bfbf152900d7b7dd71862158ee39df44c0815cd8d1b64d8ae7de\""
2025-05-16T19:50:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:23.775917254-04:00" level=info msg="StartContainer for \"8936b9af9a59bfbf152900d7b7dd71862158ee39df44c0815cd8d1b64d8ae7de\" returns successfully"
2025-05-16T19:50:23-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:23.790037046-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:50:53-04:00 k8s.containerd[36097]: time="2025-05-16T19:50:53.798798299-04:00" level=info msg="trying next host" error="failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp 185.125.188.51:443: i/o timeout" host=registry.jujucharms.com
2025-05-16T19:51:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:51:07.431688388-04:00" level=info msg="trying next host" error="failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/blobs/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" host=registry.jujucharms.com
2025-05-16T19:51:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:51:07.435445546-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp 185.125.188.51:443: i/o timeout"
2025-05-16T19:51:07-04:00 k8s.kubelet[38157]: E0516 19:51:07.437298   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp 185.125.188.51:443: i/o timeout" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:51:07-04:00 k8s.containerd[36097]: time="2025-05-16T19:51:07.441082461-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:51:26-04:00 k8s.containerd[36097]: time="2025-05-16T19:51:26.648691264-04:00" level=info msg="trying next host" error="failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" host=registry.jujucharms.com
2025-05-16T19:51:38-04:00 k8s.containerd[36097]: time="2025-05-16T19:51:38.150950476-04:00" level=info msg="trying next host" error="failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/blobs/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" host=registry.jujucharms.com
2025-05-16T19:51:38-04:00 k8s.containerd[36097]: time="2025-05-16T19:51:38.157985534-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving"
2025-05-16T19:51:38-04:00 k8s.kubelet[38157]: E0516 19:51:38.158473   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:51:38-04:00 k8s.containerd[36097]: time="2025-05-16T19:51:38.159267528-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:51:56-04:00 k8s.containerd[36097]: time="2025-05-16T19:51:56.582207271-04:00" level=info msg="trying next host" error="failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" host=registry.jujucharms.com
2025-05-16T19:52:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:52:11.943566510-04:00" level=info msg="trying next host" error="failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/blobs/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" host=registry.jujucharms.com
2025-05-16T19:52:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:52:11.950422869-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving"
2025-05-16T19:52:11-04:00 k8s.kubelet[38157]: E0516 19:52:11.952118   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:52:11-04:00 k8s.containerd[36097]: time="2025-05-16T19:52:11.952639871-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:52:30-04:00 k8s.containerd[36097]: time="2025-05-16T19:52:30.376928310-04:00" level=info msg="trying next host" error="failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" host=registry.jujucharms.com
2025-05-16T19:52:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:52:42.663376733-04:00" level=info msg="trying next host" error="failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/blobs/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" host=registry.jujucharms.com
2025-05-16T19:52:42-04:00 k8s.containerd[36097]: time="2025-05-16T19:52:42.670346445-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving"
2025-05-16T19:52:42-04:00 k8s.kubelet[38157]: E0516 19:52:42.670996   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to do request: Head \"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:52:42-04:00 k8s.kubelet[38157]: - (error decoding original message: message key "MESSAGE" truncated)
2025-05-16T19:52:42-04:00 k8s.kubelet[38157]: E0516 19:52:42.672703   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nova-spiceproxy\" with ErrImagePull: \"[rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp 185.125.188.51:443: i/o timeout, failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving]\"" pod="openstack/nova-0" podUID="0689787d-3910-4f26-84a4-d98f21f25a2b"
2025-05-16T19:52:43-04:00 k8s.kubelet[38157]: E0516 19:52:43.187706   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nova-spiceproxy\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": ErrImagePull: [rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp 185.125.188.51:443: i/o timeout, failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving]\"" pod="openstack/nova-0" podUID="0689787d-3910-4f26-84a4-d98f21f25a2b"
2025-05-16T19:52:44-04:00 k8s.kubelet[38157]: E0516 19:52:44.138429   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nova-spiceproxy\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": ErrImagePull: [rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp 185.125.188.51:443: i/o timeout, failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving]\"" pod="openstack/nova-0" podUID="0689787d-3910-4f26-84a4-d98f21f25a2b"
2025-05-16T19:52:45-04:00 k8s.kubelet[38157]: E0516 19:52:45.148735   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nova-spiceproxy\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": ErrImagePull: [rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp 185.125.188.51:443: i/o timeout, failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving]\"" pod="openstack/nova-0" podUID="0689787d-3910-4f26-84a4-d98f21f25a2b"
2025-05-16T19:52:46-04:00 k8s.kubelet[38157]: E0516 19:52:46.151333   38157 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nova-spiceproxy\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": ErrImagePull: [rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp 185.125.188.51:443: i/o timeout, failed to pull and unpack image \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to resolve reference \\\"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": failed to do request: Head \\\"https://registry.jujucharms.com/v2/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image/manifests/sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\\\": dial tcp: lookup registry.jujucharms.com on 127.0.0.53:53: server misbehaving]\"" pod="openstack/nova-0" podUID="0689787d-3910-4f26-84a4-d98f21f25a2b"
2025-05-16T19:53:00-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:00.360113911-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:53:01-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:01.501205637-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:53:01-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:01.934505038-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:53:01-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:01.943592077-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:53:01-04:00 k8s.kubelet[38157]: E0516 19:53:01.944143   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:53:01-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:01.944684312-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:53:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:03.299711743-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:53:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:03.748713665-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:53:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:03.763348146-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:53:03-04:00 k8s.kubelet[38157]: E0516 19:53:03.764398   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:53:03-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:03.765182416-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:53:04-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:04.693050266-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:53:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:05.147584830-04:00" level=info msg="trying next host" error="pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" host=registry.jujucharms.com
2025-05-16T19:53:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:05.154017351-04:00" level=error msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" failed" error="failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed"
2025-05-16T19:53:05-04:00 k8s.kubelet[38157]: E0516 19:53:05.154538   38157 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": failed to resolve reference \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed" image="registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9"
2025-05-16T19:53:05-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:05.156212236-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\""
2025-05-16T19:53:06-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:06.499700237-04:00" level=info msg="ImageCreate event &ImageCreate{Name:registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
2025-05-16T19:53:06-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:06.523715995-04:00" level=info msg="PullImage \"registry.jujucharms.com/charm/cxpoxnvod1u9zuavqxnfmu8pseyhoa3d2np1q/nova-spiceproxy-image@sha256:40106c0a0b2972c865cf453956a00b3a32422971601f69092158b93c068503f9\" returns image reference \"sha256:38aadeed93d1559105e7435dbe45ec0cd5ab525e93bd82a528ceee098b769fe1\""
2025-05-16T19:53:06-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:06.760087246-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for container &ContainerMetadata{Name:nova-spiceproxy,Attempt:0,}"
2025-05-16T19:53:06-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:06.970139860-04:00" level=info msg="CreateContainer within sandbox \"40aa67eec86cacd0d6ff89ad330b252037810698ca74194b0e1ace34a8b11d30\" for &ContainerMetadata{Name:nova-spiceproxy,Attempt:0,} returns container id \"6104bbc63f31450d673cc6d36365c8c1f1fbb78552e501c5697f43ab45dc2df6\""
2025-05-16T19:53:06-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:06.972727204-04:00" level=info msg="StartContainer for \"6104bbc63f31450d673cc6d36365c8c1f1fbb78552e501c5697f43ab45dc2df6\""
2025-05-16T19:53:08-04:00 k8s.containerd[36097]: time="2025-05-16T19:53:08.815867865-04:00" level=info msg="StartContainer for \"6104bbc63f31450d673cc6d36365c8c1f1fbb78552e501c5697f43ab45dc2df6\" returns successfully"
2025-05-16T19:53:09-04:00 k8s.kubelet[38157]: I0516 19:53:09.544230   38157 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="openstack/nova-0" podStartSLOduration=35.872284464 podStartE2EDuration="9m2.544171339s" podCreationTimestamp="2025-05-16 19:44:07 -0400 EDT" firstStartedPulling="2025-05-16 19:44:39.85718021 -0400 EDT m=+672.994776897" lastFinishedPulling="2025-05-16 19:53:06.529067085 -0400 EDT m=+1179.666663772" observedRunningTime="2025-05-16 19:53:09.526163781 -0400 EDT m=+1182.663760481" watchObservedRunningTime="2025-05-16 19:53:09.544171339 -0400 EDT m=+1182.681768026"
2025-05-16T19:59:53-04:00 k8s.kube-apiserver[37236]: I0516 19:59:53.048760   37236 alloc.go:330] "allocated clusterIPs" service="openstack/traefik-public-lb" clusterIPs={"IPv4":"10.152.183.70"}
2025-05-16T19:59:53-04:00 k8s.kube-apiserver[37236]: I0516 19:59:53.538765   37236 alloc.go:330] "allocated clusterIPs" service="openstack/traefik-lb" clusterIPs={"IPv4":"10.152.183.73"}
2025-05-16T19:59:53-04:00 k8s.kube-controller-manager[36331]: I0516 19:59:53.736918   36331 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="openstack/traefik-lb" err="EndpointSlice informer cache is out of date"
2025-05-16T20:03:27-04:00 k8s.containerd[36097]: time="2025-05-16T20:03:27.335717465-04:00" level=error msg="ExecSync for \"66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d\" failed" error="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded"
2025-05-16T20:03:27-04:00 k8s.kubelet[38157]: E0516 20:03:27.338083   38157 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded" containerID="66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d" cmd=["mongo","--port=37017","--tls","--tlsAllowInvalidHostnames","--tlsAllowInvalidCertificates","--tlsCertificateKeyFile=/var/lib/juju/server.pem","--eval","db.adminCommand('ping')"]
2025-05-16T20:09:44-04:00 k8s.kube-apiserver[37236]: I0516 20:09:44.808748   37236 alloc.go:330] "allocated clusterIPs" service="openstack/mysql-primary" clusterIPs={"IPv4":"10.152.183.112"}
2025-05-16T20:09:45-04:00 k8s.kube-apiserver[37236]: I0516 20:09:45.720740   37236 alloc.go:330] "allocated clusterIPs" service="openstack/mysql-replicas" clusterIPs={"IPv4":"10.152.183.108"}
2025-05-16T20:10:37-04:00 k8s.kube-apiserver[37236]: I0516 20:10:37.085304   37236 alloc.go:330] "allocated clusterIPs" service="openstack/placement-mysql-router-service" clusterIPs={"IPv4":"10.152.183.59"}
2025-05-16T20:12:39-04:00 k8s.kube-apiserver[37236]: I0516 20:12:39.070353   37236 alloc.go:330] "allocated clusterIPs" service="openstack/glance-mysql-router-service" clusterIPs={"IPv4":"10.152.183.214"}
2025-05-16T20:14:50-04:00 k8s.kube-apiserver[37236]: I0516 20:14:50.044314   37236 alloc.go:330] "allocated clusterIPs" service="openstack/neutron-mysql-router-service" clusterIPs={"IPv4":"10.152.183.235"}
2025-05-16T20:17:18-04:00 k8s.kube-apiserver[37236]: I0516 20:17:18.295420   37236 alloc.go:330] "allocated clusterIPs" service="openstack/nova-api-mysql-router-service" clusterIPs={"IPv4":"10.152.183.211"}
2025-05-16T20:18:48-04:00 k8s.kube-apiserver[37236]: I0516 20:18:48.602048   37236 alloc.go:330] "allocated clusterIPs" service="openstack/keystone-mysql-router-service" clusterIPs={"IPv4":"10.152.183.206"}
2025-05-16T20:20:43-04:00 k8s.kube-apiserver[37236]: I0516 20:20:43.635995   37236 alloc.go:330] "allocated clusterIPs" service="openstack/nova-mysql-router-service" clusterIPs={"IPv4":"10.152.183.79"}
2025-05-16T20:21:13-04:00 k8s.k8sd[35160]: E0516 20:21:13.971933   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-16T20:23:02-04:00 k8s.kube-apiserver[37236]: I0516 20:23:02.945175   37236 alloc.go:330] "allocated clusterIPs" service="openstack/nova-cell-mysql-router-service" clusterIPs={"IPv4":"10.152.183.222"}
2025-05-16T20:25:51-04:00 k8s.kube-apiserver[37236]: I0516 20:25:51.651958   37236 alloc.go:330] "allocated clusterIPs" service="openstack/cinder-mysql-router-service" clusterIPs={"IPv4":"10.152.183.114"}
2025-05-16T20:30:59-04:00 k8s.kube-apiserver[37236]: I0516 20:30:59.446186   37236 alloc.go:330] "allocated clusterIPs" service="openstack/horizon-mysql-router-service" clusterIPs={"IPv4":"10.152.183.36"}
2025-05-16T20:56:21-04:00 k8s.k8sd[35160]: E0516 20:56:21.162006   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-16T21:30:15-04:00 k8s.k8sd[35160]: E0516 21:30:15.507766   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-16T22:21:49-04:00 k8s.k8sd[35160]: E0516 22:21:49.001402   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-16T23:03:31-04:00 k8s.k8sd[35160]: E0516 23:03:31.792114   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-16T23:43:19-04:00 k8s.k8sd[35160]: E0516 23:43:19.842403   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T00:25:37-04:00 k8s.k8sd[35160]: E0517 00:25:37.763346   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T01:08:03-04:00 k8s.k8sd[35160]: E0517 01:08:03.764552   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T02:04:42-04:00 k8s.k8sd[35160]: E0517 02:04:42.371256   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T02:51:13-04:00 k8s.k8sd[35160]: E0517 02:51:13.012558   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T03:21:49-04:00 k8s.containerd[36097]: time="2025-05-17T03:21:49.378529332-04:00" level=error msg="Failed to get usage for snapshot \"4342cf5392e89bc2f05b568c1631681b52d3c9b3f09a1b5886ad950c7a3eff54\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/217/fs/tmp/qgh5h068: no such file or directory"
2025-05-17T03:49:04-04:00 k8s.k8sd[35160]: E0517 03:49:04.136468   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T04:36:11-04:00 k8s.k8sd[35160]: E0517 04:36:11.856579   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T05:22:03-04:00 k8s.k8sd[35160]: E0517 05:22:03.331742   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T06:01:54-04:00 k8s.k8sd[35160]: E0517 06:01:54.065325   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T06:50:03-04:00 k8s.k8sd[35160]: E0517 06:50:03.926897   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T07:21:50-04:00 k8s.k8sd[35160]: E0517 07:21:50.158607   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T08:06:02-04:00 k8s.k8sd[35160]: E0517 08:06:02.330084   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T08:41:24-04:00 k8s.k8sd[35160]: E0517 08:41:24.243911   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T09:26:35-04:00 k8s.k8sd[35160]: E0517 09:26:35.484483   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T10:18:48-04:00 k8s.k8sd[35160]: E0517 10:18:48.129691   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T10:52:36-04:00 k8s.k8sd[35160]: E0517 10:52:36.621026   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T11:42:15-04:00 k8s.k8sd[35160]: E0517 11:42:15.152487   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T12:25:29-04:00 k8s.k8sd[35160]: E0517 12:25:29.620693   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T13:18:00-04:00 k8s.k8sd[35160]: E0517 13:18:00.026262   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T13:55:13-04:00 k8s.k8sd[35160]: E0517 13:55:13.687314   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T14:37:09-04:00 k8s.containerd[36097]: time="2025-05-17T14:37:09.450879827-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-17T14:45:53-04:00 k8s.k8sd[35160]: E0517 14:45:53.395633   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T14:58:09-04:00 k8s.containerd[36097]: time="2025-05-17T14:58:09.431228165-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-17T15:23:42-04:00 k8s.k8sd[35160]: E0517 15:23:42.113266   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T16:08:06-04:00 k8s.k8sd[35160]: E0517 16:08:06.824200   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T16:57:54-04:00 k8s.k8sd[35160]: E0517 16:57:54.813889   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T17:53:22-04:00 k8s.k8sd[35160]: E0517 17:53:22.233260   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T18:06:09-04:00 k8s.containerd[36097]: time="2025-05-17T18:06:09.512748651-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-17T18:40:47-04:00 k8s.containerd[36097]: time="2025-05-17T18:40:47.619125474-04:00" level=error msg="ExecSync for \"66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d\" failed" error="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded"
2025-05-17T18:40:47-04:00 k8s.kubelet[38157]: E0517 18:40:47.619801   38157 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = DeadlineExceeded desc = failed to exec in container: timeout 1s exceeded: context deadline exceeded" containerID="66162b3a67c07afe628df37b7f2d106cdfe059e212c69336fe5d8e7af48a7f4d" cmd=["mongo","--port=37017","--tls","--tlsAllowInvalidHostnames","--tlsAllowInvalidCertificates","--tlsCertificateKeyFile=/var/lib/juju/server.pem","--eval","db.adminCommand('ping')"]
2025-05-17T18:51:53-04:00 k8s.k8sd[35160]: E0517 18:51:53.090683   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T19:14:09-04:00 k8s.containerd[36097]: time="2025-05-17T19:14:09.460171500-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_error/error.log-20250517_2314: no such file or directory"
2025-05-17T19:30:12-04:00 k8s.k8sd[35160]: E0517 19:30:12.166530   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T20:09:09-04:00 k8s.containerd[36097]: time="2025-05-17T20:09:09.452382796-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-17T20:10:37-04:00 k8s.k8sd[35160]: E0517 20:10:37.991381   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T20:48:50-04:00 k8s.k8sd[35160]: E0517 20:48:50.125119   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T21:44:17-04:00 k8s.k8sd[35160]: E0517 21:44:17.085585   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T22:09:09-04:00 k8s.containerd[36097]: time="2025-05-17T22:09:09.483041622-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-17T22:41:14-04:00 k8s.k8sd[35160]: E0517 22:41:14.860171   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-17T23:21:19-04:00 k8s.k8sd[35160]: E0517 23:21:19.202385   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T00:14:33-04:00 k8s.k8sd[35160]: E0518 00:14:33.984952   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T00:32:09-04:00 k8s.containerd[36097]: time="2025-05-18T00:32:09.474428134-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T00:51:17-04:00 k8s.k8sd[35160]: E0518 00:51:17.980937   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T00:52:09-04:00 k8s.containerd[36097]: time="2025-05-18T00:52:09.450781535-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T00:58:09-04:00 k8s.containerd[36097]: time="2025-05-18T00:58:09.542493351-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T01:15:09-04:00 k8s.containerd[36097]: time="2025-05-18T01:15:09.468730350-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T01:44:57-04:00 k8s.k8sd[35160]: E0518 01:44:57.855688   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T02:12:09-04:00 k8s.containerd[36097]: time="2025-05-18T02:12:09.456871860-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T02:15:58-04:00 k8s.k8sd[35160]: E0518 02:15:58.458572   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T03:13:14-04:00 k8s.k8sd[35160]: E0518 03:13:14.761575   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T03:43:09-04:00 k8s.containerd[36097]: time="2025-05-18T03:43:09.487019515-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T03:51:42-04:00 k8s.k8sd[35160]: E0518 03:51:42.566132   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T04:43:56-04:00 k8s.k8sd[35160]: E0518 04:43:56.652183   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T05:25:28-04:00 k8s.k8sd[35160]: E0518 05:25:28.734527   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T05:43:09-04:00 k8s.containerd[36097]: time="2025-05-18T05:43:09.454681158-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T05:45:09-04:00 k8s.containerd[36097]: time="2025-05-18T05:45:09.477748887-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T06:20:18-04:00 k8s.k8sd[35160]: E0518 06:20:18.837250   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T06:32:32-04:00 k8s.kube-controller-manager[36331]: I0518 06:32:32.190137   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="openstack/modeloperator-6cbd5bbd86" duration="169.556µs"
2025-05-18T06:32:32-04:00 k8s.kube-controller-manager[36331]: I0518 06:32:32.190319   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="67.961µs"
2025-05-18T06:32:32-04:00 k8s.kube-controller-manager[36331]: I0518 06:32:32.190411   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="60.605µs"
2025-05-18T06:32:32-04:00 k8s.kube-controller-manager[36331]: I0518 06:32:32.191351   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="190.873µs"
2025-05-18T06:32:32-04:00 k8s.kube-controller-manager[36331]: I0518 06:32:32.191842   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="67.169µs"
2025-05-18T06:32:32-04:00 k8s.kube-controller-manager[36331]: I0518 06:32:32.191947   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="55.388µs"
2025-05-18T06:32:32-04:00 k8s.kube-controller-manager[36331]: I0518 06:32:32.192523   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="controller-sunbeam-controller/modeloperator-856d6cf4bf" duration="51.739µs"
2025-05-18T06:43:09-04:00 k8s.containerd[36097]: time="2025-05-18T06:43:09.476851506-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T07:09:27-04:00 k8s.k8sd[35160]: E0518 07:09:27.256884   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T07:41:09-04:00 k8s.containerd[36097]: time="2025-05-18T07:41:09.482184253-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T07:45:09-04:00 k8s.containerd[36097]: time="2025-05-18T07:45:09.465952970-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T07:55:42-04:00 k8s.k8sd[35160]: E0518 07:55:42.095042   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T08:18:09-04:00 k8s.containerd[36097]: time="2025-05-18T08:18:09.471075807-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T08:32:56-04:00 k8s.k8sd[35160]: E0518 08:32:56.065037   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T08:34:09-04:00 k8s.containerd[36097]: time="2025-05-18T08:34:09.441363745-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_audit/audit.log-20250518_1234: no such file or directory"
2025-05-18T09:23:30-04:00 k8s.k8sd[35160]: E0518 09:23:30.553210   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T10:03:50-04:00 k8s.k8sd[35160]: E0518 10:03:50.767723   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T10:32:09-04:00 k8s.containerd[36097]: time="2025-05-18T10:32:09.626609346-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T10:37:58-04:00 k8s.k8sd[35160]: E0518 10:37:58.874837   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T11:22:09-04:00 k8s.containerd[36097]: time="2025-05-18T11:22:09.470199271-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T11:28:26-04:00 k8s.k8sd[35160]: E0518 11:28:26.896585   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T12:14:09-04:00 k8s.containerd[36097]: time="2025-05-18T12:14:09.519960925-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T12:15:03-04:00 k8s.k8sd[35160]: E0518 12:15:03.515654   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T13:05:09-04:00 k8s.containerd[36097]: time="2025-05-18T13:05:09.526864258-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_audit/audit.log-20250518_1705: no such file or directory"
2025-05-18T13:12:42-04:00 k8s.k8sd[35160]: E0518 13:12:42.953853   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T13:39:09-04:00 k8s.containerd[36097]: time="2025-05-18T13:39:09.512920659-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_error/error.log-20250518_1739: no such file or directory"
2025-05-18T13:53:37-04:00 k8s.k8sd[35160]: E0518 13:53:37.936909   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T14:13:09-04:00 k8s.containerd[36097]: time="2025-05-18T14:13:09.513289360-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T14:38:09-04:00 k8s.containerd[36097]: time="2025-05-18T14:38:09.492136163-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T14:50:03-04:00 k8s.k8sd[35160]: E0518 14:50:03.287500   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T15:06:09-04:00 k8s.containerd[36097]: time="2025-05-18T15:06:09.511016575-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T15:23:04-04:00 k8s.k8sd[35160]: E0518 15:23:04.475009   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T15:33:09-04:00 k8s.containerd[36097]: time="2025-05-18T15:33:09.514613850-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T15:55:06-04:00 k8s.k8sd[35160]: E0518 15:55:06.366727   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T16:14:09-04:00 k8s.containerd[36097]: time="2025-05-18T16:14:09.553020893-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T16:47:09-04:00 k8s.k8sd[35160]: E0518 16:47:09.285216   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T17:18:10-04:00 k8s.k8sd[35160]: E0518 17:18:10.943435   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T18:00:29-04:00 k8s.k8sd[35160]: E0518 18:00:29.485888   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T18:47:09-04:00 k8s.containerd[36097]: time="2025-05-18T18:47:09.531437121-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_error/error.log-20250518_2247: no such file or directory"
2025-05-18T18:56:11-04:00 k8s.k8sd[35160]: E0518 18:56:11.160241   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T19:35:15-04:00 k8s.k8sd[35160]: E0518 19:35:15.447438   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T20:03:09-04:00 k8s.containerd[36097]: time="2025-05-18T20:03:09.531230856-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-18T20:27:15-04:00 k8s.k8sd[35160]: E0518 20:27:15.165885   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T20:27:19-04:00 k8s.containerd[36097]: time="2025-05-18T20:27:19.627178397-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-18T21:11:14-04:00 k8s.k8sd[35160]: E0518 21:11:14.634968   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T21:57:30-04:00 k8s.k8sd[35160]: E0518 21:57:30.478222   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T22:45:27-04:00 k8s.k8sd[35160]: E0518 22:45:27.100252   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T23:23:06-04:00 k8s.k8sd[35160]: E0518 23:23:06.682042   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-18T23:59:12-04:00 k8s.k8sd[35160]: E0518 23:59:12.473344   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T00:35:17-04:00 k8s.k8sd[35160]: E0519 00:35:17.020586   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T01:10:17-04:00 k8s.k8sd[35160]: E0519 01:10:17.400703   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T01:47:36-04:00 k8s.k8sd[35160]: E0519 01:47:36.407614   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T02:36:37-04:00 k8s.k8sd[35160]: E0519 02:36:37.592741   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T03:29:15-04:00 k8s.k8sd[35160]: E0519 03:29:15.094262   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T04:09:07-04:00 k8s.k8sd[35160]: E0519 04:09:07.419171   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T04:44:02-04:00 k8s.k8sd[35160]: E0519 04:44:02.857296   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T05:31:05-04:00 k8s.k8sd[35160]: E0519 05:31:05.055646   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T06:19:12-04:00 k8s.k8sd[35160]: E0519 06:19:12.703863   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T06:33:49-04:00 k8s.containerd[36097]: time="2025-05-19T06:33:49.610074513-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_error/error.log-20250519_1033: no such file or directory"
2025-05-19T07:09:28-04:00 k8s.k8sd[35160]: E0519 07:09:28.496874   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T08:06:29-04:00 k8s.k8sd[35160]: E0519 08:06:29.551113   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T08:50:54-04:00 k8s.k8sd[35160]: E0519 08:50:54.684367   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T09:38:38-04:00 k8s.k8sd[35160]: E0519 09:38:38.043337   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T10:29:20-04:00 k8s.k8sd[35160]: E0519 10:29:20.649694   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T11:13:58-04:00 k8s.k8sd[35160]: E0519 11:13:58.985310   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T11:46:25-04:00 k8s.k8sd[35160]: E0519 11:46:25.147027   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T12:18:37-04:00 k8s.k8sd[35160]: E0519 12:18:37.229907   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T12:54:26-04:00 k8s.k8sd[35160]: E0519 12:54:26.113116   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T13:37:45-04:00 k8s.k8sd[35160]: E0519 13:37:45.721063   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T14:29:17-04:00 k8s.k8sd[35160]: E0519 14:29:17.760362   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T15:04:33-04:00 k8s.k8sd[35160]: E0519 15:04:33.509896   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T15:50:56-04:00 k8s.k8sd[35160]: E0519 15:50:56.961966   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T16:35:22-04:00 k8s.k8sd[35160]: E0519 16:35:22.253126   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T16:48:29-04:00 k8s.containerd[36097]: time="2025-05-19T16:48:29.564849146-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-19T17:14:52-04:00 k8s.k8sd[35160]: E0519 17:14:52.010438   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T17:31:33-04:00 k8s.kube-controller-manager[36331]: I0519 17:31:33.662347   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="metallb-system/metallb-controller-86cb6b5b76" duration="160.264µs"
2025-05-19T17:31:33-04:00 k8s.kube-controller-manager[36331]: I0519 17:31:33.662537   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-56d5ddcf86" duration="87.853µs"
2025-05-19T17:31:33-04:00 k8s.kube-controller-manager[36331]: I0519 17:31:33.662641   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6978488575" duration="51.653µs"
2025-05-19T17:31:33-04:00 k8s.kube-controller-manager[36331]: I0519 17:31:33.662723   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/cilium-operator-6f799b7b4" duration="53.016µs"
2025-05-19T17:31:33-04:00 k8s.kube-controller-manager[36331]: I0519 17:31:33.663849   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="controller-sunbeam-controller/modeloperator-856d6cf4bf" duration="49.383µs"
2025-05-19T17:31:33-04:00 k8s.kube-controller-manager[36331]: I0519 17:31:33.663945   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-8694c96fb7" duration="51.77µs"
2025-05-19T17:31:33-04:00 k8s.kube-controller-manager[36331]: I0519 17:31:33.664004   36331 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="openstack/modeloperator-6cbd5bbd86" duration="142.626µs"
2025-05-19T17:50:19-04:00 k8s.containerd[36097]: time="2025-05-19T17:50:19.573370264-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-19T18:00:08-04:00 k8s.k8sd[35160]: E0519 18:00:08.963286   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T18:07:09-04:00 k8s.containerd[36097]: time="2025-05-19T18:07:09.504254826-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_audit/audit.log-20250519_2207: no such file or directory"
2025-05-19T18:44:46-04:00 k8s.k8sd[35160]: E0519 18:44:46.076326   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T19:28:25-04:00 k8s.k8sd[35160]: E0519 19:28:25.461291   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T19:35:19-04:00 k8s.containerd[36097]: time="2025-05-19T19:35:19.748941891-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_audit/audit.log-20250519_2335: no such file or directory"
2025-05-19T19:37:39-04:00 k8s.containerd[36097]: time="2025-05-19T19:37:39.566567039-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-19T20:18:40-04:00 k8s.k8sd[35160]: E0519 20:18:40.298679   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T21:03:06-04:00 k8s.k8sd[35160]: E0519 21:03:06.378848   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T21:28:59-04:00 k8s.containerd[36097]: time="2025-05-19T21:28:59.566740517-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-19T21:55:47-04:00 k8s.k8sd[35160]: E0519 21:55:47.356132   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T22:48:05-04:00 k8s.k8sd[35160]: E0519 22:48:05.660374   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T23:22:16-04:00 k8s.k8sd[35160]: E0519 23:22:16.871303   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-19T23:37:59-04:00 k8s.containerd[36097]: time="2025-05-19T23:37:59.629306221-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-20T00:12:37-04:00 k8s.k8sd[35160]: E0520 00:12:37.818525   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T00:51:32-04:00 k8s.k8sd[35160]: E0520 00:51:32.549498   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T01:17:20-04:00 k8s.containerd[36097]: time="2025-05-20T01:17:20.829497639-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-20T01:27:59-04:00 k8s.k8sd[35160]: E0520 01:27:59.036210   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T02:11:04-04:00 k8s.k8sd[35160]: E0520 02:11:04.347591   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T03:08:41-04:00 k8s.k8sd[35160]: E0520 03:08:41.062604   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T03:43:18-04:00 k8s.k8sd[35160]: E0520 03:43:18.889567   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T04:26:23-04:00 k8s.k8sd[35160]: E0520 04:26:23.666859   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T05:25:59-04:00 k8s.k8sd[35160]: E0520 05:25:59.006370   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T06:16:48-04:00 k8s.k8sd[35160]: E0520 06:16:48.852398   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T07:05:58-04:00 k8s.k8sd[35160]: E0520 07:05:58.475008   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T07:54:59-04:00 k8s.k8sd[35160]: E0520 07:54:59.224943   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T08:43:19-04:00 k8s.containerd[36097]: time="2025-05-20T08:43:19.760525427-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_audit/audit.log-20250520_1243: no such file or directory"
2025-05-20T08:43:21-04:00 k8s.k8sd[35160]: E0520 08:43:21.720535   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T09:15:19-04:00 k8s.containerd[36097]: time="2025-05-20T09:15:19.584519678-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-20T09:18:00-04:00 k8s.k8sd[35160]: E0520 09:18:00.026581   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T09:42:29-04:00 k8s.containerd[36097]: time="2025-05-20T09:42:29.628118107-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-20T09:51:28-04:00 k8s.k8sd[35160]: E0520 09:51:28.907830   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T10:23:19-04:00 k8s.containerd[36097]: time="2025-05-20T10:23:19.853759558-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_error/error.log-20250520_1423: no such file or directory"
2025-05-20T10:40:19-04:00 k8s.containerd[36097]: time="2025-05-20T10:40:19.661024390-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-20T10:46:54-04:00 k8s.k8sd[35160]: E0520 10:46:54.684221   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T11:20:11-04:00 k8s.k8sd[35160]: E0520 11:20:11.904580   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T11:28:29-04:00 k8s.containerd[36097]: time="2025-05-20T11:28:29.666399083-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/audit.log: no such file or directory"
2025-05-20T11:50:19-04:00 k8s.containerd[36097]: time="2025-05-20T11:50:19.593160777-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_audit/audit.log-20250520_1550: no such file or directory"
2025-05-20T12:01:49-04:00 k8s.k8sd[35160]: E0520 12:01:49.829068   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T12:40:16-04:00 k8s.k8sd[35160]: E0520 12:40:16.591417   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T13:17:06-04:00 k8s.k8sd[35160]: E0520 13:17:06.518415   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T13:47:09-04:00 k8s.containerd[36097]: time="2025-05-20T13:47:09.574191958-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-20T14:10:47-04:00 k8s.k8sd[35160]: E0520 14:10:47.617448   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T15:03:07-04:00 k8s.k8sd[35160]: E0520 15:03:07.870889   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T15:57:59-04:00 k8s.k8sd[35160]: E0520 15:57:59.014659   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T16:25:50-04:00 k8s.containerd[36097]: time="2025-05-20T16:25:50.221106737-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/error.log: no such file or directory"
2025-05-20T16:37:25-04:00 k8s.k8sd[35160]: E0520 16:37:25.170095   35160 controllers/node_configuration.go:64] "Failed to watch configmap" err="watch closed" logger="k8sd" controller="node-configuration" name="k8sd-config" namespace="kube-system"
2025-05-20T16:59:49-04:00 k8s.containerd[36097]: time="2025-05-20T16:59:49.524278108-04:00" level=error msg="Failed to get usage for snapshot \"4baabed14a9d551f52845eb684d84560f0149b785bc3ad48c34534783efbac41\"" error="lstat /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/186/fs/var/log/mysql/archive_audit/audit.log-20250517_1513.gz: no such file or directory"
