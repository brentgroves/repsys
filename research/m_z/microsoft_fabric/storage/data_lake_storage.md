# **[What is Data Lake Storage?](https://www.actian.com/what-is-data-lake-storage/)**

What is data lake storage, and why does it matter? A data lake is a repository built by private organization IT departments or public cloud providers for the storage, processing and maintenance of data in any format and from any source, such as video, newsfeeds, your applications, web scraping, IoT, data marts, data warehouses or mobile devices. In 2010, then CTO of Pentaho, James Dixon, contrasted data marts and data lakes. Data marts or Data warehouses stored and allowed for data analysis based on known schema attributes.  On the other hand, a data lake provides for interrogation based upon any number of details contained within the acquired data. Data lake storage allows you to store most any type and size of data and subsequently search for something and not be sure of what your search will find nor exactly what the data’s format will be.

Data Lake Storage in a Modern Data Architecture
The design and management of data storage have historically been the most costly and challenging aspect of IT. As the variety of data types and sources increased, especially with most organizations presenting their services digitally over the internet, this complexity has led to modernizing data architecture (people, processes and tools). Consider that a few years ago, any data had to fit a rigid schema and was therefore highly structured, whereas today, data will be semi- or unstructured and therefore often unformatted.

Twenty-five years ago, 1TB of data storage required three large racks of disk drives, each the size of a small washing machine. Today, data lake storage provides the opportunity to have petabytes of data at your disposal – either physically in a small desktop enclosure or more likely virtualized in the cloud. Good news or a security and management nightmare? What information does the organization want to extract from its stored data when it is analyzed? The information contained within this stored data helps enterprises service customers with exceptional products, but understanding what data the organization has, how, where, and when it was acquired, and who can access it are key architectural considerations.

Best practices for modern data lake storage architecture are:

Know what you have by using a combination of catalogs (think about the library card system) with each record comprised of metadata quickly defining each piece of data within the lake, its source, date of acquisition and other attributes to simplify data queries and archival.
Audit software and active governance of what you have, why you have it, is the way you have it or received it legal, who is using it, and when can you delete it.
Access Control Lists (ACL) and other security practices are designed and governed for each data lake (see the section on Microsoft Azure data lake storage for more).
Cloud data lakes encrypt data as part of their initial intake. The skill required to use this information or transfer it in an encrypted state requires specialized software skills and changes to applications and service designs. Not only for the data lake owners but any partner or customer that share information and security tokens. Where the tokens will be stored and who has access is a modern data lake storage architecture design priority.
What is the Difference Between a Data Lake and a Data Warehouse?
Data warehouse storage was the original storage option strategy where you knew what you had, what it looked like and which specific data each application, database, datamart and other source systems delivered to it or needed to retrieve from it. Because data warehouses focused on the aggregation of structured data from departmental operational databases, they were also very structured.  And while they could be one or two orders of magnitude larger than the largest database they pulled data from, even the aggregate dataset sizes were no more than tens of terabytes if that.  Over time, as new types of data required historical aggregation, web clickstreams, archived documents, video surveillance data and other data types and sources, data warehouses seemed ill-suited as they could not absorb the massive data size associated with these non-traditional data sources.  Further, other departmental data repositories were too narrowly defined in their functions: document management systems worked only for documents, video surveillance systems only for video storage, and so forth.  The quest for a centralized yet multi-faceted data repository that wouldn’t run out of storage room led to the introduction of virtual storage (VMWare, NetApp, etc.) and facilitated the creation of cloud data storage and data lake options.

To understand data lakes, you need to return to 1992 when Ralph Kimball and Bill Inmon coined the term data warehouse to describe the rules and schemas that would control data warehouse architecture designs for decades to come.

The Wikipedia definition of a data warehouse highlights its use and weakness: “central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating trending reports for senior management reporting such as annual and quarterly comparisons.”
