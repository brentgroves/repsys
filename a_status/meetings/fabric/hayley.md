# Fabric/Power BI

## direction

Move data using Notebooks from current data sources to data lake or warehouse. Spark Notebooks are to software as multiple cores are to hardware. A pyspark notebook uses 10 vCPU cores as compared to a python notebook which uses only 2. Both use Parquet files and delta logs this is what makes them quick. Power BI has new mode for connecting with them which combines accuracy and quickness.
The One Lake plugin is also significant and is the OneDrive of Analytics. With it Maintenance can sync new spreadsheet to the structures fabric workspace. From there a notebook is used to delete previous records and add new ones to the parquet tables. The Power BI report will have the parquet table as the data source and will be available to an Entra group through a org app report menu.

The following is in markdown format and can be viewed by copying and pasting the contents below into an online markdown viewer, such as at <https://markdownlivepreview.com/>.

- **[Org Apps](https://www.youtube.com/watch?v=7W3_9J0emKM&t=124s)**

- **[Apache Spark](https://www.geeksforgeeks.org/java/components-of-apache-spark/)**
- **[Fabric Notebooks](https://www.youtube.com/watch?v=do8_gogFlLk)**
- **[Lakehouse and Delta Lake tables](https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-and-delta-tables)**
- **[Delta Lake vs Delta Table](https://community.databricks.com/t5/data-engineering/deltalkake-vs-delta-table/td-p/5027)**
- **[](https://learn.microsoft.com/en-us/fabric/data-engineering/tutorial-lakehouse-introduction)**
